{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! if [ ! $pip_done ]; then pip install -q transformers ;fi \n",
    "\n",
    "# # command-line\n",
    "# # pip uninstall folium\n",
    "# ! pip install folium==0.2.1\n",
    "\n",
    "# ! if [ ! $pip_done ]; then pip install -q datasets jiwer ;fi \n",
    "# ! if [ ! $pip_done ]; then pip install -q sentencepiece ;fi \n",
    "\n",
    "# pip_done = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing parameters: Only run on N samples if test=True\n",
    "max_sample_test = 1000\n",
    "# Maximum length of equation in # of tokens\n",
    "max_length_token = 100\n",
    "# Size of vocab in tokenizer, i.e. distinct number of tokens to learn\n",
    "vocab_size = 600\n",
    "# Batch size for DataLoader\n",
    "batch_size = 16\n",
    "# Version number for model\n",
    "version = 5\n",
    "# Report loss every N steps\n",
    "report_step = 100 \n",
    "# Number of epochs to train. Each epoch is one cycle of training over full training set.\n",
    "num_epoch = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IPython magic to enable inline display of matplotlib plots\n",
    "%matplotlib inline\n",
    "from IPython.display import Image as ipyImage\n",
    "from IPython.display import display, Latex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_df(pth, type = None):\n",
    "    df = pd.read_csv(join(data_dir, pth), header=None, sep=' ')\n",
    "    df = df.drop(2, axis=1)\n",
    "    df.rename(columns={0: \"text_index\", 1: \"file_name\"}, inplace=True)\n",
    "\n",
    "    # Replace text with formulas\n",
    "    df['text'] = df.apply (lambda row: formulas[int(row['text_index'])], axis=1)\n",
    "    df['len'] = df.apply (lambda row: row['text'].count(' '), axis=1)\n",
    "    # Sort by ascending length of formula\n",
    "    df_sorted = df.sort_values(by=\"len\")\n",
    "    df_filtered = df_sorted[df_sorted['len'] > 0 ]\n",
    "    df_trunc = df_filtered[df_filtered['len'] <= max_length_token ]\n",
    "    df = df_trunc\n",
    "    df = df.reset_index(drop=True)\n",
    "    globals()[\"{}_df\".format(type)] = df\n",
    "    # print(type+'_dataframe',df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from os.path import join\n",
    "\n",
    "data_dir = './data/'\n",
    "\n",
    "# formulas_file = join(data_dir, \"im2latex_formulas.lst\")\n",
    "# with open(formulas_file, 'r', encoding='ISO-8859-1') as f:\n",
    "#     formulas = [formula.strip('\\n') for formula in f.readlines()]\n",
    "\n",
    "formulas_file = join(data_dir, \"im2latex_formulas_utf.lst\")\n",
    "# linux 인코딩 변환 : iconv -c -f ISO-8859-1 -t utf-8 im2latex_formulas.lst > im2latex_formulas_utf.lst\n",
    "with open(formulas_file, 'r') as f:\n",
    "    formulas = [formula.strip('\\n') for formula in f.readlines()]\n",
    "\n",
    "# train_df/test_df/validate_df\n",
    "types = ['train', 'test', 'validate']\n",
    "for type in types:\n",
    "    pth = 'im2latex_{}.lst'.format(type)\n",
    "    preprocess_df(pth, type = type)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55435    H_{D7} =  \\frac{h_{D_{7}}}{2\\ell}|x_{2}^{2}| +...\n",
      "Name: text, dtype: object\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=192x64>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "# Truncate data for testing\n",
    "if test: train_df = train_df.truncate(after=max_sample_test-1)\n",
    "# Show some images as sanity check\n",
    "sample = train_df.sample(n=1)\n",
    "print(sample['text'])\n",
    "# ipyImage는 출력이 안되는 오류 자주 발생\n",
    "# ipyImage(join(data_dir, 'formula_images_processed', sample.iloc[0]['file_name']+\".png\"))\n",
    "image = Image.open(join(data_dir, 'formula_images_processed', sample.iloc[0]['file_name']+\".png\"))\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train word level tokenizer\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "max_length = max_length_token\n",
    "tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n",
    "tokenizer.enable_padding(length=max_length)\n",
    "tokenizer.enable_truncation(max_length=max_length)\n",
    "\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "trainer = WordLevelTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"],\n",
    "                     vocab_size=vocab_size,\n",
    "                     show_progress=True,\n",
    "                     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "tokenizer.post_processor = TemplateProcessing(\n",
    "    single=\"[CLS] $A [SEP]\",\n",
    "    pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
    "    special_tokens=[\n",
    "        (\"[CLS]\", 1),\n",
    "        (\"[SEP]\", 2),\n",
    "    ],\n",
    ")\n",
    "\n",
    "files = [formulas_file]\n",
    "tokenizer.train(files, trainer)\n",
    "\n",
    "tokenizer.save(\"tokenizer-wordlevel.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', '{\\\\', 'cal', 'L', '}=\\\\', 'frac', '{', '1', '}{', '2', '}\\\\', 'partial_', '{\\\\', 'mu', '}\\\\', 'phi', '\\\\', 'partial', '^{\\\\', 'mu', '}\\\\', 'phi', '+\\\\', 'frac', '{', '1', '}{', '2', '}\\\\', 'partial_', '{\\\\', 'mu', '}\\\\', 'chi', '\\\\', 'partial', '^{\\\\', 'mu', '}\\\\', 'chi', '-', 'U', '(\\\\', 'phi', ',\\\\', 'chi', '),', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "[1, 9, 46, 93, 136, 14, 6, 10, 23, 8, 13, 102, 9, 27, 13, 54, 5, 48, 32, 27, 13, 54, 76, 14, 6, 10, 23, 8, 13, 102, 9, 27, 13, 172, 5, 48, 32, 27, 13, 172, 16, 179, 30, 54, 49, 172, 176, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "# Sanity check of tokenizer\n",
    "i = 5\n",
    "\n",
    "print( tokenizer.encode(train_df.loc[i, 'text']).tokens )\n",
    "\n",
    "print( tokenizer.encode(train_df.loc[i, 'text']).ids )\n",
    "\n",
    "print( tokenizer.token_to_id(\"[PAD]\") )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jeeyoon/miniforge3/envs/imlatex/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "\n",
    "class IAMDataset(Dataset):\n",
    "    def __init__(self, root_dir, df, processor, tokenizer, max_target_length=max_length):\n",
    "        self.root_dir = root_dir\n",
    "        self.df = df\n",
    "        self.processor = processor\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_target_length = max_target_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # get file name + text \n",
    "        file_name = self.df['file_name'][idx]\n",
    "        text = self.df['text'][idx]\n",
    "        # prepare image (i.e. resize + normalize)\n",
    "        image = Image.open(self.root_dir + file_name+'.png').convert(\"RGB\")\n",
    "        pixel_values = self.processor(image, return_tensors=\"pt\").pixel_values\n",
    "#        # add labels (input_ids) by encoding the text\n",
    "#        labels = self.processor.tokenizer(text, \n",
    "#                                          padding=\"max_length\", \n",
    "#                                          max_length=self.max_target_length).input_ids\n",
    "#        # important: make sure that PAD tokens are ignored by the loss function\n",
    "#        labels = [label if label != self.processor.tokenizer.pad_token_id else -100 for label in labels]\n",
    "        labels = self.tokenizer.encode(text).ids\n",
    "        labels = [label if label != self.tokenizer.token_to_id(\"[PAD]\") else -100 for label in labels]\n",
    "\n",
    "        encoding = {\"pixel_values\": pixel_values.squeeze(), \"labels\": torch.tensor(labels)}\n",
    "        return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrOCRProcessor\n",
    "root_dir = join(data_dir, 'formula_images_processed/',) \n",
    "processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-small-printed\", Use_fast= False)\n",
    "train_dataset = IAMDataset(root_dir=root_dir,\n",
    "                           df=train_df,\n",
    "                           processor=processor,\n",
    "                           tokenizer=tokenizer)\n",
    "valid_dataset = IAMDataset(root_dir=root_dir,\n",
    "                           df=validate_df,\n",
    "                           processor=processor,\n",
    "                           tokenizer=tokenizer)\n",
    "eval_dataset = IAMDataset(root_dir=root_dir,\n",
    "                           df=test_df,\n",
    "                           processor=processor,\n",
    "                           tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 78024\n",
      "Number of training examples: 8638\n",
      "Number of validation examples: 9699\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of training examples:\", len(train_dataset))\n",
    "print(\"Number of training examples:\", len(valid_dataset))\n",
    "print(\"Number of validation examples:\", len(eval_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pixel_values torch.Size([3, 384, 384])\n",
      "labels torch.Size([100])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAABACAIAAABdtOgoAAADWElEQVR4nO2asUv7QBTH75ILQShq/wBxca+Di9ClQ1dx8I8QNxcnoTgpFBf9F9z9AxxEEP0HXJxE3IuGEmzu8p5D7K8mFkma6+9B7322JvQ43qfNvbtvJCIKhg7v9yUcU2dc9lqSogBjjBwjZqojImaD2JngopMTgIhKqeFw+PLyMhgMhBBSyqoOpJRKqSiKRqORzZkuKBMBiCil7PV6GxsbV1dXW1tbOzs7Hx8flR5HAPD8/Hx4eLi2tnZ+fi6E0FrPZeILQ1ZfYwwi3t3dCSGur68R8e3tTQjR7/cRMUkSLMfn5+fFxcXBwYEQ4vj4uNJ33UQgIgAYY0aj0fb29srKCiJqrQGg0+m02+0kSYwxAFBpXCklCyjD9yPI931jzOPj4/7+vjEmSRIpZbfbvb+/11r7vl/+/2SMiaIIuQsqx2QNkFJ6nheGoVIquzIcDj3Pq9rPKKWCILA5x4Um1wUBAABM7nnez4/MPMgJ4Ir/f3JtKAA0Gg0x3n953pR9MmOX7xKnaRoEQbvdvrm50VorpRAxSZIZnubGGDFeUdI05b/U33hCCCklAARBsLu7e3t7my3FiHh2dnZ0dLS0tKS1Lr8UZ2s4IsZx7Pt+GIZznP4CkHWj2VYgiqJut7u5ufn09NRqtTqdThzH2a+4ZFcbx/Hl5eXe3t7y8vL6+vrp6enDwwMipmlqvYNeDCZHPYgopUzTtNfrra6uvr+/n5yc+L6fXS+pEwBeX1/DMGw0GgAwGAyazWaz2aw0iFPkztp+l4kLN2+Kh504PkxGRKXUDNUvDMj+/qbyaTNjF+70iWEBxEwXUPO59K/HqjOIIxQFYO1Et36q7BRFATUTXbSRKjtF8Ti6TqKLNlJl5/i5La6T6NpKlV1DTL1aNdGdR6rsCMX3gmZOdG2lyq5RXITrJLq2UmWnsLwR41S5KpYFcMWrYlMAcqpcnSlvR4uZEl2LqbJTTFmERfVE126q7BY/e9I6ia6tVNk1cgc1NRNdtJEqu4blk7Lftebq/82UTDh3e6ZMuGaq7BR8VkwM9+nEsABiWAAxLIAYFkAMCyCGBRDDAohhAcSwAGJYADEsgBgWQAwLIIYFEMMCiGEBxLAAYlgAMSyAGBZADAsghgUQwwKIYQHEfAGexWLn5Pl7CwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=128x64 at 0x7F3D8D888A10>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding = train_dataset[0]\n",
    "for k,v in encoding.items():\n",
    "  print(k, v.shape)\n",
    "image = Image.open(train_dataset.root_dir + train_df['file_name'][0]+'.png').convert(\"RGB\")\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\ label { eq : }{\\ cal U } _ {\\ hat { U }\\ hat { V }}= 0 .\n"
     ]
    }
   ],
   "source": [
    "labels = encoding['labels']\n",
    "labels[labels == -100] = tokenizer.token_to_id(\"[PAD]\")\n",
    "label_str = tokenizer.decode(labels.tolist(), skip_special_tokens=True)\n",
    "print(label_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "from torch.utils.data import DataLoader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size=batch_size)\n",
    "validate_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "015f50d770d4426ca8054dc6a3f370d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/4.11k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45ff7e381f11430c90238621e94a4a49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/235M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-small-stage1 and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VisionEncoderDecoderModel(\n",
       "  (encoder): DeiTModel(\n",
       "    (embeddings): DeiTEmbeddings(\n",
       "      (patch_embeddings): PatchEmbeddings(\n",
       "        (projection): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): DeiTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): DeiTLayer(\n",
       "          (attention): DeiTAttention(\n",
       "            (attention): DeiTSelfAttention(\n",
       "              (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): DeiTSelfOutput(\n",
       "              (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): DeiTIntermediate(\n",
       "            (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): DeiTOutput(\n",
       "            (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (1): DeiTLayer(\n",
       "          (attention): DeiTAttention(\n",
       "            (attention): DeiTSelfAttention(\n",
       "              (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): DeiTSelfOutput(\n",
       "              (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): DeiTIntermediate(\n",
       "            (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): DeiTOutput(\n",
       "            (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (2): DeiTLayer(\n",
       "          (attention): DeiTAttention(\n",
       "            (attention): DeiTSelfAttention(\n",
       "              (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): DeiTSelfOutput(\n",
       "              (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): DeiTIntermediate(\n",
       "            (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): DeiTOutput(\n",
       "            (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (3): DeiTLayer(\n",
       "          (attention): DeiTAttention(\n",
       "            (attention): DeiTSelfAttention(\n",
       "              (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): DeiTSelfOutput(\n",
       "              (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): DeiTIntermediate(\n",
       "            (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): DeiTOutput(\n",
       "            (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (4): DeiTLayer(\n",
       "          (attention): DeiTAttention(\n",
       "            (attention): DeiTSelfAttention(\n",
       "              (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): DeiTSelfOutput(\n",
       "              (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): DeiTIntermediate(\n",
       "            (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): DeiTOutput(\n",
       "            (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (5): DeiTLayer(\n",
       "          (attention): DeiTAttention(\n",
       "            (attention): DeiTSelfAttention(\n",
       "              (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): DeiTSelfOutput(\n",
       "              (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): DeiTIntermediate(\n",
       "            (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): DeiTOutput(\n",
       "            (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (6): DeiTLayer(\n",
       "          (attention): DeiTAttention(\n",
       "            (attention): DeiTSelfAttention(\n",
       "              (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): DeiTSelfOutput(\n",
       "              (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): DeiTIntermediate(\n",
       "            (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): DeiTOutput(\n",
       "            (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (7): DeiTLayer(\n",
       "          (attention): DeiTAttention(\n",
       "            (attention): DeiTSelfAttention(\n",
       "              (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): DeiTSelfOutput(\n",
       "              (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): DeiTIntermediate(\n",
       "            (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): DeiTOutput(\n",
       "            (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (8): DeiTLayer(\n",
       "          (attention): DeiTAttention(\n",
       "            (attention): DeiTSelfAttention(\n",
       "              (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): DeiTSelfOutput(\n",
       "              (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): DeiTIntermediate(\n",
       "            (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): DeiTOutput(\n",
       "            (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (9): DeiTLayer(\n",
       "          (attention): DeiTAttention(\n",
       "            (attention): DeiTSelfAttention(\n",
       "              (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): DeiTSelfOutput(\n",
       "              (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): DeiTIntermediate(\n",
       "            (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): DeiTOutput(\n",
       "            (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (10): DeiTLayer(\n",
       "          (attention): DeiTAttention(\n",
       "            (attention): DeiTSelfAttention(\n",
       "              (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): DeiTSelfOutput(\n",
       "              (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): DeiTIntermediate(\n",
       "            (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): DeiTOutput(\n",
       "            (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (11): DeiTLayer(\n",
       "          (attention): DeiTAttention(\n",
       "            (attention): DeiTSelfAttention(\n",
       "              (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): DeiTSelfOutput(\n",
       "              (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): DeiTIntermediate(\n",
       "            (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): DeiTOutput(\n",
       "            (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "    (pooler): DeiTPooler(\n",
       "      (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (decoder): TrOCRForCausalLM(\n",
       "    (model): TrOCRDecoderWrapper(\n",
       "      (decoder): TrOCRDecoder(\n",
       "        (embed_tokens): Embedding(64044, 256, padding_idx=1)\n",
       "        (embed_positions): TrOCRLearnedPositionalEmbedding(514, 256)\n",
       "        (layernorm_embedding): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (layers): ModuleList(\n",
       "          (0): TrOCRDecoderLayer(\n",
       "            (self_attn): TrOCRAttention(\n",
       "              (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            )\n",
       "            (activation_fn): ReLU()\n",
       "            (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): TrOCRAttention(\n",
       "              (k_proj): Linear(in_features=384, out_features=256, bias=True)\n",
       "              (v_proj): Linear(in_features=384, out_features=256, bias=True)\n",
       "              (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (1): TrOCRDecoderLayer(\n",
       "            (self_attn): TrOCRAttention(\n",
       "              (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            )\n",
       "            (activation_fn): ReLU()\n",
       "            (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): TrOCRAttention(\n",
       "              (k_proj): Linear(in_features=384, out_features=256, bias=True)\n",
       "              (v_proj): Linear(in_features=384, out_features=256, bias=True)\n",
       "              (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (2): TrOCRDecoderLayer(\n",
       "            (self_attn): TrOCRAttention(\n",
       "              (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            )\n",
       "            (activation_fn): ReLU()\n",
       "            (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): TrOCRAttention(\n",
       "              (k_proj): Linear(in_features=384, out_features=256, bias=True)\n",
       "              (v_proj): Linear(in_features=384, out_features=256, bias=True)\n",
       "              (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (3): TrOCRDecoderLayer(\n",
       "            (self_attn): TrOCRAttention(\n",
       "              (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            )\n",
       "            (activation_fn): ReLU()\n",
       "            (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): TrOCRAttention(\n",
       "              (k_proj): Linear(in_features=384, out_features=256, bias=True)\n",
       "              (v_proj): Linear(in_features=384, out_features=256, bias=True)\n",
       "              (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (4): TrOCRDecoderLayer(\n",
       "            (self_attn): TrOCRAttention(\n",
       "              (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            )\n",
       "            (activation_fn): ReLU()\n",
       "            (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): TrOCRAttention(\n",
       "              (k_proj): Linear(in_features=384, out_features=256, bias=True)\n",
       "              (v_proj): Linear(in_features=384, out_features=256, bias=True)\n",
       "              (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (5): TrOCRDecoderLayer(\n",
       "            (self_attn): TrOCRAttention(\n",
       "              (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            )\n",
       "            (activation_fn): ReLU()\n",
       "            (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): TrOCRAttention(\n",
       "              (k_proj): Linear(in_features=384, out_features=256, bias=True)\n",
       "              (v_proj): Linear(in_features=384, out_features=256, bias=True)\n",
       "              (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (output_projection): Linear(in_features=256, out_features=64044, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import VisionEncoderDecoderModel\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-small-stage1\")\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set special tokens used for creating the decoder_input_ids from the labels\n",
    "model.config.decoder_start_token_id = tokenizer.token_to_id(\"[CLS]\")\n",
    "model.config.pad_token_id = tokenizer.token_to_id(\"[PAD]\")\n",
    "# make sure vocab size is set correctly\n",
    "model.config.vocab_size = vocab_size\n",
    "\n",
    "# set beam search parameters\n",
    "model.config.eos_token_id = tokenizer.token_to_id(\"[SEP]\")\n",
    "model.config.max_length = max_length #64\n",
    "model.config.early_stopping = True\n",
    "model.config.no_repeat_ngram_size = 3\n",
    "model.config.length_penalty = 2.0\n",
    "model.config.num_beams = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.tokenizer = tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f04a67f58de4d9fab978a2d7c50a1bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/2.16k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "cer_metric = load_metric(\"cer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "cer_metric = load_metric(\"cer\")\n",
    "\n",
    "def compute_cer(pred_ids, label_ids):\n",
    "    pred_str = tokenizer.decode_batch(pred_ids.tolist(), skip_special_tokens=True)\n",
    "    label_ids[label_ids == -100] = tokenizer.token_to_id(\"[PAD]\")\n",
    "    label_str = tokenizer.decode_batch(label_ids.tolist(), skip_special_tokens=True)\n",
    "    \n",
    "    # Filter out empty label strings\n",
    "#    for l in enumerate(label_str):\n",
    "#        if not l:\n",
    "#            label_str.pop(l)\n",
    "#            pred_str.pop(l)\n",
    "\n",
    "    cer = cer_metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return cer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7898a15008be4b4eb0478eceb1bf3dea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4877 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/formula_images_processed/74eb7c041b.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-941eb6146574>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m      \u001b[0;31m# get the inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m      \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m             \u001b[0mit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m                 \u001b[0;31m# return super(tqdm...) will not catch exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1194\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1195\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1196\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    650\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 652\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    653\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    690\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 692\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    693\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    694\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-83d8de6a9373>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m# prepare image (i.e. resize + normalize)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RGB\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mpixel_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpixel_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m#        # add labels (input_ids) by encoding the text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode)\u001b[0m\n\u001b[1;32m   2841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2842\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2843\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2844\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2845\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/formula_images_processed/74eb7c041b.png'"
     ]
    }
   ],
   "source": [
    " num_epoch = 5\n",
    " report_step = 100 \n",
    " \n",
    "from transformers import AdamW\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "for epoch in range(num_epoch):  # loop over the dataset multiple times\n",
    "   # train\n",
    "   model.train()\n",
    "   train_loss = 0.0\n",
    "   for i, batch in enumerate(tqdm(train_dataloader)):\n",
    "      # get the inputs\n",
    "      for k,v in batch.items():\n",
    "        batch[k] = v.to(device)\n",
    "\n",
    "      # forward + backward + optimize\n",
    "      outputs = model(**batch)\n",
    "      loss = outputs.loss\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      train_loss += loss.item()\n",
    "      if i % report_step == 0: print(f\"Loss: {loss.item()}\") \n",
    "\n",
    "   print(f\"Loss after epoch {epoch}:\", train_loss/len(train_dataloader))\n",
    "   model.save_pretrained(f\"version_{version}/epoch_{epoch}\")\n",
    "    \n",
    "\n",
    "model.save_pretrained(f\"version_{version}/final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(eval_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "i = 0\n",
    "for batch in (test_dataloader):\n",
    "    for k,v in batch.items():\n",
    "        batch[k] = v.to(device)\n",
    "    outputs = model.generate(batch[\"pixel_values\"].to(device))\n",
    "    px = batch[\"pixel_values\"][i].cpu().numpy()\n",
    "    labels = batch[\"labels\"][i]\n",
    "    pred = outputs[i]\n",
    "    break\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(px.transpose([1,2,0]), aspect=1/5)\n",
    "label_true = tokenizer.decode(labels.tolist()).replace(\" \", \"\")\n",
    "label_pred = tokenizer.decode(pred.tolist()).replace(\" \", \"\")\n",
    "\n",
    "print( 'True label: ' + label_true )\n",
    "print( 'Pred. label: ' + label_pred )\n",
    "# print( pred.tolist() )\n",
    "\n",
    "pred_latex = Latex(f'${label_pred}$')\n",
    "display(pred_latex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate\n",
    "model.eval()\n",
    "valid_cer = 0.0\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(tqdm(eval_dataloader)):\n",
    "        # run batch generation\n",
    "        outputs = model.generate(batch[\"pixel_values\"].to(device))\n",
    "        # compute metrics\n",
    "        cer = compute_cer(pred_ids=outputs, label_ids=batch[\"labels\"])\n",
    "        \n",
    "        valid_cer += cer\n",
    "\n",
    "print(\"Validation CER:\", valid_cer / len(eval_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred_and_label_str(pred, label):\n",
    "    pred_str = tokenizer.decode_batch(pred.tolist(), skip_special_tokens=True)\n",
    "    label[label == -100] = tokenizer.token_to_id(\"[PAD]\")\n",
    "    label_str = tokenizer.decode_batch(label.tolist(), skip_special_tokens=True)\n",
    "    return (pred_str, label_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "candidate_corpus = []\n",
    "references_corpus = []\n",
    "valid_bleu = 0.0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(tqdm(eval_dataloader)):\n",
    "        # run batch generation\n",
    "        outputs = model.generate(batch[\"pixel_values\"].to(device))\n",
    "        # compute metrics\n",
    "        pred, label = get_pred_and_label_str(outputs, batch[\"labels\"])\n",
    "        \n",
    "        for s in pred: s = s.split(\" \")\n",
    "        for s in label: s = s.split(\" \")\n",
    "        candidate_corpus.extend(pred)\n",
    "        references_corpus.extend(label)\n",
    "    \n",
    "valid_bleu =  nltk.translate.bleu_score.corpus_bleu(\n",
    "        references_corpus, candidate_corpus,\n",
    "        weights=(0.25, 0.25, 0.25, 0.25)\n",
    ")\n",
    "valid_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!v=$version; cp tokenizer-wordlevel.json version_$$v/final\n",
    "!v=$version; tar zcvf version_$$v.tar.gz --directory=version_$$v/final "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
  },
  "kernelspec": {
   "display_name": "imlatex",
   "language": "python",
   "name": "imlatex"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
