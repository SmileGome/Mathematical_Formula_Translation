{"statistics": [{"chapter": "2.3", "title": "Measures of the Center of the Data", "mathml": "<math display=\"block\"><semantics><mrow>\n<mover>\n<mi>x</mi>\n<mo>\u00e2\u0080\u0093</mo>\n</mover>\n<mo>=</mo>\n<mfrac>\n<mrow>\n<mn>1</mn>\n<mo>+</mo>\n<mn>1</mn>\n<mo>+</mo>\n<mn>1</mn>\n<mo>+</mo>\n<mn>2</mn>\n<mo>+</mo>\n<mn>2</mn>\n<mo>+</mo>\n<mn>3</mn>\n<mo>+</mo>\n<mn>4</mn>\n<mo>+</mo>\n<mn>4</mn>\n<mo>+</mo>\n<mn>4</mn>\n<mo>+</mo>\n<mn>4</mn>\n<mo>+</mo>\n<mn>4</mn>\n</mrow>\n<mn>11</mn>\n</mfrac>\n<mo>=</mo>\n<mn>2.7</mn>\n</mrow><annotation-xml encoding=\"MathML-Content\"><mover>\n<mi>x</mi>\n<mo>\u00e2\u0080\u0093</mo>\n</mover><mo>=</mo><mfrac>\n<mrow>\n<mn>1</mn>\n<mo>+</mo>\n<mn>1</mn>\n<mo>+</mo>\n<mn>1</mn>\n<mo>+</mo>\n<mn>2</mn>\n<mo>+</mo>\n<mn>2</mn>\n<mo>+</mo>\n<mn>3</mn>\n<mo>+</mo>\n<mn>4</mn>\n<mo>+</mo>\n<mn>4</mn>\n<mo>+</mo>\n<mn>4</mn>\n<mo>+</mo>\n<mn>4</mn>\n<mo>+</mo>\n<mn>4</mn>\n</mrow>\n<mn>11</mn>\n</mfrac><mo>=</mo><mn>2.7</mn></annotation-xml></semantics></math>\n</div", "content": "\nThe \"center\" of a data set is also a way of describing location.\nThe two most widely used measures of the \"center\" of the data are the mean (average) and the median. To calculate the mean weight of 50 people, add the 50 weights together and divide by 50. Technically this is the arithmetic mean. We will discuss the geometric mean later. To find the  median weight of the 50 people, order the data and find the number that splits the data into two equal parts meaning an equal number of observations on each side. The weight of 25 people are below this weight and 25 people are heavier than this weight. The median is generally a better measure of the center when there are extreme values or outliers because it is not affected by the precise numerical values of the outliers. The mean is the most common measure of the center.\nNOTE\n\n\nThe words \u00e2\u0080\u009cmean\u00e2\u0080\u009d and \u00e2\u0080\u009caverage\u00e2\u0080\u009d are often used interchangeably. The substitution of one word for the other is common practice. The technical term is \u00e2\u0080\u009carithmetic mean\u00e2\u0080\u009d and \u00e2\u0080\u009caverage\u00e2\u0080\u009d is technically a center location. Formally, the arithmetic mean is called the first moment of the distribution by mathematicians. However, in practice among non-statisticians, \u00e2\u0080\u009caverage\" is commonly accepted for \u00e2\u0080\u009carithmetic mean.\u00e2\u0080\u009d\nWhen each value in the data set is not unique, the mean can be calculated by multiplying each distinct value by its frequency and then dividing the sum by the total number of data values. The letter used to represent the sample mean is an x with a bar over it (pronounced \u00e2\u0080\u009cx bar\u00e2\u0080\u009d): \nx\n\u00e2\u0080\u0093\n\nx\n\u00e2\u0080\u0093\n.The Greek letter \u00ce\u00bc (pronounced \"mew\") represents the population mean. One of the requirements for the sample mean to be a good estimate of the population mean is for the sample taken to be truly random.\nTo see that both ways of calculating the mean are the same, consider the sample:\n1; 1; 1; 2; 2; 3; 4; 4; 4; 4; 4\n\nx\n\u00e2\u0080\u0093\n\n=\n\n\n1\n+\n1\n+\n1\n+\n2\n+\n2\n+\n3\n+\n4\n+\n4\n+\n4\n+\n4\n+\n4\n\n11\n\n=\n2.7\n\nx\n\u00e2\u0080\u0093\n=\n\n1\n+\n1\n+\n1\n+\n2\n+\n2\n+\n3\n+\n4\n+\n4\n+\n4\n+\n4\n+\n4\n\n11\n=2.7\n\n\n\n\nx\n\u00e2\u0080\u0093\n\n=\n\n3(1)+2(2)+1(3)+5(4)\n\n\n11\n\n\n=2.7\n\n\n\nx\n\u00e2\u0080\u0093\n\n=\n\n3(1)+2(2)+1(3)+5(4)\n\n\n11\n\n\n=2.7\nIn the second calculation, the frequencies are 3, 2, 1, and 5.You can quickly find the location of the median by using the expression \n\n\nn\n+\n1\n\n2\n\n\n\nn\n+\n1\n\n2\n.\nThe letter n is the total number of data values in the sample. If n is an odd number, the median is the middle value of the ordered data (ordered smallest to largest). If n is an even number, the median is equal to the two middle values added together and divided by two after the data has been ordered. For example, if the total number of data values is 97, then \n\n\nn\n+\n1\n\n2\n\n\n\nn\n+\n1\n\n2\n=\n\n\n\n97\n+\n1\n\n2\n\n\n\n97\n+\n1\n\n2\n = 49. The median is the 49th value in the ordered data. If the total number of data values is 100, then \n\n\nn\n+\n1\n\n2\n\n\n\nn\n+\n1\n\n2\n=\n\n\n\n100\n+\n1\n\n2\n\n\n\n100\n+\n1\n\n2\n = 50.5. The median occurs midway between the 50th and 51st values. The location of the median and the value of the median are not the same. The upper case letter M is often used to represent the median. The next example illustrates the location of the median and the value of the median.\nExample \n2.24\n \n\n\n\n\nAIDS data indicating the number of months a patient with AIDS lives after taking a new antibody drug are as follows (smallest to largest):\n3; 4; 8; 8; 10; 11; 12; 13; 14; 15; 15; 16; 16; 17; 17; 18; 21; 22; 22; 24; 24; 25; 26; 26; 27; 27; 29; 29; 31; 32; 33; 33; 34; 34; 35; 37; 40; 44; 44; 47;\nCalculate the mean and the median.\n\n\n\nSolution\n1\n\n\nThe calculation for the mean is:\n\n\nx\n\u00e2\u0080\u0093\n\n=\n\n\n[\n3\n+\n4\n+\n(\n8\n)\n(\n2\n)\n+\n10\n+\n11\n+\n12\n+\n13\n+\n14\n+\n(\n15\n)\n(\n2\n)\n+\n(\n16\n)\n(\n2\n)\n+\n...\n+\n35\n+\n37\n+\n40\n+\n(\n44\n)\n(\n2\n)\n+\n47\n]\n\n40\n\n=\n23.6\n\nx\n\u00e2\u0080\u0093\n=\n\n[\n3\n+\n4\n+\n(\n8\n)\n(\n2\n)\n+\n10\n+\n11\n+\n12\n+\n13\n+\n14\n+\n(\n15\n)\n(\n2\n)\n+\n(\n16\n)\n(\n2\n)\n+\n...\n+\n35\n+\n37\n+\n40\n+\n(\n44\n)\n(\n2\n)\n+\n47\n]\n\n40\n=23.6\nTo find the median, M, first use the formula for the location. The location is:\n\n\n\nn\n+\n1\n\n2\n\n=\n\n\n40\n+\n1\n\n2\n\n=\n20.5\n\n\nn\n+\n1\n\n2\n=\n\n40\n+\n1\n\n2\n=20.5\nStarting at the smallest value, the median is located between the 20th and 21st values (the two 24s):\n3; 4; 8; 8; 10; 11; 12; 13; 14; 15; 15; 16; 16; 17; 17; 18; 21; 22; 22; 24; 24; 25; 26; 26; 27; 27; 29; 29; 31; 32; 33; 33; 34; 34; 35; 37; 40; 44; 44; 47;\n\nM\n=\n\n\n24\n+\n24\n\n2\n\n=\n24\nM=\n\n24\n+\n24\n\n2\n=24\n\n\n\n\n\n\n\nExample \n2.25\n \n\n\n\n\nSuppose that in a small town of 50 people, one person earns $5,000,000 per year and the other 49 each earn $30,000. Which is the better measure of the \"center\": the mean or the median?\n\n\n\nSolution\n1\n\n\n\n\n\n\n\n\nx\n\u00e2\u0080\u0093\n\n=\n\n5,000,000+49(30,000)\n\n\n50\n\n\n=129,400\n\n\n\n\n\nx\n\u00e2\u0080\u0093\n\n=\n\n5,000,000+49(30,000)\n\n\n50\n\n\n=129,400\n\n\n\n\nM = 30,000\n(There are 49 people who earn $30,000 and one person who earns $5,000,000.)\nThe median is a better measure of the \"center\" than the mean because 49 of  the values are 30,000 and one is 5,000,000.  The 5,000,000 is an outlier.  The 30,000 gives us a better sense of the middle of the data.\n\n\n\n\n\nAnother measure of the center is the mode. The mode is the most frequent value. There can be more than one mode in a data set as long as those values have the same frequency and that frequency is the highest. A data set with two modes is called bimodal.\n\nExample \n2.26\n \n\nStatistics exam scores for 20 students are as follows:\n\n50; 53; 59; 59; 63; 63; 72; 72; 72; 72; 72; 76; 78; 81; 83; 84; 84; 84; 90; 93\n\n\nFind the mode.\n\n\n\n\nSolution\n1\n\n\nThe most frequent score is 72, which occurs five times. Mode  =  72.\n\n\n\n\n\n\nExample \n2.27\n \n\n\nFive real estate exam scores are 430, 430, 480, 480, 495. The data set is bimodal because the scores 430 and 480 each occur twice.\nWhen is the mode the best measure of the \"center\"? Consider a weight loss program that advertises a mean weight loss of six pounds the first week of the program. The mode might indicate that most people lose two pounds the first week, making the program less appealing.\n\nNOTE\n\n\nThe mode can be calculated for qualitative data as well as for quantitative data. For example, if the data set is: red, red, red, green, green, yellow, purple, black, blue, the mode is red.\n\nCalculating the Arithmetic Mean of Grouped Frequency TablesWhen only grouped data is available, you do not know the individual data values (we only know intervals and interval frequencies); therefore, you cannot compute an exact mean for the data set. What we must do is estimate the actual mean by calculating the mean of a frequency table. A frequency table is a data representation in which grouped data is displayed along with the corresponding frequencies. To calculate the mean from a grouped frequency table we can apply the basic definition of mean: mean = \n\n data\u00a0sum\n\n number\u00a0of\u00a0data\u00a0values\n\n\n\n\n\n data\u00a0sum\n\n number\u00a0of\u00a0data\u00a0values\n\n\n We simply need to modify the definition to fit within the restrictions of a frequency table.\nSince we do not know the individual data values we can instead find the midpoint of each interval. The midpoint is \n\n\n lower\u00a0boundary+upper\u00a0boundary\n\n2\n\n\n\n\n lower\u00a0boundary+upper\u00a0boundary\n\n2\n\n. We can now modify the mean definition to be  Mean\u00a0of\u00a0Frequency\u00a0Table=\n\n\n\u00e2\u0088\u0091 \nfm\n\n\n\n\n\n\u00e2\u0088\u0091 f\n\n\n Mean\u00a0of\u00a0Frequency\u00a0Table=\n\n\n\u00e2\u0088\u0091 \nfm\n\n\n\n\n\n\u00e2\u0088\u0091 f\n\n\n where f = the frequency of the interval and m = the midpoint of the interval.\n\nExample \n2.28\n \n\n\n\n\nA frequency table displaying professor Blount\u00e2\u0080\u0099s last statistic test is shown. Find the best estimate of the class mean.\n\nGrade interval Number of students\n\n50\u00e2\u0080\u009356.5 1\n56.5\u00e2\u0080\u009362.5 0\n62.5\u00e2\u0080\u009368.5 4\n68.5\u00e2\u0080\u009374.5 4\n74.5\u00e2\u0080\u009380.5 2\n80.5\u00e2\u0080\u009386.5 3\n86.5\u00e2\u0080\u009392.5 4\n92.5\u00e2\u0080\u009398.5 1\n\nTable \n2.24\n \n \n\n\n\n\n\nSolution\n1\n\n\n\nFind the midpoints for all intervals\n\n\n\nGrade interval Midpoint\n\n\n50\u00e2\u0080\u009356.5 53.25\n56.5\u00e2\u0080\u009362.5 59.5\n62.5\u00e2\u0080\u009368.5 65.5\n68.5\u00e2\u0080\u009374.5 71.5\n74.5\u00e2\u0080\u009380.5 77.5\n80.5\u00e2\u0080\u009386.5 83.5\n86.5\u00e2\u0080\u009392.5 89.5\n92.5\u00e2\u0080\u009398.5 95.5\n\n\nTable \n2.25\n \n \n\nCalculate the sum of the product of each interval frequency and midpoint.\n\n\n\u00e2\u0088\u0091\n\u00e2\u0080\u008b\n\nfm\n\n\n\n\u00e2\u0088\u0091\n\u00e2\u0080\u008b\n\nfm\n\n\n53.25(1)+59.5(0)+65.5(4)+71.5(4)+77.5(2)+83.5(3)+89.5(4)+95.5(1)=1460.25\n\n\n53.25(1)+59.5(0)+65.5(4)+71.5(4)+77.5(2)+83.5(3)+89.5(4)+95.5(1)=1460.25\n\n\n\n\u00ce\u00bc=\n\n\n\u00e2\u0088\u0091 \nfm\n\n\n\n\n\n\u00e2\u0088\u0091 f\n\n\n\n=\n\n1460.25\n\n\n19\n\n\n=76.86\n\n\n\n\u00ce\u00bc=\n\n\n\u00e2\u0088\u0091 \nfm\n\n\n\n\n\n\u00e2\u0088\u0091 f\n\n\n\n=\n\n1460.25\n\n\n19\n\n\n=76.86\n\n\n\n\n\n\n\n\nTry It \n2.28\n\n\n\n\nMaris conducted a study on the effect that playing video games has on memory recall. As part of her study, she compiled the following data:\nHours teenagers spend on video games Number of teenagers\n0\u00e2\u0080\u00933.53\n3.5\u00e2\u0080\u00937.57\n7.5\u00e2\u0080\u009311.512\n11.5\u00e2\u0080\u009315.57\n15.5\u00e2\u0080\u009319.59\n\nTable \n2.26\n \n \n\nWhat is the best estimate for the mean number of hours spent playing video games?\n\n\n\n"}, {"chapter": "2.4", "title": "Sigma Notation and Calculating the Arithmetic Mean", "mathml": "<math display=\"block\"><semantics><mrow>\n<mi>\u00ce\u00bc</mi>\n<mo>=</mo>\n<mfrac><mn>1</mn><mi>N</mi></mfrac>\n<munderover><mo>\u00e2\u0088\u0091</mo>\n<mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>\n<mi>N</mi>\n</munderover>\n<msub><mi>x</mi><mi>i</mi></msub>\n</mrow><annotation-xml encoding=\"MathML-Content\"><mi>\u00ce\u00bc</mi><mo>=</mo><mfrac><mn>1</mn><mi>N</mi></mfrac><munderover><mo>\u00e2\u0088\u0091</mo>\n<mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>\n<mi>N</mi>\n</munderover><msub><mi>x</mi><mi>i</mi></msub></annotation-xml></semantics></math></div", "content": "\nFormula for Population Mean\n\u00ce\u00bc\n=\n1N\n\u00e2\u0088\u0091\ni=1\nN\n\nxi\n\u00ce\u00bc=1N\u00e2\u0088\u0091\ni=1\nN\nxiFormula for Sample Mean\n\nx\n\u00e2\u0080\u0093\n\n=\n1n\n\u00e2\u0088\u0091\ni=1\nn\n\nxi\n\nx\n\u00e2\u0080\u0093\n=1n\u00e2\u0088\u0091\ni=1\nn\nxiThis unit is here to remind you of material that you once studied and said at the time \u00e2\u0080\u009cI am sure that I will never need this!\u00e2\u0080\u009dHere are the formulas for a population mean and the sample mean. The Greek letter \u00ce\u00bc is the symbol for the population mean and x\u00e2\u0080\u0093x\u00e2\u0080\u0093 is the symbol for the sample mean. Both formulas have a mathematical symbol that tells us how to make the calculations. It is called Sigma notation because the symbol is the Greek capital letter sigma: \u00ce\u00a3. Like all mathematical symbols it tells us what to do: just as the plus sign tells us to add and the x tells us to multiply. These are called mathematical operators. The \u00ce\u00a3 symbol tells us to add a specific list of numbers. Let\u00e2\u0080\u0099s say we have a sample of animals from the local animal shelter and we are interested in their average age.  If we list each value, or observation, in a column, you can give each one an index number. The first number will be number 1 and the second number 2  and so on. \n\nAnimal\nAge\n\n\n1\n9\n\n\n2\n1\n\n\n3\n8.5\n\n\n4\n10.5\n\n\n5\n10\n\n\n6\n8.5\n\n\n7\n12\n\n\n8\n8\n\n\n9\n1\n\n\n10\n9.5\n\n\nTable \n2.27\n \n \n\nEach observation represents a particular animal in the sample. Purr is animal number one and is a 9 year old cat, Toto is animal number 2 and is a 1 year old puppy and so on.To calculate the mean we are told by the formula to add up all these numbers, ages in this case, and then divide the sum by 10, the total number of animals in the sample.Animal number one, the cat Purr, is designated as X1, animal number 2, Toto, is designated as X2 and so on through Dundee who is animal number 10 and is designated as X10.The i in the formula tells us which of the observations   to add together. In this case it is X1 through X10 which is all of them. We know which ones to add by the indexing notation, the i = 1 and the n or capital N for the population. For this example the indexing notation would be i = 1 and because it is a sample we use a small n on the top of the \u00ce\u00a3 which would be 10.The standard deviation requires the same mathematical operator and so it would be helpful to recall this knowledge from your past. The sum of the ages is found to be 78 and dividing by 10 gives us the sample mean age as 7.8 years.\n"}, {"chapter": "2.5", "title": "Geometric Mean", "mathml": "<math display=\"block\"><semantics><mrow>\n<mrow><mover><mi>x</mi><mo>~</mo></mover></mrow>\n<mo>=</mo>\n<msup><mrow>\n<mo>(</mo><mrow><munderover><mi>\u00e2\u0088\u008f</mi><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover></mrow><mrow><msub><mi>x</mi><mi>i</mi></msub></mrow><mo>)</mo></mrow>\n<mrow><mfrac><mn>1</mn><mi>n</mi></mfrac></mrow></msup>\n<mo>=</mo>\n<mroot><mrow><msub><mi>x</mi><mn>1</mn></msub><mo>\u00c2\u00b7</mo><msub><mi>x</mi><mn>2</mn></msub><mn>\u00c2\u00b7\u00c2\u00b7\u00c2\u00b7</mn><msub><mi>x</mi><mi>n</mi></msub></mrow>\n<mrow>\n<mi>n</mi></mrow>\n</mroot>\n<mo>=</mo>\n<msup><mrow><mo>(</mo><msub><mi>x</mi><mn>1</mn></msub><mo>\u00c2\u00b7</mo><msub><mi>x</mi><mn>2</mn></msub><mn>\u00c2\u00b7\u00c2\u00b7\u00c2\u00b7</mn><msub><mi>x</mi><mi>n</mi></msub><mo>)</mo></mrow><mrow><mfrac><mn>1</mn><mi>n</mi></mfrac></mrow></msup>\n</mrow><annotation-xml encoding=\"MathML-Content\"><mrow><mover><mi>x</mi><mo>~</mo></mover></mrow><mo>=</mo><msup><mrow>\n<mo>(</mo><mrow><munderover><mi>\u00e2\u0088\u008f</mi><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover></mrow><mrow><msub><mi>x</mi><mi>i</mi></msub></mrow><mo>)</mo></mrow>\n<mrow><mfrac><mn>1</mn><mi>n</mi></mfrac></mrow></msup><mo>=</mo><mroot><mrow><msub><mi>x</mi><mn>1</mn></msub><mo>\u00c2\u00b7</mo><msub><mi>x</mi><mn>2</mn></msub><mn>\u00c2\u00b7\u00c2\u00b7\u00c2\u00b7</mn><msub><mi>x</mi><mi>n</mi></msub></mrow>\n<mrow>\n<mi>n</mi></mrow>\n</mroot><mo>=</mo><msup><mrow><mo>(</mo><msub><mi>x</mi><mn>1</mn></msub><mo>\u00c2\u00b7</mo><msub><mi>x</mi><mn>2</mn></msub><mn>\u00c2\u00b7\u00c2\u00b7\u00c2\u00b7</mn><msub><mi>x</mi><mi>n</mi></msub><mo>)</mo></mrow><mrow><mfrac><mn>1</mn><mi>n</mi></mfrac></mrow></msup></annotation-xml></semantics></math></div", "content": "\nThe mean (Arithmetic), median and mode are all measures of the \u00e2\u0080\u009ccenter\u00e2\u0080\u009d of the data, the \u00e2\u0080\u009caverage\u00e2\u0080\u009d. They are all in their own way trying to measure the \u00e2\u0080\u009ccommon\u00e2\u0080\u009d point within the data, that which is \u00e2\u0080\u009cnormal\u00e2\u0080\u009d. In the case of the arithmetic mean this is solved by finding the value from which all points are equal linear distances. We can imagine that all the data values are combined through addition and then distributed back to each data point in equal amounts.  The sum of all the values is what is redistributed in equal amounts such that the total sum remains the same.The geometric mean redistributes not the sum of the values but the product of multiplying all the individual values and then redistributing them in equal portions such that the total product remains the same. This can be seen from the formula for the geometric mean, x~x~: (Pronounced x-tilde)\n\nx~\n=\n\n(\u00e2\u0088\u008fi=1nxi)\n1n\n=\nx1\u00c2\u00b7x2\u00c2\u00b7\u00c2\u00b7\u00c2\u00b7xn\n\nn\n\n=\n(x1\u00c2\u00b7x2\u00c2\u00b7\u00c2\u00b7\u00c2\u00b7xn)1n\nx~=\n(\u00e2\u0088\u008fi=1nxi)\n1n=x1\u00c2\u00b7x2\u00c2\u00b7\u00c2\u00b7\u00c2\u00b7xn\n\nn\n=(x1\u00c2\u00b7x2\u00c2\u00b7\u00c2\u00b7\u00c2\u00b7xn)1nwhere \u00cf\u0080\u00cf\u0080 is another mathematical operator, that tells us to multiply all the xixi numbers in the same way capital Greek sigma tells us to add all the xixi numbers. Remember that a fractional exponent is calling for the nth root of the number thus an exponent of 1/3 is the cube root of the number. \nThe geometric mean answers the question, \"if all the quantities had the same value, what would that value have to be in order to achieve the same product?\u00e2\u0080\u009d The geometric mean gets its name from the fact that when redistributed in this way the sides form a geometric shape for which all sides have the same length. To see this, take the example of the numbers 10, 51.2 and 8. The geometric mean is the product of multiplying these three numbers together (4,096) and taking the cube root because there are three numbers among which this product is to be distributed. Thus the geometric mean of these three numbers is 16. This describes a cube 16x16x16 and has a volume of 4,096 units. The geometric mean is relevant in Economics and Finance for dealing with growth: growth of markets, in investment, population and other variables the growth in which there is an interest.  Imagine that our box of 4,096 units (perhaps dollars) is the value of an investment after three years and that the investment returns in percents were the three numbers in our example. The geometric mean will provide us with the answer to the question, what is the average rate of return: 16 percent. The arithmetic mean of these three numbers is 23.6 percent. The reason for this difference, 16 versus 23.6, is that the arithmetic mean is additive and thus does not account for the interest on the interest, compound interest, embedded in the investment growth process. The same issue arises when asking for the average rate of growth of a population or sales or market penetration, etc., knowing the annual rates of growth. The formula for the geometric mean rate of return, or any other growth rate, is:rs=(x1\u00c2\u00b7x2\u00c2\u00b7\u00c2\u00b7\u00c2\u00b7xn)1n-1rs=(x1\u00c2\u00b7x2\u00c2\u00b7\u00c2\u00b7\u00c2\u00b7xn)1n-1Manipulating the formula for the geometric mean can also provide a calculation of the average rate of growth between two periods knowing only the initial value a0a0  and the ending value anan  and the number of periods, nn. The following formula provides this information:(ana0)1n=x~(ana0)1n=x~Finally, we note that the formula for the geometric mean requires that all numbers be positive, greater than zero. The reason of course is that the root of a negative number is undefined for use outside of mathematical theory. There are ways to avoid this problem however. In the case of rates of return and other simple growth problems we can convert the negative values to meaningful positive equivalent values. Imagine that the annual returns for the past three years are +12%, -8%, and +2%. Using the decimal multiplier equivalents of 1.12, 0.92, and 1.02, allows us to compute a geometric mean of 1.0167. Subtracting 1 from this value gives the geometric mean of +1.67% as a net rate of population growth (or financial return). From this example we can see that the geometric mean provides us with this formula for calculating the geometric (mean) rate of return for a series of annual rates of return:rs=x~-1rs=x~-1where rsrs is average rate of return and x~x~ is the geometric mean of the returns during some number of time periods. Note that the length of each time period must be the same.As a general rule one should convert the percent values to its decimal equivalent multiplier. It is important to recognize that when dealing with percents, the geometric mean of percent values does not equal the geometric mean of the decimal multiplier equivalents and it is the decimal multiplier equivalent geometric mean that is relevant.   \n"}, {"chapter": "2.6", "title": "Skewness and the Mean, Median, and Mode", "mathml": "<math display=\"block\"><semantics><mrow><msub><mi>a</mi><mn>3</mn></msub><mo>=</mo><mo>\u00e2\u0088\u0091</mo><mfrac><msup><mrow><mo>(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>\u00e2\u0088\u0092</mo><mover><mi>x</mi><mo>\u00c2\u00af</mo></mover><mo>)</mo></mrow><mn>3</mn></msup><mrow><mi>n</mi><msup><mi>s</mi><mn>3</mn></msup></mrow></mfrac></mrow><annotation-xml encoding=\"MathML-Content\"><msub><mi>a</mi><mn>3</mn></msub><mo>=</mo><mo>\u00e2\u0088\u0091</mo><mfrac><msup><mrow><mo>(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>\u00e2\u0088\u0092</mo><mover><mi>x</mi><mo>\u00c2\u00af</mo></mover><mo>)</mo></mrow><mn>3</mn></msup><mrow><mi>n</mi><msup><mi>s</mi><mn>3</mn></msup></mrow></mfrac></annotation-xml></semantics></math></div", "content": "\nConsider the following data set.\n4; 5; 6; 6; 6; 7; 7; 7; 7; 7; 7; 8; 8; 8; 9; 10This data set can be represented by following histogram. Each interval has width one, and each value is located in the middle of an interval.\n\n\nFigure \n2.11\n\nThe histogram displays a symmetrical distribution of data. A distribution is symmetrical if a vertical line can be drawn at some point in the histogram such that the shape to the left and the right of the vertical line are mirror images of each other. The mean, the median, and the mode are each seven for these data. In a perfectly symmetrical distribution, the mean and the median are the same. This example has one mode (unimodal), and the mode is the same as the mean and median.  In a symmetrical distribution that has two modes (bimodal), the two modes would be different from the mean and median.\nThe histogram for the data: \n4; 5; 6; 6; 6; 7; 7; 7; 7; 8 shown in Figure 2.11 is not symmetrical. The right-hand side seems \"chopped off\" compared to the left side. A distribution of this type is called skewed to the left because it is pulled out to the left. We can formally measure the skewness of a distribution just as we can mathematically measure the center weight of the data or its general \"speadness\". The mathematical formula for skewness is: a3=\u00e2\u0088\u0091(xi\u00e2\u0088\u0092x\u00c2\u00af)3ns3a3=\u00e2\u0088\u0091(xi\u00e2\u0088\u0092x\u00c2\u00af)3ns3. The greater the deviation from zero indicates a greater degree of skewness. If the skewness is negative then the distribution is skewed left as in Figure 2.12. A positive measure of skewness indicates right skewness such as Figure 2.13. \n\nFigure \n2.12\n\nThe mean is 6.3, the median is 6.5, and the mode is seven.  Notice that the mean is less than the median, and they are both less than the mode. The mean and the median both reflect the skewing, but the mean reflects it more so.The histogram for the data:\n\n6; 7; 7; 7; 7; 8; 8; 8; 9; 10 shown in Figure 2.12, is also not symmetrical. It is skewed to the right.\n\n\nFigure \n2.13\n\nThe mean is 7.7, the median is 7.5, and the mode is seven. Of the three statistics, the mean is the largest, while the mode is the smallest. Again, the mean reflects the skewing the most.\nTo summarize, generally if the distribution of data is skewed to the left, the mean is less than the median, which is often less than the mode. If the distribution of data is skewed to the right, the mode is often less than the median, which is less than the mean.\nAs with the mean, median and mode, and as we will see shortly, the variance, there are mathematical formulas that give us precise measures of these characteristics of the distribution of the data. Again looking at the formula for skewness we see that this is a relationship between the mean of the data and the individual observations cubed.\na3=\u00e2\u0088\u0091(xi\u00e2\u0088\u0092x\u00c2\u00af)3ns3a3=\u00e2\u0088\u0091(xi\u00e2\u0088\u0092x\u00c2\u00af)3ns3where ss is the sample standard deviation of the data, XiXi , and x\u00c2\u00afx\u00c2\u00af is the arithmetic mean and nn is the sample size.Formally the arithmetic mean is known as the first moment of the distribution. The second moment we will see is the variance, and skewness is the third moment. The variance measures the squared differences of the data from the mean and skewness measures the cubed differences of the data from the mean. While a variance can never be a negative number, the measure of skewness can and this is how we determine if the data are skewed right of left. The skewness for a normal distribution is zero, and any symmetric data should have skewness near zero. Negative values for the skewness indicate data that are skewed left and positive values for the skewness indicate data that are skewed right. By skewed left, we mean that the left tail is long relative to the right tail. Similarly, skewed right means that the right tail is long relative to the left tail. The skewness characterizes the degree of asymmetry of a distribution around its mean. While the mean and standard deviation are dimensional quantities (this is why we will take the square root of the variance ) that is, have the same units as the measured quantities XiXi, the skewness is conventionally defined in such a way as to make it nondimensional. It is a pure number that characterizes only the shape of the distribution. A positive value of skewness signifies a distribution with an asymmetric tail extending out towards more positive X and a negative value signifies a distribution whose tail extends out towards more negative X. A zero measure of skewness will indicate a symmetrical distribution.Skewness and symmetry become important when we discuss probability distributions in later chapters. \n"}, {"chapter": "3.1", "title": "Terminology", "mathml": "<math display=\"block\"><semantics><mrow>\n<mfrac>\n<mrow>\n<mi>P</mi><mo>(</mo><mi>A</mi><mo>)</mo>\n</mrow>\n<mrow>\n<mn>1</mn><mo>\u00e2\u0088\u0092</mo><mi>P</mi><mo>(</mo><mi>A</mi><mo>)</mo>\n</mrow>\n</mfrac>\n</mrow><annotation-xml encoding=\"MathML-Content\"><mfrac>\n<mrow>\n<mi>P</mi><mo>(</mo><mi>A</mi><mo>)</mo>\n</mrow>\n<mrow>\n<mn>1</mn><mo>\u00e2\u0088\u0092</mo><mi>P</mi><mo>(</mo><mi>A</mi><mo>)</mo>\n</mrow>\n</mfrac></annotation-xml></semantics></math></div", "content": "\nProbability is a measure that is associated with how certain we are of outcomes of a particular experiment or activity. An experiment is a planned operation carried out under controlled conditions. If the result is not predetermined, then the experiment is said to be a chance experiment. Flipping one fair coin twice is an example of an experiment.\nA result of an experiment is called an outcome. The sample space of an experiment is the set of all possible outcomes. Three ways to represent a sample space are: to list the possible outcomes, to create a tree diagram, or to create a Venn diagram. The uppercase letter S is used to denote the sample space. For example, if you flip one fair coin, S = {H, T} where H = heads and T = tails are the outcomes.An event is any combination of outcomes. Upper case letters like A and B represent events. For example, if the experiment is to flip one fair coin, event A might be getting at most one head. The probability of an event A is written P(A).\nThe probability of any outcome is the long-term relative frequency of that outcome. Probabilities are between zero and one, inclusive (that is, zero and one and all numbers between these values). P(A) = 0 means the event A can never happen. P(A) = 1 means the event A always happens. P(A) = 0.5 means the event A is equally likely to occur or not to occur. For example, if you flip one fair coin repeatedly (from 20 to 2,000 to 20,000 times) the relative frequency of heads approaches 0.5 (the probability of heads).\nEqually likely means that each outcome of an experiment occurs with equal probability. For example, if you toss a fair, six-sided die, each face (1, 2, 3, 4, 5, or 6) is as likely to occur as any other face. If you toss a fair coin, a Head (H) and a Tail (T) are equally likely to occur. If you randomly guess the answer to a true/false question on an exam, you are equally likely to select a correct answer or an incorrect answer.\nTo calculate the probability of an event A when all outcomes in the sample space are equally likely, count the number of outcomes for event A and divide by the total number of outcomes in the sample space. For example, if you toss a fair dime and a fair nickel, the sample space is {HH, TH, HT, TT} where T = tails and H = heads. The sample space has four outcomes. A = getting one head. There are two outcomes that meet this condition {HT, TH}, so P(A) = \n\n\n2\n4\n\n\n\n\n2\n4\n\n = 0.5.Suppose you roll one fair six-sided die, with the numbers {1, 2, 3, 4, 5, 6} on its faces. Let event E = rolling a number that is at least five. There are two outcomes {5, 6}. P(E) = \n\n\n2\n6\n\n\n\n\n2\n6\n\n. If you were to roll the die only a few times, you would not be surprised if your observed results did not match the probability. If you were to roll the die a very large number of times, you would expect that, overall, 2626 of the rolls would result in an outcome of \"at least five\". You would not expect exactly 2626. The long-term relative frequency of obtaining this result would approach the theoretical probability of 2626 as the number of repetitions grows larger and larger.\nThis important characteristic of probability experiments is known as the law of large numbers which states that as the number of repetitions of an experiment is increased, the relative frequency obtained in the experiment tends to become closer and closer to the theoretical probability. Even though the outcomes do not happen according to any set pattern or order, overall, the long-term observed relative frequency will approach the theoretical probability. (The word empirical is often used instead of the word observed.)\nIt is important to realize that in many situations, the outcomes are not equally likely. A coin or die may be unfair, or biased. Two math professors in Europe had their statistics students test the Belgian one Euro coin and discovered that in 250 trials, a head was obtained 56% of the time and a tail was obtained 44% of the time. The data seem to show that the coin is not a fair coin; more repetitions would be helpful to draw a more accurate conclusion about such bias. Some dice may be biased. Look at the dice in a game you have at home; the spots on each face are usually small holes carved out and then painted to make the spots visible. Your dice may or may not be biased; it is possible that the outcomes may be affected by the slight weight differences due to the different numbers of holes in the faces. Gambling casinos make a lot of money depending on outcomes from rolling dice, so casino dice are made differently to eliminate bias. Casino dice have flat faces; the holes are completely filled with paint having the same density as the material that the dice are made out of so that each face is equally likely to occur. Later we will learn techniques to use to work with probabilities for events that are not equally likely.\n\"\u00e2\u0088\u00aa\u00e2\u0088\u00aa\" Event:  The UnionAn outcome is in the event A \u00e2\u0088\u00aa\u00e2\u0088\u00aa B if the outcome is in A or is in B or is in both A and B. For example, let A = {1, 2, 3, 4, 5} and B = {4, 5, 6, 7, 8}. A \u00e2\u0088\u00aa\u00e2\u0088\u00aa B = {1, 2, 3, 4, 5, 6, 7, 8}. Notice that 4 and 5 are NOT listed twice.\"\u00e2\u0088\u00a9\u00e2\u0088\u00a9\" Event: The IntersectionAn outcome is in the event A \u00e2\u0088\u00a9\u00e2\u0088\u00a9 B if the outcome is in both A and B at the same time. For example, let A and B be {1, 2, 3, 4, 5} and {4, 5, 6, 7, 8}, respectively. Then A \u00e2\u0088\u00a9\u00e2\u0088\u00a9 B = {4, 5}.The complement of event A is denoted A\u00e2\u0080\u00b2 (read \"A prime\"). A\u00e2\u0080\u00b2 consists of all outcomes that are NOT in A. Notice that P(A) + P(A\u00e2\u0080\u00b2) = 1. For example, let S = {1, 2, 3, 4, 5, 6} and let A = {1, 2, 3, 4}. Then, A\u00e2\u0080\u00b2 = {5, 6}. P(A) = 4646, P(A\u00e2\u0080\u00b2) = 2626, and P(A) + P(A\u00e2\u0080\u00b2) = \n\n\n4\n6\n\n + \n2\n6\n\n\n\n\n4\n6\n\n + \n2\n6\n\n = 1The conditional probability of A given B is written P(A||B). P(A||B) is the probability that event A will occur given that the event B has already occurred. A conditional reduces the sample space. We calculate the probability of A from the reduced sample space B. The formula to calculate P(A||B) is P(A||B) = \n\n\n\nP(A\u00e2\u0088\u00a9B)\n\n\nP(B)\n\n\n\n\n\n\nP(A\u00e2\u0088\u00a9B)\n\n\nP(B)\n\n\n where P(B) is greater than zero.For example, suppose we toss one fair, six-sided die. The sample space S = {1, 2, 3, 4, 5, 6}. Let A = face is 2 or 3 and B = face is even (2, 4, 6). To calculate P(A||B), we count the number of outcomes 2 or 3 in the sample space B = {2, 4, 6}. Then we divide that by the number of outcomes B (rather than S).We get the same result by using the formula. Remember that S has six outcomes.P(A||B) = \n\n\n\nP(A\u00e2\u0088\u00a9B)\n\n\nP(B)\n\n\n=\n\n\n\n(the number of outcomes that are 2 or 3 and even in S)\n\n6\n\n\n\n\n\n(the number of outcomes that are even in S)\n\n6\n\n\n\n=\n\n\n1\n6\n\n\n\n\n3\n6\n\n\n\n=\n1\n3\n\n\n\n\n\nP(A\u00e2\u0088\u00a9B)\n\n\nP(B)\n\n\n=\n\n\n\n(the number of outcomes that are 2 or 3 and even in S)\n\n6\n\n\n\n\n\n(the number of outcomes that are even in S)\n\n6\n\n\n\n=\n\n\n1\n6\n\n\n\n\n3\n6\n\n\n\n=\n1\n3\n\n\nOddsThe odds of an event presents the probability as a ratio of success to failure. This is common in various gambling formats. Mathematically, the odds of an event can be defined as:\n\n\nP(A)\n\n\n1\u00e2\u0088\u0092P(A)\n\n\n\n\nP(A)\n\n\n1\u00e2\u0088\u0092P(A)\n\nwhere P(A) is the probability of success and of course 1 \u00e2\u0088\u0092 P(A) is the probability of failure. Odds are always quoted as \"numerator to denominator,\" e.g. 2 to 1. Here the probability of winning is twice that of losing; thus, the probability of winning is 0.66. A probability of winning of 0.60 would generate odds in favor of winning of 3 to 2. While the calculation of odds can be useful in gambling venues in determining payoff amounts, it is not helpful for understanding probability or statistical theory.Understanding Terminology and SymbolsIt is important to read each problem carefully to think about and understand what the events are. Understanding the wording is the first very important step in solving probability problems. Reread the problem several times if necessary. Clearly identify the event of interest. Determine whether there is a condition stated in the wording that would indicate that the probability is conditional; carefully identify the condition, if any.\n\nExample \n3.1\n \n\n\n\n\nThe sample space S is the whole numbers starting at one and less than 20.\nS = _____________________________\nLet event A = the even numbers and event B = numbers greater than 13.\n\nA = _____________________, B = _____________________\nP(A) = _____________, P(B) = ________________\nA \u00e2\u0088\u00a9\u00e2\u0088\u00a9 B = ____________________, A OR B = ________________\nP(A \u00e2\u0088\u00a9\u00e2\u0088\u00a9 B) = _________, P(A \u00e2\u0088\u00aa\u00e2\u0088\u00aa B) = _____________\nA\u00e2\u0080\u00b2 = _____________, P(A\u00e2\u0080\u00b2) = _____________\nP(A) + P(A\u00e2\u0080\u00b2) = ____________\nP(A||B) = ___________, P(B||A) = _____________; are the probabilities equal?\n\n\n\n\nSolution\n1\n\n\nS = {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19}\nA = {2, 4, 6, 8, 10, 12, 14, 16, 18}, B = {14, 15, 16, 17, 18, 19}\nP(A) = 919\n919\n, P(B) = \n6\n\n19\n\n\n\n\n6\n\n19\n\n\n\nA \u00e2\u0088\u00a9\u00e2\u0088\u00a9 B = {14,16,18}, A OR B = {2, 4, 6, 8, 10, 12, 14, 15, 16, 17, 18, 19}\nP(A \u00e2\u0088\u00a9\u00e2\u0088\u00a9 B) = 319 319 , P(A \u00e2\u0088\u00aa\u00e2\u0088\u00aa B) = \n\n\n12\n\n19\n\n\n\n\n\n12\n\n19\n\n\n\nA\u00e2\u0080\u00b2 = 1, 3, 5, 7, 9, 11, 13, 15, 17, 19; P(A\u00e2\u0080\u00b2) = \n1019 1019 \nP(A) + P(A\u00e2\u0080\u00b2) = 1 (\n919 919  + \n1019 1019  = 1)\nP(A||B) = \n\n\n\nP(A\u00e2\u0088\u00a9B)\n\n\nP(B)\n\n\n\n\n\n\nP(A\u00e2\u0088\u00a9B)\n\n\nP(B)\n\n\n = \n\n\n3\n6\n\n\n\n\n3\n6\n\n, P(B||A) = \n\n\n\nP(A\u00e2\u0088\u00a9B)\n\n\nP(A)\n\n\n\n\n\n\nP(A\u00e2\u0088\u00a9B)\n\n\nP(A)\n\n\n = \n\n\n3\n9\n\n\n\n\n3\n9\n\n, No\n\n\n\n\n\nTry It \n3.1\n\n\n\n\nThe sample space S is all the ordered pairs of two whole numbers, the first from one to three and the second from one to four (Example: (1, 4)).S = _____________________________\nLet event A = the sum is even and event B = the first number is prime.\nA = _____________________, B = _____________________\nP(A) = _____________, P(B) = ________________\nA \u00e2\u0088\u00a9\u00e2\u0088\u00a9 B = ____________________, A \u00e2\u0088\u00aa\u00e2\u0088\u00aa B = ________________\nP(A \u00e2\u0088\u00a9\u00e2\u0088\u00a9 B) = _________, P(A \u00e2\u0088\u00aa\u00e2\u0088\u00aa B) = _____________\nB\u00e2\u0080\u00b2 = _____________, P(B\u00e2\u0080\u00b2) = _____________\nP(A) + P(A\u00e2\u0080\u00b2) = ____________\nP(A||B) = ___________, P(B||A) = _____________; are the probabilities equal?\n\n\n\nExample \n3.2\n \n\n\n\n\nA fair, six-sided die is rolled. Describe the sample space S, identify each of the following events with a subset of S and compute its probability (an outcome is the number of dots that show up).\nEvent T = the outcome is two.\nEvent A = the outcome is an even number.\nEvent B = the outcome is less than four.\nThe complement of A.\nA || B\nB || A\nA \u00e2\u0088\u00a9\u00e2\u0088\u00a9 B\nA \u00e2\u0088\u00aa\u00e2\u0088\u00aa B\nA \u00e2\u0088\u00aa\u00e2\u0088\u00aa B\u00e2\u0080\u00b2\nEvent N = the outcome is a prime number.\nEvent I = the outcome is seven.\n\n\n\n\nSolution\n1\n\n\nT = {2}, P(T) = 1616\nA = {2, 4, 6}, P(A) = 1212\nB = {1, 2, 3}, P(B) = 1212\nA\u00e2\u0080\u00b2 = {1, 3, 5}, P(A\u00e2\u0080\u00b2) = 1212\nA||B = {2}, P(A||B) = 1313\nB||A = {2}, P(B||A) = 1313\nA\u00e2\u0088\u00a9\u00e2\u0088\u00a9B = {2}, P(A \u00e2\u0088\u00a9\u00e2\u0088\u00a9 B) = 1616\nA\u00e2\u0088\u00aa\u00e2\u0088\u00aa B = {1, 2, 3, 4, 6}, P(A \u00e2\u0088\u00aa\u00e2\u0088\u00aa B) = 5656\nA \u00e2\u0088\u00aa\u00e2\u0088\u00aa B\u00e2\u0080\u00b2 = {2, 4, 5, 6}, P(A \u00e2\u0088\u00aa\u00e2\u0088\u00aa B\u00e2\u0080\u00b2) = 2323\nN = {2, 3, 5}, P(N) = 1212\nA six-sided die does not have seven dots. P(7) = 0.\n\n\n\n\n\n\nExample \n3.3\n \n\n\nTable 3.1 describes the distribution of a random sample S of 100 individuals, organized by gender and whether they are right- or left-handed.\n\n\n\nRight-handed\nLeft-handed\n\n\n\nMales\n43\n9\n\n\nFemales\n44\n4\n\n\nTable \n3.1\n \n \n\n\n\n\nLet\u00e2\u0080\u0099s denote the events M = the subject is male, F = the subject is female, R = the subject is right-handed, L = the subject is left-handed. Compute the following probabilities:P(M)\nP(F)\nP(R)\nP(L)\nP(M \u00e2\u0088\u00a9\u00e2\u0088\u00a9 R)\nP(F \u00e2\u0088\u00a9\u00e2\u0088\u00a9 L)\nP(M \u00e2\u0088\u00aa\u00e2\u0088\u00aa F)\nP(M \u00e2\u0088\u00aa\u00e2\u0088\u00aa R)\nP(F \u00e2\u0088\u00aa\u00e2\u0088\u00aa L)\nP(M')\nP(R||M)\nP(F||L)\nP(L||F)\n\n\n\nSolution\n1\n\n\nP(M) = 0.52\nP(F) = 0.48\nP(R) = 0.87\nP(L) = 0.13\nP(M \u00e2\u0088\u00a9\u00e2\u0088\u00a9 R) = 0.43\nP(F \u00e2\u0088\u00a9\u00e2\u0088\u00a9 L) = 0.04\nP(M \u00e2\u0088\u00aa\u00e2\u0088\u00aa F) = 1\nP(M \u00e2\u0088\u00aa\u00e2\u0088\u00aa R) = 0.96\nP(F \u00e2\u0088\u00aa\u00e2\u0088\u00aa L) = 0.57\nP(M') = 0.48\nP(R||M) = 0.8269 (rounded to four decimal places)\nP(F||L) = 0.3077 (rounded to four decimal places)\nP(L||F) = 0.0833\n\n\n\n\n"}, {"chapter": "4.1", "title": "Hypergeometric Distribution", "mathml": "<math display=\"block\"><semantics><mrow><mfenced><mtable><mtr><mtd><mn>4</mn></mtd></mtr><mtr><mtd><mn>2</mn></mtd></mtr></mtable></mfenced>\n<mo>=</mo><mfrac><mrow><mn>4</mn><mo>!</mo></mrow><mrow><mn>2</mn><mo>!</mo><mo>(</mo><mn>4</mn><mo>-</mo><mn>2</mn><mo>)</mo><mo>!</mo></mrow></mfrac><mo>=</mo><mn>6</mn></mrow><annotation-xml encoding=\"MathML-Content\"><mfenced><mtable><mtr><mtd><mn>4</mn></mtd></mtr><mtr><mtd><mn>2</mn></mtd></mtr></mtable></mfenced><mo>=</mo><mfrac><mrow><mn>4</mn><mo>!</mo></mrow><mrow><mn>2</mn><mo>!</mo><mo>(</mo><mn>4</mn><mo>-</mo><mn>2</mn><mo>)</mo><mo>!</mo></mrow></mfrac><mo>=</mo><mn>6</mn></annotation-xml></semantics></math></div", "content": "\nThe simplest probability density function is the hypergeometric. This is the most basic one because it is created by combining our knowledge of probabilities from Venn diagrams, the addition and multiplication rules, and the combinatorial counting formula.To find the number of ways to get 2 aces from the four in the deck we computed:42\n=4!2!(4-2)!=642=4!2!(4-2)!=6And if we did not care what else we had in our hand for the other three cards we would compute:483\n=48!3!45!=17,296483=48!3!45!=17,296Putting this together, we can compute the probability of getting exactly two aces in a 5 card poker hand as:\n42483525=.039942483525=.0399\nThis solution is really just the probability distribution known as the Hypergeometric. The generalized formula is:\nh(x)\n=\n\n\n\n\n\n\nA\n\n\n\n\nx\n\n\n\n\n\n\n\n\nN-A\n\n\n\n\nn-x\n\n\n\n\n\n\n\n\nNnh(x)=\n\n\n\n\n\nA\n\n\n\n\nx\n\n\n\n\n\n\n\n\nN-A\n\n\n\n\nn-x\n\n\n\n\n\n\n\n\nNn\nwhere x = the number we are interested in coming from the group with A objects.h(x) is the probability of x successes, in n attempts, \twhen A successes (aces in this case) are in a population that contains N elements. \nThe hypergeometric distribution is an example of a discrete probability distribution because there is no possibility of partial success, that is, there can be no poker hands with  2 1/2 aces. Said another way, a discrete random variable has to be a whole, or counting, number only. This probability distribution works in cases where the probability of a success changes with each draw. Another way of saying this is that the events are NOT independent. In using a deck of cards, we are sampling WITHOUT replacement. If we put each card back after it was drawn then the hypergeometric distribution be an inappropriate Pdf.\nFor the hypergeometric to work, \nthe population must be dividable into two and only two independent subsets (aces and non-aces in our example). The random variable X = the\nnumber of items from the group of interest. \nthe experiment must have changing probabilities of success with each experiment (the fact that cards are not replaced after the draw in our example makes this true in this case). Another way to say this is that you sample without replacement and therefore each pick is not independent. \nthe random variable must be discrete, rather than continuous. \n\n\nExample \n4.1\n \n\n\nA candy dish contains 30 jelly beans and 20 gumdrops. Ten candies are picked at random. What is the probability that 5 of the 10 are gumdrops? The two groups are jelly beans and gumdrops. Since the probability question asks for the probability of picking gumdrops, the group of interest (first group A in the formula) is gumdrops. The size of the group of interest (first group) is 30. The size of the second group is 20. The size of the sample is 10 (jelly beans or gumdrops). Let X = the number of gumdrops in the sample of 10. X takes on the values x = 0, 1, 2, ..., 10. a. What is the probability statement written mathematically? b. What is the hypergeometric probability density function written out to solve this problem? c. What is the answer to the question \"What is the probability of drawing 5 gumdrops in 10 picks from the dish?\"\n\n\n\nSolution\n1\n\n\na. P(x=5)P(x=5)  b. P(x=5)=(530)(520)(1050)P(x=5)=(530)(520)(1050)\n c. P(x=5)=0.215P(x=5)=0.215\n\n\n\n\n\nTry It \n4.1\n\n\n\nA bag contains letter tiles. Forty-four of the tiles are vowels, and 56 are consonants. Seven tiles are picked at random. You want to know the probability that four of the seven tiles are vowels. What is the group of interest, the size of the group of interest, and the size of the sample?\n\n"}, {"chapter": "5.3", "title": "The Exponential Distribution", "mathml": "<math display=\"block\"><semantics><mrow> <mi>f</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mfrac><mn>1</mn><mi>\u00ce\u00bc</mi></mfrac><msup><mi>e</mi><mrow><mo>\u00e2\u0088\u0092</mo><mfrac><mn>1</mn><mi>\u00ce\u00bc</mi></mfrac><mi>x</mi></mrow></msup></mrow><annotation-xml encoding=\"MathML-Content\"><mi>f</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mfrac><mn>1</mn><mi>\u00ce\u00bc</mi></mfrac><msup><mi>e</mi><mrow><mo>\u00e2\u0088\u0092</mo><mfrac><mn>1</mn><mi>\u00ce\u00bc</mi></mfrac><mi>x</mi></mrow></msup></annotation-xml></semantics></math></div", "content": "\nThe exponential distribution is often concerned with the amount of time until some specific event occurs. For example, the amount of time (beginning now) until an earthquake occurs has an exponential distribution. Other examples include the length of time, in minutes, of long distance business telephone calls, and the amount of time, in months, a car battery lasts. It can be shown, too, that the value of the change that you have in your pocket or purse approximately follows an exponential distribution.\nValues for an exponential random variable occur in the following way. There are fewer large values and more small values. For example, marketing studies have shown that the amount of money customers spend in one trip to the supermarket follows an exponential distribution. There are more people who spend small amounts of money and fewer people who spend large amounts of money.Exponential distributions are commonly used in calculations of product reliability, or the length of time a product lasts.\nThe random variable for the exponential distribution is continuous and often measures a passage of time, although it can be used in other applications. Typical questions may be, \u00e2\u0080\u009cwhat is the probability that some event will occur within the next xx hours or days, or what is the probability that some event will occur between x1x1 hours and x2x2 hours, or what is the probability that the event will take more than x1x1 hours to perform?\u00e2\u0080\u009d In short, the random variable X equals (a) the time between events or (b) the passage of time to complete an action, e.g. wait on a customer. The probability density function is given by: f(x)=1\u00ce\u00bce\u00e2\u0088\u00921\u00ce\u00bcxf(x)=1\u00ce\u00bce\u00e2\u0088\u00921\u00ce\u00bcx\nwhere \u00ce\u00bc is the historical average waiting time.and has a mean and standard deviation of 1/\u00ce\u00bc.An alternative form of the exponential distribution formula recognizes what is often called the decay factor. The decay factor simply measures how rapidly the probability of an event declines as the random variable X increases. When the notation using the decay parameter m is used, the probability density function is presented as:\nf(x)\u00a0=\u00a0me\u00e2\u0088\u0092mxf(x)\u00a0=\u00a0me\u00e2\u0088\u0092mxwhere m=1\u00ce\u00bcm=1\u00ce\u00bcIn order to calculate probabilities for specific probability density functions, the cumulative density function is used. The cumulative density function (cdf) is simply the integral of the pdf and is:F(x)=\u00e2\u0088\u00ab0\u00e2\u0088\u009e[1\u00ce\u00bce-x\u00ce\u00bc]=1-e-x\u00ce\u00bcF(x)=\u00e2\u0088\u00ab0\u00e2\u0088\u009e[1\u00ce\u00bce-x\u00ce\u00bc]=1-e-x\u00ce\u00bc\nExample \n5.3\n \n\nLet X = amount of time (in minutes) a postal clerk spends with a customer. The time is known from historical data to have an average amount of time equal to four minutes.It is given that \u00ce\u00bc = 4 minutes, that is, the average time the clerk spends with a customer is 4 minutes. Remember that we are still doing probability and thus we have to be told the population parameters such as the mean. To do any calculations, we need to know the mean of the distribution: the historical time to provide a service, for example. Knowing the historical mean allows the calculation of the decay parameter, m.\nm=1\u00ce\u00bcm=1\u00ce\u00bc. Therefore, m=14=0.25m=14=0.25.\nWhen the notation used the decay parameter, m, the probability density function is presented as f(x)\u00a0=\u00a0me\u00e2\u0088\u0092mxf(x)\u00a0=\u00a0me\u00e2\u0088\u0092mx, which is simply the original formula with m substituted for 1\u00ce\u00bc1\u00ce\u00bc, or  f(x)=1\u00ce\u00bce\u00e2\u0088\u00921\u00ce\u00bcxf(x)=1\u00ce\u00bce\u00e2\u0088\u00921\u00ce\u00bcx.To calculate probabilities for an exponential probability density function, we need to use the cumulative density function. As shown below, the curve for the cumulative density function is:f(x) = 0.25e\u00e2\u0080\u00930.25x where x is at least zero and m = 0.25.For example, f(5) = 0.25e(-0.25)(5) = 0.072. In other words, the function has a value of .072 when x = 5.The graph is as follows:\n\n\n\n\n\nFigure \n5.13\n\n\nNotice the graph is a declining curve. When x = 0,\nf(x) = 0.25e(\u00e2\u0088\u00920.25)(0) = (0.25)(1) = 0.25 = m. The maximum value on the y-axis is always m, one divided by the mean.\nTry It \n5.3\n\n\n\n\nThe amount of time spouses shop for anniversary cards can be modeled by an exponential distribution with the average amount of time equal to eight minutes. Write the distribution, state the probability density function, and graph the distribution.\n\n\n\nExample \n5.4\n \n\n\n\na. Using the information in Example 5.3, find the probability that a clerk spends four to five minutes with a randomly selected customer.\n\n\n\nSolution\n1\n\n\na. Find P(4 < x < 5).\nThe cumulative distribution function (CDF) gives the area to the left.\nP(x < x) = 1 \u00e2\u0080\u0093 e\u00e2\u0080\u0093mx\nP(x < 5) = 1 \u00e2\u0080\u0093 e(\u00e2\u0080\u00930.25)(5) = 0.7135 and P(x < 4) = 1 \u00e2\u0080\u0093 e(\u00e2\u0080\u00930.25)(4) = 0.6321\nP(4 < x < 5)= 0.7135 \u00e2\u0080\u0093 0.6321 = 0.0814\n\n\n\n\nFigure \n5.14\n\n\n\n\n\n\n\nTry It \n5.4\n\n\n\n\nThe number of days ahead travelers purchase their airline tickets can be modeled by an exponential distribution with the average amount of time equal to 15 days. Find the probability that a traveler will purchase a ticket fewer than ten days in advance. How many days do half of all travelers wait?\n\n\n\n\nExample \n5.5\n \n\n\nOn the average, a certain computer part lasts ten years. The length of time the computer part lasts is exponentially distributed.\n\na. What is the probability that a computer part lasts more than 7 years?\n\n\n\n\nSolution\n1\n\n\na. Let x = the amount of time (in years) a computer part lasts.\n\u00ce\u00bc = 10 so \n\nm=\n1\n\u00ce\u00bc\n\n=\n1\n\n10\n\n\n=0.1\n\n\nm=\n1\n\u00ce\u00bc\n\n=\n1\n\n10\n\n\n=0.1\n\nFind P(x > 7). Draw the graph.\nP(x > 7) = 1 \u00e2\u0080\u0093 P(x < 7).\nSince P(X < x) = 1 \u00e2\u0080\u0093 e\u00e2\u0080\u0093mx then P(X > x) = 1 \u00e2\u0080\u0093 ( 1 \u00e2\u0080\u0093e\u00e2\u0080\u0093mx) = e\u00e2\u0080\u0093mx\nP(x > 7) = e(\u00e2\u0080\u00930.1)(7) = 0.4966. The probability that a computer part lasts more than seven years is 0.4966.\n\n\n\n\n\n\n\nFigure \n5.15\n\n\n\nb. On the average, how long would five computer parts last if they are used one after another?\n\n\n\n\nSolution\n2\n\n\nb. On the average, one computer part lasts ten years. Therefore, five computer parts, if they are used one right after the other would last, on the average, (5)(10) = 50 years.\n\n\n\n\nd. What is the probability that a computer part lasts between nine and 11 years?\n\n\n\n\nSolution\n3\n\n\nd. Find P(9 < x < 11). Draw the graph.\n\n\n\n\n\nFigure \n5.16\n\n\nP(9 < x < 11) = P(x < 11) \u00e2\u0080\u0093 P(x < 9) = (1 \u00e2\u0080\u0093 e(\u00e2\u0080\u00930.1)(11)) \u00e2\u0080\u0093 (1 \u00e2\u0080\u0093 e(\u00e2\u0080\u00930.1)(9)) = 0.6671 \u00e2\u0080\u0093 0.5934 = 0.0737. The probability that a computer part lasts between nine and 11 years is 0.0737.\n\n\n\n\n\nTry It \n5.5\n\n\n\n\nOn average, a pair of running shoes can last 18 months if used every day. The length of time running shoes last is exponentially distributed. What is the probability that a pair of running shoes last more than 15 months? On average, how long would six pairs of running shoes last if they are used one after the other? Eighty percent of running shoes last at most how long if used every day?\n\n\n\n\nExample \n5.6\n \n\n\nSuppose that the length of a phone call, in minutes, is an exponential random variable with decay parameter \n\n\n1\n\n12\n\n\n\n\n\n1\n\n12\n\n\n. The decay p[parameter is another way to view 1/\u00ce\u00bb. If another person arrives at a public telephone just before you, find the probability that you will have to wait more than five minutes. Let X  = the length of a phone call, in minutes.\n\nWhat is m, \u00ce\u00bc, and \u00cf\u0083? The probability that you must wait more than five minutes is _______ .\n\n\n\n\nSolution\n1\n\n\n\nm = \n\n\n1\n\n12\n\n\n\n\n\n1\n\n12\n\n\n\n\u00ce\u00bc = 12\n\u00cf\u0083 = 12\n\nP(x > 5) = 0.6592\n\n\n\n\n\n\nExample \n5.7\n \n\n\nThe time spent waiting between events is often modeled using the exponential distribution. For example, suppose that an average of 30 customers per hour arrive at a store and the time between arrivals is exponentially distributed.\nOn average, how many minutes elapse between two successive arrivals?\nWhen the store first opens, how long on average does it take for three customers to arrive?\nAfter a customer arrives, find the probability that it takes less than one minute for the next customer to arrive.\nAfter a customer arrives, find the probability that it takes more than five minutes for the next customer to arrive.\nIs an exponential distribution reasonable for this situation?\n\n\n\n\nSolution\n1\n\n\nSince we expect 30 customers to arrive per hour (60 minutes), we expect on average one customer to arrive every two minutes on average.\nSince one customer arrives every two minutes on average, it will take six minutes on average for three customers to arrive.\nLet X = the time between arrivals, in minutes. By part a, \u00ce\u00bc = 2, so m = \n\n\n1\n2\n\n\n\n\n1\n2\n\n = 0.5.\nThe cumulative distribution function is P(X < x) = 1 \u00e2\u0080\u0093 e(-0.5)(x)\nTherefore P(X < 1) = 1 \u00e2\u0080\u0093 e(\u00e2\u0080\u00930.5)(1) = 0.3935.\n\n\n\n\n\n\nFigure \n5.17\n\n\n\nP(X > 5) = 1 \u00e2\u0080\u0093 P(X < 5) = 1 \u00e2\u0080\u0093 (1 \u00e2\u0080\u0093 e(-0.5)(5)) = e\u00e2\u0080\u00932.5 \u00e2\u0089\u0088 0.0821.\n\n\n\n\n\nFigure \n5.18\n\n\n\nThis model assumes that a single customer arrives at a time, which may not be reasonable since people might shop in groups, leading to several customers arriving at the same time. It also assumes that the flow of customers does not change throughout the day, which is not valid if some times of the day are busier than others.\n\n\n\n\nMemorylessness of the Exponential Distribution\nRecall that the amount of time between customers for the postal clerk discussed earlier is exponentially distributed with a mean of two minutes. Suppose that five minutes have elapsed since the last customer arrived. Since an unusually long amount of time has now elapsed, it would seem to be more likely for a customer to arrive within the next minute. With the exponential distribution, this is not the case\u00e2\u0080\u0093the additional time spent waiting for the next customer does not depend on how much time has already elapsed since the last customer. This is referred to as the memoryless property. The exponential and geometric probability density functions are the only probability functions that have the memoryless property. Specifically, the memoryless property says thatP (X > r + t | X > r) = P (X > t) for all r \u00e2\u0089\u00a5 0 and t \u00e2\u0089\u00a5 0\nFor example, if five minutes have elapsed since the last customer arrived, then the probability that more than one minute will elapse before the next customer arrives is computed by using r = 5 and t = 1 in the foregoing equation.P(X > 5 + 1 | X > 5) = P(X > 1) = \n\n\ne\n\n(\n\n\u00e2\u0080\u00930.5\n\n)(\n1\n)\n\n\n\n\n\ne\n\n(\n\n\u00e2\u0080\u00930.5\n\n)(\n1\n)\n\n\n = 0.6065.This is the same probability as that of waiting more than one minute for a customer to arrive after the previous arrival.\nThe exponential distribution is often used to model the longevity of an electrical or mechanical device. In Example 5.5, the lifetime of a certain computer part has the exponential distribution with a mean of ten years. The memoryless property says that knowledge of what has occurred in the past has no effect on future probabilities. In this case it means that an old part is not any more likely to break down at any particular time than a brand new part. In other words, the part stays as good as new until it suddenly breaks. For example, if the part has already lasted ten years, then the probability that it lasts another seven years is P(X > 17|X > 10) = P(X > 7) = 0.4966, where the vertical line is read as \"given\".\n\nExample \n5.8\n \n\n\nRefer back to the postal clerk again where the time a postal clerk spends with his or her customer has an exponential distribution with a mean of four minutes. Suppose a customer has spent four minutes with a postal clerk. What is the probability that he or she will spend at least an additional three minutes with the postal clerk?The decay parameter of X is m = \n\n\n1\n4\n\n\n\n\n1\n4\n\n\n= 0.25, so X \u00e2\u0088\u00bc Exp(0.25).\nThe cumulative distribution function is P(X < x) = 1 \u00e2\u0080\u0093 e\u00e2\u0080\u00930.25x.\n\nWe want to find P(X > 7|X > 4). The memoryless property says that P(X > 7|X > 4) = P (X > 3), so we just need to find the probability that a customer spends more than three minutes with a postal clerk.\nThis is P(X > 3) = 1 \u00e2\u0080\u0093 P (X < 3) = 1 \u00e2\u0080\u0093 (1 \u00e2\u0080\u0093 e\u00e2\u0080\u00930.25\u00e2\u008b\u00853) = e\u00e2\u0080\u00930.75 \u00e2\u0089\u0088 0.4724.\n\n\n\n\n\nFigure \n5.19\n\n\n\nRelationship between the Poisson and the Exponential Distribution\nThere is an interesting relationship between the exponential distribution and the Poisson distribution. Suppose that the time that elapses between two successive events follows the exponential distribution with a mean of \u00ce\u00bc units of time. Also assume that these times are independent, meaning that the time between events is not affected by the times between previous events. If these assumptions hold, then the number of events per unit time follows a Poisson distribution with mean \u00ce\u00bc. Recall that if X has the Poisson distribution with mean \u00ce\u00bc, then \n\nP(X=x)=\n\n\n\u00ce\u00bc\nx\n\n\ne\n\n\u00e2\u0088\u0092\u00ce\u00bc\n\n\n\n\nx!\n\n\n\n\nP(X=x)=\n\n\n\u00ce\u00bc\nx\n\n\ne\n\n\u00e2\u0088\u0092\u00ce\u00bc\n\n\n\n\nx!\n\n\n. The formula for the exponential distribution:  P(X=x)=me-mx=1\u00ce\u00bce-1\u00ce\u00bcxP(X=x)=me-mx=1\u00ce\u00bce-1\u00ce\u00bcx  Where m = the rate parameter, or \u00ce\u00bc = average time between occurrences. We see that the exponential is the cousin of the Poisson distribution and they are linked through this formula. There are important differences that make each distribution relevant for different types of probability problems.First, the Poisson has a discrete random variable, x, where time; a continuous variable is artificially broken into discrete pieces. We saw that the number of occurrences of an event in a given time interval, x, follows the Poisson distribution. For example, the number of times the telephone rings per hour. By contrast, the time between occurrences follows the exponential distribution. For example. The telephone just rang, how long will it be until it rings again? We are measuring length of time of the interval, a continuous random variable, exponential, not events during an interval, Poisson.The Exponential Distribution v. the Poisson Distribution\nA visual way to show both the similarities and differences between these two distributions is with a time line.\n\n\n\nFigure \n5.20\n\nThe random variable for the Poisson distribution is discrete and thus counts events during a given time period, t1 to t2 on Figure 5.20, and calculates the probability of that number occurring. The number of events, four in the graph, is measured in counting numbers; therefore, the random variable of the Poisson is a discrete random variable.The exponential probability distribution calculates probabilities of the passage of time, a continuous random variable. In Figure 5.20 this is shown as the bracket from t1 to the next occurrence of the event marked with a triangle.Classic Poisson distribution questions are \"how many people will arrive at my checkout window in the next hour?\".Classic exponential distribution questions are \"how long it will be until the next person arrives,\" or a variant, \"how long will the person remain here once they have arrived?\".Again, the formula for the exponential distribution is:f(x)=me-mx  or  f(x)=1\u00ce\u00bce-1\u00ce\u00bcxf(x)=me-mx  or  f(x)=1\u00ce\u00bce-1\u00ce\u00bcxWe see immediately the similarity between the exponential formula and the Poisson formula.P(x) =\n\n\n\u00ce\u00bc\nx\n\n\ne\n\n\u00e2\u0088\u0092\u00ce\u00bc\n\n\n\n\nx!\n\n\nP(x)=\n\n\n\u00ce\u00bc\nx\n\n\ne\n\n\u00e2\u0088\u0092\u00ce\u00bc\n\n\n\n\nx!\n\nBoth probability density functions are based upon the relationship between time and exponential growth or decay. The \u00e2\u0080\u009ce\u00e2\u0080\u009d in the formula is a constant with the approximate value  of 2.71828 and is the base of the natural logarithmic exponential growth formula. When people say that something has grown exponentially this is what they are talking about. An example of the exponential and the Poisson will make clear the differences been the two. It will also show the interesting applications they have. Poisson DistributionSuppose that historically 10 customers arrive at the checkout lines each hour. Remember that this is still probability so we have to be told these historical values. We see this is a Poisson probability problem.We can put this information into the Poisson probability density function and get a general formula that will calculate the probability of any specific number of customers arriving in the next hour. The formula is for any value of the random variable we chose, and so the x is put into the formula. This is the formula: \nf\n(x)\n=\n\n\n10x\ne-10\n\n\nx!\n\n\nf(x)=\n\n10x\ne-10\n\n\nx!\n\nAs an example, the probability of  15 people arriving at the checkout counter in the next hour would be\nP\n(x=15)\n=\n\n\n1015\ne-10\n\n\n15!\n\n\n=\n0.0611\nP(x=15)=\n\n1015\ne-10\n\n\n15!\n\n=0.0611\nHere we have inserted x = 15 and calculated the probability that in the next hour 15 people will arrive is .061. Exponential DistributionIf we keep the same historical facts that 10 customers arrive each hour, but we now are interested in the service time a person spends at the counter, then we would use the exponential distribution. The exponential probability function for any value of x, the random variable, for this particular checkout counter historical data is:\nf(x)\n=\n\n1\n.1\n\n\ne\n\n-x.1\n\n\n=\n10\n\ne\n\n-10x\n\n\nf(x)=\n1\n.1\n\ne\n\n-x.1\n\n=10\ne\n\n-10x\n\n\nTo calculate \u00c2\u00b5, the historical average service time, we simply divide the number of people that arrive per hour, 10 , into the time period, one hour, and have \u00c2\u00b5 = 0.1. Historically, people spend 0.1 of an hour at the checkout counter, or 6 minutes. This explains the .1 in the formula.There is a natural confusion with \u00c2\u00b5 in both the Poisson and exponential formulas. They have different meanings, although they have the same symbol. The mean of the exponential is one divided by the mean of the Poisson. If you are given the historical number of arrivals you have the mean of the Poisson. If you are given an historical length of time between events you have the mean of an exponential.  Continuing with our example at the checkout clerk; if we wanted to know the probability that a person would spend 9 minutes or less checking out, then we use this formula. First, we convert to the same time units which are parts of one hour.  Nine minutes is 0.15 of one hour. Next we note that we are asking for a range of values. This is always the case for a continuous random variable. We write the probability question as: \np(x\u00e2\u0089\u00a49)\n=\n1\n-\n10\n\ne\n\n-10\nx\n\n\np(x\u00e2\u0089\u00a49)=1-10\ne\n\n-10\nx\n\nWe can now put the numbers into the formula and we have our result. \np(x=.15)\n=\n1-10\ne\n-10(.15)\n\n=\n0.7769\np(x=.15)=1-10e\n-10(.15)\n=0.7769\nThe probability that a customer will spend 9 minutes or less checking out is 0.7769. We see that we have a high probability of getting out in less than nine minutes and a tiny probability of having 15 customers arriving in the next hour.\n\n"}, {"chapter": "7.1", "title": "The Central Limit Theorem for Sample Means", "mathml": "<math display=\"block\"><semantics><mrow>\n<mrow>\n<mi>Z</mi><mo>=</mo><mfrac>\n<mrow>\n<mover>\n<mi>X</mi>\n<mo>\u00e2\u0080\u0093</mo>\n</mover>\n<mo>\u00e2\u0088\u0092</mo><msub>\n<mi>\u00ce\u00bc</mi>\n<mover>\n<mi>X</mi>\n<mo>\u00e2\u0080\u0093</mo>\n</mover>\n</msub>\n</mrow>\n<mrow>\n<msub>\n<mi>\u00cf\u0083</mi>\n<mover>\n<mi>X</mi>\n<mo>\u00e2\u0080\u0093</mo>\n</mover>\n</msub>\n</mrow>\n</mfrac>\n</mrow>\n<mo>=</mo>\n<mfrac>\n<mrow>\n<mover>\n<mi>X</mi>\n<mo>\u00e2\u0080\u0093</mo>\n</mover>\n<mo>-</mo>\n<mi>\u00ce\u00bc</mi></mrow>\n<mrow>\n<mfrac><mrow>\n<mi>\u00cf\u0083</mi>\n</mrow><mrow>\n<msqrt><mi>n</mi></msqrt></mrow></mfrac></mrow></mfrac>\n</mrow><annotation-xml encoding=\"MathML-Content\"><mrow>\n<mi>Z</mi><mo>=</mo><mfrac>\n<mrow>\n<mover>\n<mi>X</mi>\n<mo>\u00e2\u0080\u0093</mo>\n</mover>\n<mo>\u00e2\u0088\u0092</mo><msub>\n<mi>\u00ce\u00bc</mi>\n<mover>\n<mi>X</mi>\n<mo>\u00e2\u0080\u0093</mo>\n</mover>\n</msub>\n</mrow>\n<mrow>\n<msub>\n<mi>\u00cf\u0083</mi>\n<mover>\n<mi>X</mi>\n<mo>\u00e2\u0080\u0093</mo>\n</mover>\n</msub>\n</mrow>\n</mfrac>\n</mrow><mo>=</mo><mfrac>\n<mrow>\n<mover>\n<mi>X</mi>\n<mo>\u00e2\u0080\u0093</mo>\n</mover>\n<mo>-</mo>\n<mi>\u00ce\u00bc</mi></mrow>\n<mrow>\n<mfrac><mrow>\n<mi>\u00cf\u0083</mi>\n</mrow><mrow>\n<msqrt><mi>n</mi></msqrt></mrow></mfrac></mrow></mfrac></annotation-xml></semantics></math></div", "content": "\nThe sampling distribution is a theoretical distribution. It is created by taking many samples of size n from a population. Each sample mean is then treated like a single observation of this new distribution, the sampling distribution. The genius of thinking this way is that it recognizes that when we sample we are creating an observation and that observation must come from some particular distribution. The Central Limit Theorem answers the question: from what distribution did a sample mean come? If this is discovered, then we can treat a sample mean just like any other observation and calculate probabilities about what values it might take on. We have effectively moved from the world of statistics where we know only what we have from the sample, to the world of probability where we know the distribution from which the sample mean came and the parameters of that distribution. The reasons that one samples a population are obvious. The time and expense of checking every invoice to determine its validity or every shipment to see if it contains all the items may well exceed the cost of errors in billing or shipping. For some products, sampling would require destroying them, called destructive sampling. One such example is measuring the ability of a metal to withstand saltwater corrosion for parts on ocean going vessels.Sampling thus raises an important question; just which sample was drawn. Even if the sample were randomly drawn, there are theoretically an almost infinite number of samples. With just 100 items, there are more than 75 million unique samples of size five that can be drawn. If six are in the sample, the number of possible samples increases to just more than one billion.  Of the 75 million possible samples, then, which one did you get? If there is variation in the items to be sampled, there will be variation in the samples. One could draw an \"unlucky\" sample and make very wrong conclusions concerning the population. This recognition that any sample we draw is really only one from a distribution of samples provides us with what is probably the single most important theorem is statistics: the Central Limit Theorem. Without the Central Limit Theorem it would be impossible to proceed to inferential statistics from simple probability theory. In its most basic form, the Central Limit Theorem states that regardless of the underlying probability density function of the population data, the theoretical distribution of the means of samples from the population will be normally distributed. In essence, this says that the mean of a sample should be treated like an observation drawn from a normal distribution. The Central Limit Theorem only holds if the sample size is \"large enough\" which has been shown to be only 30 observations or more. Figure 7.2 graphically displays this very important proposition. \nFigure \n7.2\n\nNotice that the horizontal axis in the top panel is labeled X. These are the individual observations of the population. This is the unknown distribution of the population values. The graph is purposefully drawn all squiggly to show that it does not matter just how odd ball it really is. Remember, we will never know what this distribution looks like, or its mean or standard deviation for that matter.The horizontal axis in the bottom panel is labeled \nX\u00e2\u0080\u0093\n\nX\u00e2\u0080\u0093\n's.  This is the theoretical distribution called the sampling distribution of the means. Each observation on this distribution is a sample mean. All these sample means were calculated from individual samples with the same sample size.  The theoretical sampling distribution contains all of the sample mean values from all the possible samples that could have been taken from the population. Of course, no one would ever actually take all of these samples, but if they did this is how they would look. And the Central Limit Theorem says that they will be normally distributed.The Central Limit Theorem goes even further and tells us the mean and standard deviation of this theoretical distribution. \n\nParameter\nPopulation distribution\nSample\nSampling distribution of \n\nX\n\u00e2\u0080\u0093\n\n\nX\n\u00e2\u0080\u0093\n's\n\n\n\nMean\n\u00ce\u00bc\nX\u00e2\u0080\u0093X\u00e2\u0080\u0093\n\u00ce\u00bcx\u00e2\u0080\u0093and E(\u00ce\u00bcx\u00e2\u0080\u0093)=\u00ce\u00bc\u00ce\u00bcx\u00e2\u0080\u0093and E(\u00ce\u00bcx\u00e2\u0080\u0093)=\u00ce\u00bc\n\n\nStandard deviation\n\u00cf\u0083\ns\n\u00cf\u0083x\u00e2\u0080\u0093=\u00cf\u0083n\u00cf\u0083x\u00e2\u0080\u0093=\u00cf\u0083n\n\n\nTable \n7.1\n \n \n\nThe practical significance of The Central Limit Theorem is that now we can compute probabilities for drawing a sample mean, \n\nX\n\u00e2\u0080\u0093\n\n\nX\n\u00e2\u0080\u0093\n, in just the same way as we did for drawing specific observations, X's, when we knew the population mean and standard deviation and that the population data were normally distributed.. The standardizing formula has to be amended to recognize that the mean and standard deviation of the sampling distribution, sometimes, called the standard error of the mean, are different from those of the population distribution, but otherwise nothing has changed. The new standardizing formula is\n\n\nZ=\n\n\nX\n\u00e2\u0080\u0093\n\n\u00e2\u0088\u0092\n\u00ce\u00bc\n\nX\n\u00e2\u0080\u0093\n\n\n\n\n\n\u00cf\u0083\n\nX\n\u00e2\u0080\u0093\n\n\n\n\n\n=\n\n\n\nX\n\u00e2\u0080\u0093\n\n-\n\u00ce\u00bc\n\n\n\u00cf\u0083\n\nn\n\nZ=\n\n\nX\n\u00e2\u0080\u0093\n\n\u00e2\u0088\u0092\n\u00ce\u00bc\n\nX\n\u00e2\u0080\u0093\n\n\n\n\n\n\u00cf\u0083\n\nX\n\u00e2\u0080\u0093\n\n\n\n\n=\n\n\nX\n\u00e2\u0080\u0093\n\n-\n\u00ce\u00bc\n\n\n\u00cf\u0083\n\nnNotice that \u00c2\u00b5\n\nX\n\u00e2\u0080\u0093\n\n\u00c2\u00b5\n\nX\n\u00e2\u0080\u0093\n in the first formula has been changed to simply \u00c2\u00b5 in the second version. The reason is that mathematically it can be shown that the expected value of  \u00c2\u00b5\n\nX\n\u00e2\u0080\u0093\n\n\u00c2\u00b5\n\nX\n\u00e2\u0080\u0093\n is equal to \u00c2\u00b5. This was stated in Table 7.1 above. Mathematically, the E(x) symbol read the \u00e2\u0080\u009cexpected value of x\u00e2\u0080\u009d.  This formula will be used in the next unit to provide estimates of the unknown population parameter \u00ce\u00bc.\n"}, {"chapter": "7.3", "title": "The Central Limit Theorem for Proportions", "mathml": "<math display=\"block\"><semantics><mrow><mi>E</mi><mo stretchy=\"false\">(</mo><mi>p</mi><mo>'</mo><mo stretchy=\"false\">)</mo><mo>=</mo><mi>E</mi><mo>(</mo><mfrac><mi>x</mi><mi>n</mi></mfrac><mo>)</mo><mo>=</mo><mo>(</mo><mfrac><mn>1</mn><mi>n</mi></mfrac><mo>)</mo><mi>E</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mo>(</mo><mfrac><mn>1</mn><mi>n</mi></mfrac><mo>)</mo><mi>n</mi><mi>p</mi><mo>=</mo><mi>p</mi></mrow><annotation-xml encoding=\"MathML-Content\"><mi>E</mi><mo stretchy=\"false\">(</mo><mi>p</mi><mo>'</mo><mo stretchy=\"false\">)</mo><mo>=</mo><mi>E</mi><mo>(</mo><mfrac><mi>x</mi><mi>n</mi></mfrac><mo>)</mo><mo>=</mo><mo>(</mo><mfrac><mn>1</mn><mi>n</mi></mfrac><mo>)</mo><mi>E</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mo>(</mo><mfrac><mn>1</mn><mi>n</mi></mfrac><mo>)</mo><mi>n</mi><mi>p</mi><mo>=</mo><mi>p</mi></annotation-xml></semantics></math></div", "content": "\nThe Central Limit Theorem tells us that the point estimate for the sample mean, x\u00c2\u00afx\u00c2\u00af, comes from a normal distribution of x\u00c2\u00afx\u00c2\u00af's. This theoretical distribution is called the sampling distribution of x\u00c2\u00afx\u00c2\u00af's. We now investigate the sampling distribution for another important parameter we wish to estimate; p from the binomial probability density function.\nIf the random variable is discrete, such as for categorical data, then the parameter we wish to estimate is the population proportion. This is, of course, the probability of drawing a success in any one random draw. Unlike the case just discussed for a continuous random variable where we did not know the population \ndistribution of X's, here we actually know the underlying probability density function for these data; it is the binomial. The random variable is X = the number of successes and the parameter we wish to know is p, the probability of drawing a \nsuccess which is of course the proportion of successes in the population. The question at issue is: from what distribution was the sample proportion, p'=xnp'=xn drawn? The sample size is n and X is the number of successes found in that sample. This is a parallel question that was just answered by the Central Limit Theorem: from what distribution was the sample mean, x\u00c2\u00afx\u00c2\u00af, drawn? We saw that once we knew that the distribution was the Normal distribution then we were able to create confidence intervals for the population parameter, \u00c2\u00b5. We will also use this same information to test hypotheses about the population mean later. We wish now to be able to develop confidence intervals for the population parameter \"p\" from the binomial probability density function. In order to find the distribution from which sample proportions come we need to develop the sampling distribution of sample proportions just as we did for sample means. So again imagine that we randomly sample say 50 people and ask them if they support the new school bond issue. From this we find a sample proportion, p', and graph it on the axis of p's. We do this again and again etc., etc. until we have the theoretical distribution of p's. Some sample proportions will show high favorability toward the bond issue and others will show low favorability because random sampling will reflect the variation of views within the population. What we have done can be seen in Figure 7.9. The top panel is the population distributions of probabilities for each possible value of the random variable X. While we do not know what the specific distribution looks like because we do not know p, the population parameter, we do know that it must look something like this. In reality, we do not know either the mean or the standard deviation of this population distribution, the same difficulty we faced when analyzing the X's previously. \nFigure \n7.9\n\nFigure 7.9 places the mean on the distribution of population probabilities as \u00c2\u00b5=np\u00c2\u00b5=np but of course we do not actually know the population mean because we do not know the population probability of success, pp. Below the distribution of the population values is the sampling distribution of pp's. Again the Central Limit Theorem tells us that this distribution is normally distributed just like the case of the sampling distribution for  x\u00c2\u00afx\u00c2\u00af's. This sampling distribution also has a mean, the mean of the pp's, and a standard deviation, \u00cf\u0083p'\u00cf\u0083p'.Importantly, in the case of the analysis of the distribution of sample means, the Central Limit Theorem told us the expected value of the mean of the sample means in the sampling distribution, and the standard deviation of the sampling distribution. Again the Central Limit Theorem provides this information for the sampling distribution for proportions. The answers are:\nThe expected value of the mean of sampling distribution of sample proportions, \u00c2\u00b5p'\u00c2\u00b5p', is the population proportion, p.\nThe standard deviation of the sampling distribution of sample proportions, \u00cf\u0083p'\u00cf\u0083p', is the population standard deviation divided by the square root of the sample size, n.\nBoth these conclusions are the same as we found for the sampling distribution for sample means. However in this case, because the mean and standard deviation of the binomial distribution both rely upon pp, the formula for the standard deviation of the sampling distribution requires algebraic manipulation to be useful. We will take that up in the next chapter. The proof of these important conclusions from the Central Limit Theorem is provided below.\nE(p')=E(xn)=(1n)E(x)=(1n)np=pE(p')=E(xn)=(1n)E(x)=(1n)np=p\n(The expected value of X, E(x), is simply the mean of the binomial distribution which we know to be np.) \n\u00cf\u0083p'2=Var(p')=Var(xn)=1n2(Var(x))=1n2(np(1\u00e2\u0088\u0092p))=p(1\u00e2\u0088\u0092p)n\u00cf\u0083p'2=Var(p')=Var(xn)=1n2(Var(x))=1n2(np(1\u00e2\u0088\u0092p))=p(1\u00e2\u0088\u0092p)nThe standard deviation of the sampling distribution for proportions is thus:\u00cf\u0083p'=p(1\u00e2\u0088\u0092P)n\u00cf\u0083p'=p(1\u00e2\u0088\u0092P)n\n\nParameter\nPopulation distribution\nSample\nSampling distribution of p's\n\n\n\nMean\n\u00c2\u00b5 = np\np'=xnp'=xn\np' and E(p') = p\n\n\nStandard Deviation\n\u00cf\u0083=npq\u00cf\u0083=npq\n\n\u00cf\u0083p'=p(1\u00e2\u0088\u0092p)n\u00cf\u0083p'=p(1\u00e2\u0088\u0092p)n\n\n\nTable \n7.2\n \n \n\nTable 7.2 summarizes these results and shows the relationship between the population, sample and sampling distribution. Notice the parallel between this Table and Table 7.1 for the case where the random variable is continuous and we were developing the sampling distribution for means. Reviewing the formula for the standard deviation of the sampling distribution for proportions we see that as n increases the standard deviation decreases. This is the same observation we made for the standard deviation for the sampling distribution for means. Again, as the sample size increases, the point estimate for either \u00c2\u00b5 or p is found to come from a distribution with a narrower and narrower distribution. We concluded that with a given level of probability, the range from which the point estimate comes is smaller as the sample size, n, increases. Figure 7.8 shows this result for the case of sample means. Simply substitute p'p' for x\u00c2\u00afx\u00c2\u00af and we can see the impact of the sample size on the estimate of the sample proportion. \n"}, {"chapter": "7.4", "title": "Finite Population Correction Factor", "mathml": "<math display=\"block\"><semantics><mrow><mi>Z</mi><mo>=</mo>\n<mfrac>\n<mrow>\n<mover><mi>x</mi><mo>\u00c2\u00af</mo></mover>\n<mo>\u00e2\u0088\u0092</mo>\n<mi>\u00c2\u00b5</mi>\n</mrow>\n<mrow>\n<mfrac><mi>\u00cf\u0083</mi>\n<msqrt><mi>n</mi></msqrt>\n</mfrac>\n<mo>\u00c2\u00b7</mo>\n<msqrt>\n<mfrac><mrow><mi>N</mi><mo>\u00e2\u0088\u0092</mo><mi>n</mi></mrow>\n<mrow><mi>N</mi><mo>\u00e2\u0088\u0092</mo><mn>1</mn></mrow>\n</mfrac>\n</msqrt>\n</mrow>\n</mfrac>\n</mrow><annotation-xml encoding=\"MathML-Content\"><mi>Z</mi><mo>=</mo><mfrac>\n<mrow>\n<mover><mi>x</mi><mo>\u00c2\u00af</mo></mover>\n<mo>\u00e2\u0088\u0092</mo>\n<mi>\u00c2\u00b5</mi>\n</mrow>\n<mrow>\n<mfrac><mi>\u00cf\u0083</mi>\n<msqrt><mi>n</mi></msqrt>\n</mfrac>\n<mo>\u00c2\u00b7</mo>\n<msqrt>\n<mfrac><mrow><mi>N</mi><mo>\u00e2\u0088\u0092</mo><mi>n</mi></mrow>\n<mrow><mi>N</mi><mo>\u00e2\u0088\u0092</mo><mn>1</mn></mrow>\n</mfrac>\n</msqrt>\n</mrow>\n</mfrac></annotation-xml></semantics></math>\n</div", "content": "\nWe saw that the sample size has an important effect on the variance and thus the standard deviation of the sampling distribution. Also of interest is the proportion of the total population that has been sampled. We have assumed that the population is extremely large and that we have sampled a small part of the population. As the population becomes smaller and we sample a larger number of observations the sample observations are not independent of each other. To correct for the impact of this, the Finite Correction Factor can be used to adjust the variance of the sampling distribution. It is appropriate when more than 5% of the population is being sampled and the population has a known population size. There are cases when the population is known, and therefore the correction factor must be applied. The issue arises for both the sampling distribution of the means and the sampling distribution of proportions. The Finite Population Correction Factor for the variance of the means shown in the standardizing formula is: \nZ=\n\n\nx\u00c2\u00af\n\u00e2\u0088\u0092\n\u00c2\u00b5\n\n\n\u00cf\u0083\nn\n\n\u00c2\u00b7\n\nN\u00e2\u0088\u0092n\nN\u00e2\u0088\u00921\n\n\n\n\nZ=\n\nx\u00c2\u00af\n\u00e2\u0088\u0092\n\u00c2\u00b5\n\n\n\u00cf\u0083\nn\n\n\u00c2\u00b7\n\nN\u00e2\u0088\u0092n\nN\u00e2\u0088\u00921\n\n\n\n\nand for the variance of proportions is:\n\n\u00cf\u0083p'\n=\n\np(1\u00e2\u0088\u0092p)\nn\n\n\n\u00c3\u0097\n\n\n\nN\u00e2\u0088\u0092n\nN\u00e2\u0088\u00921\n\n\n\u00cf\u0083p'=\np(1\u00e2\u0088\u0092p)\nn\n\n\u00c3\u0097\n\n\nN\u00e2\u0088\u0092n\nN\u00e2\u0088\u00921\n\n\nThe following examples show how to apply the factor. Sampling variances get adjusted using the above formula. \nExample \n7.1\n \n\nIt is learned that the population of White German Shepherds in the USA is 4,000 dogs, and the mean weight for German Shepherds is 75.45 pounds. It is also learned that the population standard deviation is 10.37 pounds. \n\nIf the sample size is 100 dogs, then find the probability that a sample will have a mean that differs from the true probability mean by less than 2 pounds.\n\n\n\nSolution\n1\n\n\nN=4000,\nn=100,\n\u00cf\u0083=10.37,\n\u00c2\u00b5=75.45,\n(x\u00c2\u00af\u00e2\u0088\u0092\u00c2\u00b5)=\u00c2\u00b12N=4000,n=100,\u00cf\u0083=10.37,\u00c2\u00b5=75.45,(x\u00c2\u00af\u00e2\u0088\u0092\u00c2\u00b5)=\u00c2\u00b12\n\n\n\n\nZ=\n\nx\u00c2\u00af\u00e2\u0088\u0092\u00c2\u00b5\n\n\u00cf\u0083n\n\u00c2\u00b7\n\nN\u00e2\u0088\u0092n\nN\u00e2\u0088\u00921\n\n\n\n\n=\n\n\n\u00c2\u00b12\n\n\n10.37100\n\u00c2\u00b7\n4000\u00e2\u0088\u0092100\n4000\u00e2\u0088\u00921\n\n\n\n=\n\u00c2\u00b11.95\n\n\nZ\n=\n\nx\u00c2\u00af\u00e2\u0088\u0092\u00c2\u00b5\n\n\u00cf\u0083n\n\u00c2\u00b7\n\nN\u00e2\u0088\u0092n\nN\u00e2\u0088\u00921\n\n\n\n\n=\n\n\n\u00c2\u00b12\n\n\n10.37100\n\u00c2\u00b7\n4000\u00e2\u0088\u0092100\n4000\u00e2\u0088\u00921\n\n\n\n=\n\u00c2\u00b1\n1.95\n\n\n\n\n\n\n\nf(Z)=\n0.4744\n\u00c2\u00b7\n2\n=\n0.9488\n\n\nf\n(\nZ\n)\n=\n0.4744\n\u00c2\u00b7\n2\n=\n0.9488\n\n\n\n\n\nNote that \"differs by less\" references the area on both sides of the mean within 2 pounds right or left.\n\n\n\n\nExample \n7.2\n \n\n\n  When a customer places an order with Rudy's On-Line Office Supplies, a computerized accounting information system (AIS) automatically checks to see if  the customer has exceeded his or her credit limit. Past records indicate that the probability of customers exceeding their credit limit is .06. \n\n\n    Suppose that on a given day, 3,000 orders are placed in total. If we randomly select 360 orders, what is the probability that between 10 and 20 customers will exceed their credit limit? \n  \n\n\n\n\nSolution\n1\n\n\nN=3000,\nn=360,\np=0.06N=3000,n=360,p=0.06\n\n\u00cf\u0083p'=\n\np(1\u00e2\u0088\u0092p)\nn\n\n\n\u00c3\u0097\n\n\nN\u00e2\u0088\u0092n\nN\u00e2\u0088\u00921\n\n\n=\n\n0.06(1\u00e2\u0088\u00920.06)\n360\n\n\n\u00c3\u0097\n\n\n3000\u00e2\u0088\u0092360\n3000\u00e2\u0088\u00921\n\n\n=\n0.0117\n\u00cf\u0083p'=\np(1\u00e2\u0088\u0092p)\nn\n\n\u00c3\u0097\n\nN\u00e2\u0088\u0092n\nN\u00e2\u0088\u00921\n\n=\n0.06(1\u00e2\u0088\u00920.06)\n360\n\n\u00c3\u0097\n\n3000\u00e2\u0088\u0092360\n3000\u00e2\u0088\u00921\n\n=0.0117\n\n\n\np1=10360\n=\n0.0278\n,\np2\n=\n20360\n=\n0.0556\n\n\n\np\n1\n\n=\n\n10\n360\n\n=\n0.0278\n,\n\n\np\n2\n\n=\n\n20\n360\n\n=\n0.0556\n\n\n\n\nZ=\n\n\np'\u00e2\u0088\u0092p\n\np(1\u00e2\u0088\u0092p)\nn\n\n\n\u00c2\u00b7\n\n\nN\u00e2\u0088\u0092n\nN\u00e2\u0088\u00921\n\n\n\n=\n\n0.0278\u00e2\u0088\u00920.06\n0.011744\n\n=\n\u00e2\u0088\u00922.74\nZ=\n\np'\u00e2\u0088\u0092p\n\np(1\u00e2\u0088\u0092p)\nn\n\n\n\u00c2\u00b7\n\n\nN\u00e2\u0088\u0092n\nN\u00e2\u0088\u00921\n\n\n=\n0.0278\u00e2\u0088\u00920.06\n0.011744\n=\u00e2\u0088\u00922.74\nZ=\n\np'\u00e2\u0088\u0092p\n\n\np(1\u00e2\u0088\u0092p)\nn\n\n\n\u00c2\u00b7\n\n\nN\u00e2\u0088\u0092n\nN\u00e2\u0088\u00921\n\n\n\n\n=\n0.0556\u00e2\u0088\u00920.06\n0.011744\n\n=\n\u00e2\u0088\u00920.38\nZ=\np'\u00e2\u0088\u0092p\n\n\np(1\u00e2\u0088\u0092p)\nn\n\n\n\u00c2\u00b7\n\n\nN\u00e2\u0088\u0092n\nN\u00e2\u0088\u00921\n\n\n\n=0.0556\u00e2\u0088\u00920.06\n0.011744\n=\u00e2\u0088\u00920.38\n\np\n(\n0.0278\u00e2\u0088\u00920.06\n0.011744\n\n<\nz\n<\n\n0.0556\u00e2\u0088\u00920.06\n0.011744\n\n)\n=\np\n(\n\u00e2\u0088\u00922.74\n<\nz\n<\n\u00e2\u0088\u00920.38\n)\n=\n0.4969\n\u00e2\u0088\u00920.1480=0.3489\np(0.0278\u00e2\u0088\u00920.06\n0.011744\n<z<\n0.0556\u00e2\u0088\u00920.06\n0.011744\n)=p(\u00e2\u0088\u00922.74<z<\u00e2\u0088\u00920.38)=0.4969\u00e2\u0088\u00920.1480=0.3489\n\n\n\n\n\n"}, {"chapter": "8.2", "title": "A Confidence Interval for a  Population Standard Deviation Unknown, Small Sample Case", "mathml": "<math display=\"block\"><semantics><mrow>\n<mover><mi>x</mi><mo>-</mo></mover>\n<mo>-</mo>\n<msub>\n<mi>t</mi>\n<mi>v,\u00ce\u00b1</mi>\n</msub>\n<mo>(</mo>\n<mfrac>\n<mi>s</mi>\n<mrow>\n<msqrt><mi>n</mi></msqrt>\n</mrow>\n</mfrac>\n<mo>)</mo>\n<mo>\u00e2\u0089\u00a4</mo>\n<mi>\u00ce\u00bc</mi>\n<mo>\u00e2\u0089\u00a4</mo>\n<mover><mi>x</mi><mo>-</mo></mover>\n<mo>+</mo>\n<msub>\n<mi>t</mi>\n<mi>v,\u00ce\u00b1</mi>\n</msub>\n<mo>(</mo>\n<mfrac>\n<mi>s</mi>\n<mrow>\n<msqrt><mi>n</mi></msqrt>\n</mrow>\n</mfrac>\n<mo>)</mo>\n</mrow><annotation-xml encoding=\"MathML-Content\"><mover><mi>x</mi><mo>-</mo></mover><mo>-</mo><msub>\n<mi>t</mi>\n<mi>v,\u00ce\u00b1</mi>\n</msub><mo>(</mo><mfrac>\n<mi>s</mi>\n<mrow>\n<msqrt><mi>n</mi></msqrt>\n</mrow>\n</mfrac><mo>)</mo><mo>\u00e2\u0089\u00a4</mo><mi>\u00ce\u00bc</mi><mo>\u00e2\u0089\u00a4</mo><mover><mi>x</mi><mo>-</mo></mover><mo>+</mo><msub>\n<mi>t</mi>\n<mi>v,\u00ce\u00b1</mi>\n</msub><mo>(</mo><mfrac>\n<mi>s</mi>\n<mrow>\n<msqrt><mi>n</mi></msqrt>\n</mrow>\n</mfrac><mo>)</mo></annotation-xml></semantics></math></div", "content": "\nIn practice, we rarely know the population standard deviation. In the past, when the sample size was large, this did not present a problem to statisticians. They used the sample standard deviation s as an estimate for \u00cf\u0083 and proceeded as before to calculate a confidence interval with close enough results. This is what we did in Example 8.4 above. The point estimate for the standard deviation, s, was substituted in the formula for the confidence interval for the population standard deviation. In this case there 80 observation well above the suggested 30 observations to eliminate any bias from a small sample. However, statisticians ran into problems when the sample size was small. A small sample size caused inaccuracies in the confidence interval. William S. Goset (1876\u00e2\u0080\u00931937) of the Guinness brewery in Dublin, Ireland ran into this  problem. His experiments with hops and barley produced very few samples. Just replacing \u00cf\u0083 with s did not produce accurate results when he tried to calculate a confidence interval. He realized that he could not use a normal distribution for the calculation; he found that the actual distribution depends on the sample size. This problem led him to \"discover\" what is called the Student's t-distribution. The name comes from the fact that Gosset wrote under the pen name \"A Student.\"Up until the mid-1970s, some statisticians used the normal distribution approximation for large sample sizes and used the Student's t-distribution only for sample sizes of at most 30 observations. If you draw a simple random sample of size n from a population with mean \u00ce\u00bc and unknown population standard deviation \u00cf\u0083 and calculate the t-score t = \n\n\n\n\nx\n-\n\n\u00e2\u0080\u0093\u00ce\u00bc\n\n\n(\n\n\ns\n\n\nn\n\n\n\n\n)\n\n\n\n\n\n\n\nx\n-\n\n\u00e2\u0080\u0093\u00ce\u00bc\n\n\n(\n\n\ns\n\n\nn\n\n\n\n\n)\n\n\n, then the t-scores follow a Student's t-distribution with n \u00e2\u0080\u0093 1 degrees of freedom. The t-score has the same interpretation as the z-score. It measures how far in standard deviation units \n\nx\n-\n\n\nx\n-\n is from its mean \u00ce\u00bc. For each sample size n, there is a different Student's t-distribution.The degrees of freedom, n \u00e2\u0080\u0093 1, come from the calculation of the sample standard deviation s. Remember when we first calculated a sample standard deviation we divided the sum of the squared deviations by n \u00e2\u0088\u0092 1, but we used n deviations \n\n(x\u00e2\u0080\u0093\nx\n-\n\nvalues)\n\n\n(x\u00e2\u0080\u0093\nx\n-\n\nvalues)\n to calculate s. Because the sum of the deviations is zero, we can find the last deviation once we know the other n \u00e2\u0080\u0093 1 deviations. The other n \u00e2\u0080\u0093 1 deviations can change or vary freely. We call the number n \u00e2\u0080\u0093 1 the degrees of freedom (df) in recognition that one is lost in the calculations. The effect of losing a degree of freedom is that the t-value increases and the confidence interval increases in width.Properties of the Student's t-DistributionThe graph for the Student's t-distribution is similar to the standard normal curve and at infinite degrees of freedom it is the normal distribution. You can confirm this by reading the bottom line at infinite degrees of freedom for a familiar level of confidence, e.g. at column 0.05, 95% level of confidence, we find the t-value of 1.96 at infinite degrees of freedom.\nThe mean for the Student's t-distribution is zero and the distribution is symmetric about zero, again like the standard normal distribution.\nThe Student's t-distribution has more probability in its tails than the standard normal distribution because the spread of the t-distribution is greater than the spread of the standard normal. So the graph of the Student's t-distribution will be thicker in the tails and shorter in the center than the graph of the standard normal distribution.\nThe exact shape of the Student's t-distribution depends on the degrees of freedom. As the degrees of freedom increases, the graph of Student's t-distribution becomes more like the graph of the standard normal distribution.\nThe underlying population of individual observations is assumed to be normally distributed with unknown population mean \u00ce\u00bc and unknown population standard deviation \u00cf\u0083. This assumption comes from the Central Limit theorem because the individual observations in this case are the x\u00c2\u00afx\u00c2\u00afs of the sampling distribution. The size of the underlying population is generally not relevant unless it is very small. If it is normal then the assumption is met and doesn't need discussion.\nA probability table for the Student's t-distribution is used to calculate t-values at various commonly-used levels of confidence. The table gives t-scores that correspond to the confidence level (column) and degrees of freedom (row). When using a t-table, note that some tables are formatted to show the confidence level in the column headings, while the column headings in some tables may show only corresponding area in one or both tails. Notice that at the bottom the table will show the t-value for infinite degrees of freedom. Mathematically, as the degrees of freedom increase, the t-distribution approaches the standard normal distribution. You can find familiar Z-values by looking in the relevant alpha column and reading value in the last row.\nA Student's t table (See Appendix A Statistical Tables) gives t-scores given the degrees of freedom and the right-tailed probability.\nThe Student's t-distribution has one of the most desirable properties of the normal: it is symmetrical. What the Student's t-distribution does is spread out the horizontal axis so it takes a larger number of standard deviations to capture the same amount of probability. In reality there are an infinite number of Student's t-distributions, one for each adjustment to the sample size. As the sample size increases, the Student's t-distribution become more and more like the normal distribution. When the sample size reaches 30 the normal distribution is usually substituted for the Student's t because they are so much alike. This relationship between the Student's t-distribution and the normal distribution is shown in Figure 8.8.\nFigure \n8.8\n\nThis is another example of one distribution limiting another one, in this case the normal distribution is the limiting distribution of the Student's t when the degrees of freedom in the Student's t approaches infinity. This conclusion comes directly from the derivation of the Student's t-distribution by Mr. Gosset. He recognized the problem as having few observations and no estimate of the population standard deviation. He was substituting the sample standard deviation and getting volatile results. He therefore created the Student's t-distribution as a ratio of the normal distribution and Chi squared distribution. The Chi squared distribution is itself a ratio of two variances, in this case the sample variance and the unknown population variance. The Student's t-distribution thus is tied to the normal distribution, but has degrees of freedom that come from those of the Chi squared distribution. The algebraic solution demonstrates this result.Development of Student's t-distribution:t=z\u00cf\u00872vt=z\u00cf\u00872v\nwhere z is the standard normal variable and \u00cf\u00872 is the chi-squared distribution with v degrees of freedom. Substitute values and simplify:\n\n t=(x\u00c2\u00af\u00e2\u0088\u0092\u00ce\u00bc)\u00cf\u0083s2(n\u00e2\u0088\u00921)\u00cf\u00832(n\u00e2\u0088\u00921)=\n(x\u00c2\u00af\u00e2\u0088\u0092\u00ce\u00bc)\u00cf\u0083\ns2\n\u00cf\u00832\n=\n(x\u00c2\u00af\u00e2\u0088\u0092\u00ce\u00bc)\u00cf\u0083\ns\u00cf\u0083\n\n=\nx\u00c2\u00af\u00e2\u0088\u0092\u00ce\u00bc\nsn\n\nt=(x\u00c2\u00af\u00e2\u0088\u0092\u00ce\u00bc)\u00cf\u0083s2(n\u00e2\u0088\u00921)\u00cf\u00832(n\u00e2\u0088\u00921)=(x\u00c2\u00af\u00e2\u0088\u0092\u00ce\u00bc)\u00cf\u0083\ns2\n\u00cf\u00832=(x\u00c2\u00af\u00e2\u0088\u0092\u00ce\u00bc)\u00cf\u0083\ns\u00cf\u0083\n=x\u00c2\u00af\u00e2\u0088\u0092\u00ce\u00bc\nsn\n\n\n\nt=snt=snRestating the formula for a confidence interval for the mean for cases when the sample size is smaller than 30 and we do not know the population standard deviation, \u00cf\u0083:\nx-\n-\n\nt\nv,\u00ce\u00b1\n\n(\n\ns\n\nn\n\n\n)\n\u00e2\u0089\u00a4\n\u00ce\u00bc\n\u00e2\u0089\u00a4\nx-\n+\n\nt\nv,\u00ce\u00b1\n\n(\n\ns\n\nn\n\n\n)\nx--\nt\nv,\u00ce\u00b1\n(\ns\n\nn\n\n)\u00e2\u0089\u00a4\u00ce\u00bc\u00e2\u0089\u00a4x-+\nt\nv,\u00ce\u00b1\n(\ns\n\nn\n\n)Here the point estimate of the population standard deviation, s has been substituted for the population standard deviation, \u00cf\u0083, and t\u00ce\u00bd,\u00ce\u00b1 has been substituted for Z\u00ce\u00b1. The Greek letter \u00ce\u00bd (pronounced nu) is placed in the general formula in recognition that there are many Student tv distributions, one for each sample size. \u00ce\u00bd is the symbol for the degrees of freedom of the distribution and depends on the size of the sample. Often df is used to abbreviate degrees of freedom. For this type of problem, the degrees of freedom is \u00ce\u00bd = n-1, where n is the sample size. To look up a probability in the Student's t table we have to know the degrees of freedom in the problem.  \nExample \n8.5\n \n\n\nThe average earnings per share (EPS)  for 10 industrial stocks randomly selected from those listed on the Dow-Jones Industrial Average was found to be X-X-   = 1.85 with a standard deviation of  s=0.395. Calculate a 99% confidence interval for the average EPS of all the industrials listed on the DJIA.\nx-\n-\n\nt\nv,\u00ce\u00b1\n\n(\n\ns\n\nn\n\n\n)\n\u00e2\u0089\u00a4\n\u00ce\u00bc\n\u00e2\u0089\u00a4\nx-\n+\n\nt\nv,\u00ce\u00b1\n\n(\n\ns\n\nn\n\n\n)\nx--\nt\nv,\u00ce\u00b1\n(\ns\n\nn\n\n)\u00e2\u0089\u00a4\u00ce\u00bc\u00e2\u0089\u00a4x-+\nt\nv,\u00ce\u00b1\n(\ns\n\nn\n\n)\n\n\n\n\nSolution\n1\n\nTo help visualize the process of calculating a confident interval we draw the appropriate distribution for the problem. In this case this is the Student\u00e2\u0080\u0099s t because we do not know the population standard deviation and the sample is small, less than 30.\n\n\n\nFigure \n8.9\n\n\nTo find the appropriate t-value requires two pieces of information, the level of confidence desired and the degrees of freedom. The question asked for a 99% confidence level. On the graph this is shown where (1-\u00ce\u00b1) , the level of confidence , is in the unshaded area. The tails, thus, have .005 probability each, \u00ce\u00b1/2. The degrees of freedom for this type of problem is n-1= 9. From the Student\u00e2\u0080\u0099s t table, at the row marked 9 and column marked .005, is the number of standard deviations to capture 99% of the probability, 3.2498. These are then placed on the graph remembering that the Student\u00e2\u0080\u0099s t is symmetrical and so the t-value is both plus or minus on each side of the mean.Inserting these values into the formula gives the result. These values can be placed on the graph to see the relationship between the distribution of the sample means, X-X-'s  and the Student\u00e2\u0080\u0099s t-distribution.  \n\n\n\n\u00ce\u00bc=\nX-\n\u00c2\u00b1\nt\u00ce\u00b1/2,df=n-1\n\nsn\n\n=\n1.851\u00c2\u00b13.2498\n\n0.395\n10\n\n=\n1.8551\u00c2\u00b10.406\n\n\n\u00ce\u00bc\n=\n\nX\n-\n\n\u00c2\u00b1\n\nt\n\u00ce\u00b1/2,df=n-1\n\n\nsn\n\n=\n1.851\n\u00c2\u00b1\n3.2498\n\n0.395\n10\n\n=\n1.8551\n\u00c2\u00b1\n0.406\n\n\n\n\n\n\n\n\n1.445\n\u00e2\u0089\u00a4\n\u00ce\u00bc\n\u00e2\u0089\u00a4\n2.257\n\n\n1.445\n\u00e2\u0089\u00a4\n\u00ce\u00bc\n\u00e2\u0089\u00a4\n2.257\n\n\n\nWe state the formal conclusion as :With 99% confidence level, the average EPS of all the industries listed at DJIA is from $1.44 to $2.26.\n\n\n\nTry It \n8.5\n\n\n\nYou do a study of hypnotherapy to determine how effective it is in increasing the number of hours of sleep subjects get each night. You measure hours of sleep for 12 subjects with the following results. Construct a 95% confidence interval for the mean number of hours slept for the population (assumed normal) from which you took the data.8.2; 9.1; 7.7; 8.6; 6.9; 11.2; 10.1; 9.9; 8.9; 9.2; 7.5; 10.5\n\n\n"}, {"chapter": "8.3", "title": "A Confidence Interval for A Population Proportion", "mathml": "<math display=\"block\"><semantics><mrow><msub><mi>\u00cf\u0083</mi><mi>p'</mi></msub><mo>=</mo><msqrt><mfrac><mrow><mi>p</mi><mo>(</mo><mn>1</mn><mo>\u00e2\u0088\u0092</mo><mi>p</mi><mo>)</mo></mrow><mrow><mi>n</mi></mrow></mfrac></msqrt></mrow><annotation-xml encoding=\"MathML-Content\"><msub><mi>\u00cf\u0083</mi><mi>p'</mi></msub><mo>=</mo><msqrt><mfrac><mrow><mi>p</mi><mo>(</mo><mn>1</mn><mo>\u00e2\u0088\u0092</mo><mi>p</mi><mo>)</mo></mrow><mrow><mi>n</mi></mrow></mfrac></msqrt></annotation-xml></semantics></math></div", "content": "\nDuring an election year, we see articles in the newspaper that state confidence intervals in terms of proportions or percentages. For example, a poll for a particular candidate running for president might show that the candidate has 40% of the vote within three percentage points  (if the sample is large enough). Often, election polls are calculated with 95% confidence, so, the pollsters would be 95% confident that the true proportion of voters who favored the candidate would be between 0.37 and 0.43.Investors in the stock market are interested in the true proportion of stocks that go up and down each week. Businesses that sell personal computers are interested in the proportion of households in the United States that own personal computers. Confidence intervals can be calculated for the true proportion of stocks that go up or down each week and for the true proportion of households in the United States that own personal computers.\nThe procedure to find the confidence interval for a population proportion is similar to that for the population mean, but the formulas are a bit different although conceptually identical. While the formulas are different, they are based upon the same mathematical foundation given to us by the Central Limit Theorem. Because of this we will see the same basic format using the same three pieces of information: the sample value of the parameter in question, the standard deviation of the relevant sampling distribution, and the number of standard deviations we need to have the confidence in our estimate that we desire.How do you know you are dealing with a proportion problem? First, the underlying distribution has a binary random variable and therefore is a binomial distribution. (There is no mention of a mean or average.) If X is a binomial random variable, then X ~ B(n, p)  where n is the number of trials and p is the probability of a success. To form a sample proportion, take X, the random variable for the number of successes and divide it by n, the number of trials (or the sample size). The random variable P\u00e2\u0080\u00b2 (read \"P prime\") is the sample proportion,\n  \n\n\nP\n\u00e2\u0080\u00b2\n\n=\nX\nn\n\n\n\n\nP\n\u00e2\u0080\u00b2\n\n=\nX\nn\n\n\n\n(Sometimes the random variable is denoted as \n\nP\n^\n\n\nP\n^\n, read \"P hat\".)\np\u00e2\u0080\u00b2 = the estimated proportion of successes or sample proportion of successes (p\u00e2\u0080\u00b2 is a point estimate for p, the true population proportion, and thus q is the probability of a failure in any one trial.)x = the number of successes in the samplen = the size of the sample\nThe formula for the confidence interval for a population proportion follows the same format as that for an estimate of a population mean. Remembering the sampling distribution for the proportion from Chapter 7, the standard deviation was found to be:\u00cf\u0083p'=p(1\u00e2\u0088\u0092p)n\u00cf\u0083p'=p(1\u00e2\u0088\u0092p)nThe confidence interval for a population proportion, therefore, becomes:p=p\u00e2\u0080\u00b2\u00c2\u00b1[Z(a2)p\u00e2\u0080\u00b2(1\u00e2\u0088\u0092p\u00e2\u0080\u00b2)n]p=p\u00e2\u0080\u00b2\u00c2\u00b1[Z(a2)p\u00e2\u0080\u00b2(1\u00e2\u0088\u0092p\u00e2\u0080\u00b2)n]Z(a2)Z(a2) is set according to our desired degree of confidence and p\u00e2\u0080\u00b2(1\u00e2\u0088\u0092p\u00e2\u0080\u00b2)np\u00e2\u0080\u00b2(1\u00e2\u0088\u0092p\u00e2\u0080\u00b2)n is the standard deviation of the sampling distribution.The  sample proportions p\u00e2\u0080\u00b2 and q\u00e2\u0080\u00b2 are estimates of the unknown population proportions p and q. The estimated proportions p\u00e2\u0080\u00b2 and q\u00e2\u0080\u00b2 are used because p and q are not known. Remember that as p moves further from 0.5 the binomial distribution becomes less symmetrical. Because we are estimating the binomial with the symmetrical normal distribution the further away from symmetrical the binomial becomes the less confidence we have in the estimate.This conclusion can be demonstrated through the following analysis. Proportions are based upon the binomial probability distribution. The possible outcomes are binary, either \u00e2\u0080\u009csuccess\u00e2\u0080\u009d or \u00e2\u0080\u009cfailure\u00e2\u0080\u009d. This gives rise to a proportion, meaning the percentage of the outcomes that are \u00e2\u0080\u009csuccesses\u00e2\u0080\u009d. It was shown that the binomial distribution could be fully understood if we knew only the probability of a success in any one trial, called p. The mean and the standard deviation of the binomial were found to be:\n\n\u00ce\u00bc = np\n\n\n\u00ce\u00bc = np\n\n\u00cf\u0083\n=\n\nnp\nq\n\n\u00cf\u0083=\nnp\nq\nIt was also shown that the binomial could be estimated by the normal distribution if BOTH np AND nq were greater than 5. From the discussion above, it was found that the standardizing formula for the binomial distribution is:\nZ\n=\n\n\np'\n-\np\n\n\n\n(\n\n\np\nq\n\n\nn\n\n\n)\n\n\n\nZ=\n\np'\n-\np\n\n\n\n(\n\n\np\nq\n\n\nn\n\n\n)\n\n\nwhich is nothing more than a restatement of the general standardizing formula with appropriate substitutions for \u00ce\u00bc and \u00cf\u0083 from the binomial. We can use the standard normal distribution, the reason Z is in the equation, because the normal distribution is the limiting distribution of the binomial. This is another example of the Central Limit Theorem. We have already seen that the sampling distribution of means is normally distributed. Recall the extended discussion in Chapter 7 concerning the sampling distribution of proportions and the conclusions of the Central Limit Theorem.We can now manipulate this formula in just the same way we did for finding the confidence intervals for a mean, but to find the confidence interval for the binomial population parameter,  p.\np'\n-\nZ\u00ce\u00b1\np'q'\nn\n\n\n \u00e2\u0089\u00a4 p\n \u00e2\u0089\u00a4 \np'\n+\nZ\u00ce\u00b1\np'q'\nn\n\n\np'-Z\u00ce\u00b1p'q'\nn\n\n \u00e2\u0089\u00a4 p \u00e2\u0089\u00a4 p'+Z\u00ce\u00b1p'q'\nn\n\nWhere p\u00e2\u0080\u00b2 = x/n, the point estimate of p taken from the sample.   Notice that p\u00e2\u0080\u00b2 has replaced p in the formula. This is because we do not know p, indeed, this is just what we are trying to estimate. Unfortunately, there is no correction factor for cases where the sample size is small so np\u00e2\u0080\u00b2 and nq' must always be greater than 5 to develop an interval estimate for p.\nExample \n8.6\n \n\n\n\nSuppose that a market research firm is hired to estimate the percent of adults living in a large city who have cell phones.  Five hundred randomly selected adult residents in this city are surveyed to determine whether they have cell phones. Of\n  the 500 people sampled, 421 responded yes - they own cell phones. Using a 95% confidence level, compute a confidence interval estimate for the true proportion of adult residents of this city who have cell phones.\n\n\n\n\nSolution\n1\n\n\nThe solution step-by-step.\nLet X = the number of people in the sample who have cell phones. X is binomial: the random variable is binary, people either have a cell phone or they do not. To calculate the confidence interval, we must find p\u00e2\u0080\u00b2, q\u00e2\u0080\u00b2.n = 500\nx = the number of successes in the sample = 421\n\n\np\n\u00e2\u0080\u00b2\n\n=\nx\nn\n\n=\n\n421\n\n\n500\n\n\n=0.842\n\n\n\np\n\u00e2\u0080\u00b2\n\n=\nx\nn\n\n=\n\n421\n\n\n500\n\n\n=0.842\n\n\np\u00e2\u0080\u00b2 = 0.842 is the sample proportion; this is the point estimate of the population proportion.\nq\u00e2\u0080\u00b2 = 1 \u00e2\u0080\u0093 p\u00e2\u0080\u00b2 = 1 \u00e2\u0080\u0093 0.842 = 0.158\nSince the requested confidence level is CL = 0.95, then \u00ce\u00b1 = 1 \u00e2\u0080\u0093 CL = 1 \u00e2\u0080\u0093 0.95 = 0.05 \n\n(\n\n\n\u00ce\u00b1\n2\n\n\n)\n\n\n(\n\n\n\u00ce\u00b1\n2\n\n\n)\n = 0.025.Then \n\nz\n\n\n\u00ce\u00b1\n\n\n2\n\n\n\n=\n\nz\n0.025\n\n=\n1.96\n\nz\n\n\n\u00ce\u00b1\n\n\n2\n\n\n=\nz\n0.025\n=1.96\nThis can be found using the Standard Normal probability table in Appendix A Statistical Tables. This can also be found in the students t table at the 0.025 column and infinity degrees of freedom because at infinite degrees of freedom the students t-distribution becomes the standard normal distribution, Z.The confidence interval for the true binomial population proportion is \n\n\n\np'\n-\n\nZ\n\u00ce\u00b1\n\n\n\n\np'q'\n\n\nn\n\n\n\n\u00e2\u0089\u00a4\np\n\u00e2\u0089\u00a4\np'\n+\n\nZ\n\u00ce\u00b1\n\n\n\n\np'q'\n\n\nn\n\n\n\n\n\np'\n-\n\nZ\n\u00ce\u00b1\n\n\n\n\np'q'\n\n\nn\n\n\n\n\u00e2\u0089\u00a4\np\n\u00e2\u0089\u00a4\np'\n+\n\nZ\n\u00ce\u00b1\n\n\n\n\np'q'\n\n\nn\n\n\n\n\n\n\n\n\n\n\nSubstituting in the values from above we find the confidence interval is : \n0.810\n\u00e2\u0089\u00a4\np\n\u00e2\u0089\u00a4\n0.874\n\n\nSubstituting in the values from above we find the confidence interval is : \n0.810\n\u00e2\u0089\u00a4\np\n\u00e2\u0089\u00a4\n0.874\n\n\n\nInterpretationWe estimate with 95% confidence that between 81% and 87.4% of all adult residents of this city have cell phones.\n Explanation of 95% Confidence LevelNinety-five percent of the confidence intervals constructed in this way would contain the true value for the population proportion of all adult residents of this city who have cell phones.\n\n\n\n\n\nTry It \n8.6\n\n\n\n\nSuppose 250 randomly selected people are surveyed to determine if they own a tablet. Of the 250 surveyed, 98 reported owning a tablet. Using a 95% confidence level, compute a confidence interval estimate for the true proportion of people who own tablets.\n  \n\n\n\nExample \n8.7\n \n\n\nThe Dundee Dog Training School has a larger than average proportion of clients who compete in competitive professional events. A confidence interval for the population proportion of dogs that compete in professional events from 150 different training schools is constructed. The lower limit is determined to be 0.08 and the upper limit is determined to be 0.16. Determine the level of confidence used to construct the interval of the population proportion of dogs that compete in professional events.\n\n\n\nSolution\n1\n\n\nWe begin with the formula for a confidence interval for a proportion because the random variable is binary; either the client competes in professional competitive dog events or they don't.\n\n\n\np\n=\np\n\u00e2\u0080\u00b2\n\u00c2\u00b1\n[\n\nZ\n\n(\n\na\n2\n\n)\n\n\n\n\n\n\np\n\u00e2\u0080\u00b2\n(\n1\n\u00e2\u0088\u0092\np\n\u00e2\u0080\u00b2\n)\n\n\nn\n\n\n\n\n]\n\n\np\n=\np\n\u00e2\u0080\u00b2\n\u00c2\u00b1\n[\n\nZ\n\n(\n\na\n2\n\n)\n\n\n\n\n\n\np\n\u00e2\u0080\u00b2\n(\n1\n\u00e2\u0088\u0092\np\n\u00e2\u0080\u00b2\n)\n\n\nn\n\n\n\n\n]\n\n\n\n\nNext we find the sample proportion:\n\n\n\n\np\n\u00e2\u0080\u00b2\n=\n\n\n0.08\n+\n0.16\n\n\n2\n\n\n=\n0.12\n\n\np\n\u00e2\u0080\u00b2\n=\n\n\n0.08\n+\n0.16\n\n\n2\n\n\n=\n0.12\n\n\n\n\nThe \u00c2\u00b1 that makes up the confidence interval is thus 0.04; 0.12 + 0.04 = 0.16 and 0.12 \u00e2\u0088\u0092 0.04 = 0.08, the boundaries of the confidence interval. Finally, we solve for Z.\n[Z\u00e2\u008b\u00850.12(1\u00e2\u0088\u00920.12)150]=0.04[Z\u00e2\u008b\u00850.12(1\u00e2\u0088\u00920.12)150]=0.04, therefore Z = 1.51\nAnd then look up the probability for 1.51 standard deviations on the standard normal table.p(Z=1.51)=0.4345p(Z=1.51)=0.4345, p(Z)\u00e2\u008b\u00852=0.8690p(Z)\u00e2\u008b\u00852=0.8690 or 86.90%86.90%.\n\n\n\nExample \n8.8\n \n\n\nA financial officer for a company wants to estimate the percent of accounts receivable that are more than 30 days overdue. He surveys 500 accounts and finds that 300 are more than 30 days overdue. Compute a 90% confidence interval for the true percent of accounts receivable that are more than 30 days overdue, and interpret the confidence interval.\n\n\n\nSolution\n1\n\n\nThe solution is step-by-step:\nx = 300 and n = 500\n\n\n\n\n\n\np\n\u00e2\u0080\u00b2\n\n=\nx\nn\n\n=\n\n300\n\n\n500\n\n\n=0.600\n\n\n\n\n\np\n\u00e2\u0080\u00b2\n\n=\nx\nn\n\n=\n\n300\n\n\n500\n\n\n=0.600\n\n\n\n\n\n\n\n\n\n\n\nq\n\u00e2\u0080\u00b2\n\n=1-\np\n\u00e2\u0080\u00b2\n\n=1-0.600=0.400\n\n\n\n\n\nq\n\u00e2\u0080\u00b2\n\n=1-\np\n\u00e2\u0080\u00b2\n\n=1-0.600=0.400\n\n\n\n\n\nSince confidence level = 0.90, then \u00ce\u00b1 = 1 \u00e2\u0080\u0093 confidence level = (1 \u00e2\u0080\u0093 0.90) = 0.10\n\n(\n\n\n\u00ce\u00b1\n2\n\n\n)\n\n\n(\n\n\n\u00ce\u00b1\n2\n\n\n)\n = 0.05\n\n\nZ\n\n\n\u00ce\u00b1\n\n\n2\n\n\n\n\nZ\n\n\n\u00ce\u00b1\n\n\n2\n\n\n = Z0.05 = 1.645\nThis Z-value can be found using a standard normal probability table. The student's t-table can also be used by entering the table at the 0.05 column and reading at the line for infinite degrees of freedom. The t-distribution is the normal distribution at infinite degrees of freedom. This is a handy trick to remember in finding Z-values for commonly used levels of confidence. We use this formula  for a confidence interval for a proportion:\n\n\n\np'\n-\nZ\u00ce\u00b1\np'q'\nn\n\n\n \u00e2\u0089\u00a4 p\n \u00e2\u0089\u00a4 \np'\n+\nZ\u00ce\u00b1\np'q'\nn\n\n\n\n\np'\n-\n\nZ\n\u00ce\u00b1\n\np'q'\nn\n\n\n \u00e2\u0089\u00a4 \np\n \u00e2\u0089\u00a4 \np'\n+\n\nZ\n\u00ce\u00b1\n\np'q'\nn\n\n\n\n\n\n\nSubstituting in the values from above we find the confidence interval for the true binomial population proportion is 0.564 \u00e2\u0089\u00a4 p \u00e2\u0089\u00a4 0.636\n\nInterpretation\n\nWe estimate with 90% confidence that the true percent of all accounts receivable overdue 30 days is between 56.4% and 63.6%.\nAlternate Wording: We estimate with 90% confidence that between 56.4% and 63.6% of ALL accounts are overdue 30 days.\n\nExplanation of 90% Confidence LevelNinety percent of all confidence intervals constructed in this way contain the true value for the population percent of accounts receivable that are overdue 30 days.\n\n\n\nTry It \n8.8\n\n\n\n\nA student polls his school to see if students in the school district are for or against the new legislation regarding school uniforms. She surveys 600 students and finds that 480 are against the new legislation.a. Compute a 90% confidence interval for the true percent of students who are against the new legislation, and interpret the confidence interval.\n\n\n\nb. In a sample of 300 students, 68% said they own an iPod and a smart phone. Compute a 97% confidence interval for the true percent of students who own an iPod and a smartphone.\n\n\n"}, {"chapter": "8.4", "title": "Calculating the Sample Size n: Continuous and Binary Random Variables", "mathml": "<math display=\"block\"><semantics><mrow>\n<mi>n</mi>\n<mo>=</mo>\n<mfrac>\n<mrow>\n<msubsup>\n<mi>Z</mi>\n<mi>\u00ce\u00b1</mi>\n<mn>2</mn>\n</msubsup>\n<msup>\n<mi>\u00cf\u0083</mi>\n<mn>2</mn>\n</msup>\n</mrow>\n<mrow>\n<mo>(</mo>\n<mover>\n<mi>X</mi>\n<mo>\u00e2\u0080\u0093</mo>\n</mover>\n<mo>-</mo>\n<mi>\u00ce\u00bc</mi>\n<msup>\n<mo>)</mo>\n<mn>2</mn>\n</msup>\n</mrow>\n</mfrac>\n<mo>=</mo>\n<mfrac>\n<mrow>\n<msubsup>\n<mi>Z</mi>\n<mi>\u00ce\u00b1</mi>\n<mn>2</mn>\n</msubsup>\n<msup>\n<mi>\u00cf\u0083</mi>\n<mn>2</mn>\n</msup>\n</mrow>\n<mrow>\n<msup>\n<mi>e</mi>\n<mn>2</mn>\n</msup>\n</mrow>\n</mfrac>\n</mrow><annotation-xml encoding=\"MathML-Content\"><mi>n</mi><mo>=</mo><mfrac>\n<mrow>\n<msubsup>\n<mi>Z</mi>\n<mi>\u00ce\u00b1</mi>\n<mn>2</mn>\n</msubsup>\n<msup>\n<mi>\u00cf\u0083</mi>\n<mn>2</mn>\n</msup>\n</mrow>\n<mrow>\n<mo>(</mo>\n<mover>\n<mi>X</mi>\n<mo>\u00e2\u0080\u0093</mo>\n</mover>\n<mo>-</mo>\n<mi>\u00ce\u00bc</mi>\n<msup>\n<mo>)</mo>\n<mn>2</mn>\n</msup>\n</mrow>\n</mfrac><mo>=</mo><mfrac>\n<mrow>\n<msubsup>\n<mi>Z</mi>\n<mi>\u00ce\u00b1</mi>\n<mn>2</mn>\n</msubsup>\n<msup>\n<mi>\u00cf\u0083</mi>\n<mn>2</mn>\n</msup>\n</mrow>\n<mrow>\n<msup>\n<mi>e</mi>\n<mn>2</mn>\n</msup>\n</mrow>\n</mfrac></annotation-xml></semantics></math></div", "content": "\nContinuous Random VariablesUsually we have no control over the sample size of a data set. However, if we are able to set the sample size, as in cases where we are taking a survey, it is very helpful to know just how large it should be to provide the most information. Sampling can be very costly in both time and product. Simple telephone surveys will cost approximately $30.00 each, for example, and some sampling requires the destruction of the product.If we go back to our standardizing formula for the sampling distribution for means, we can see that it is possible to solve it for n. If we do this we have (X\u00e2\u0080\u0093-\u00ce\u00bc)(X\u00e2\u0080\u0093-\u00ce\u00bc)   in the denominator.\nn\n=\n\n\n\nZ\n\u00ce\u00b1\n2\n\n\n\u00cf\u0083\n2\n\n\n\n(\n\nX\n\u00e2\u0080\u0093\n\n-\n\u00ce\u00bc\n\n)\n2\n\n\n\n=\n\n\n\nZ\n\u00ce\u00b1\n2\n\n\n\u00cf\u0083\n2\n\n\n\n\ne\n2\n\n\n\nn=\n\n\nZ\n\u00ce\u00b1\n2\n\n\n\u00cf\u0083\n2\n\n\n\n(\n\nX\n\u00e2\u0080\u0093\n\n-\n\u00ce\u00bc\n\n)\n2\n\n\n=\n\n\nZ\n\u00ce\u00b1\n2\n\n\n\u00cf\u0083\n2\n\n\n\n\ne\n2\n\n\nBecause we have not taken a sample yet we do not know any of the variables in the formula except that we can set Z\u00ce\u00b1 to the level of confidence we desire just as we did when determining confidence intervals. If we set a predetermined acceptable error, or tolerance, for the difference between X\u00e2\u0080\u0093X\u00e2\u0080\u0093 and \u00ce\u00bc, called e in the formula, we are much further in solving for the sample size n. We still do not know the population standard deviation, \u00cf\u0083.  In practice, a pre-survey is usually done which allows for fine tuning the questionnaire and will give a sample standard deviation that can be used. In other cases, previous information from other surveys may be used for \u00cf\u0083 in the formula. While crude, this method of determining the sample size may help in reducing cost significantly. It will be the actual data gathered that determines the inferences about the population, so caution in the sample size is appropriate calling for high levels of confidence and small sampling errors.Binary Random VariablesWhat was done in cases when looking for the mean of a distribution can also be done when sampling to determine the population parameter p for proportions. Manipulation of the standardizing formula for proportions gives:\nn\n=\n\n\nZ\u00ce\u00b12pq\n\n\ne2\n\n\nn=\n\nZ\u00ce\u00b12pq\n\n\ne2\n\nwhere e = (p\u00e2\u0080\u00b2-p), and is the acceptable sampling error, or tolerance, for this application. This will be measured in percentage points.In this case the very object of our search is in the formula, p, and of course q because q =1-p. This result occurs because the binomial distribution is a one parameter distribution. If we know p then we know the mean and the standard deviation. Therefore, p shows up in the standard deviation of the sampling distribution which is where we got this formula.  If, in an abundance of caution, we substitute 0.5 for p we will draw the largest required sample size that will provide the level of confidence specified by Z\u00ce\u00b1 and the tolerance we have selected. This is true because of all combinations of two fractions that add to one, the largest multiple is when each is 0.5. Without any other information concerning the population parameter p, this is the common practice. This may result in oversampling, but certainly not under sampling, thus, this is a cautious approach. There is an interesting trade-off between the level of confidence and the sample size that shows up here when considering the cost of sampling. Table 8.1 shows the appropriate sample size at different levels of confidence and different level of the acceptable error, or tolerance.\n\nRequired sample size (90%)\nRequired sample size (95%)\nTolerance level\n\n\n\n1691\n2401\n2%\n\n\n752\n1067\n3%\n\n\n271\n384\n5%\n\n\n68\n96\n10%\n\n\nTable \n8.1\n \n \n\nThis table is designed to show the maximum sample size required at different levels of confidence given an assumed p= 0.5 and q=0.5 as discussed above. The acceptable error, called tolerance in the table, is measured in plus or minus values from the actual proportion. For example, an acceptable error of 5% means that if the sample proportion was found to be 26 percent, the conclusion would be that the actual population proportion is between 21 and 31 percent with a 90 percent level of confidence if a sample of 271 had been taken. Likewise, if the acceptable error was set at 2%, then the population proportion would be between 24 and 28 percent with a 90 percent level of confidence, but would require that the sample size be increased from 271 to 1,691. If we wished a higher level of confidence, we would require a larger sample size. Moving from a 90 percent level of confidence to a 95 percent level at a plus or minus 5% tolerance requires changing the sample size from 271 to 384. A very common sample size often seen reported in political surveys is 384. With the survey results it is frequently stated that the results are good to a plus or minus 5% level of \u00e2\u0080\u009caccuracy\u00e2\u0080\u009d. \nExample \n8.9\n \n\n\nSuppose a mobile phone company wants to determine the current percentage of customers aged 50+ who use text messaging on their cell phones. How many customers aged 50+ should the company survey in order to be 90% confident that the estimated (sample) proportion is within three percentage points of the true population proportion of customers aged 50+ who use text messaging on their cell phones.\n\n\n\nSolution\n1\n\n\nFrom the problem, we know that the acceptable error, e, is 0.03 (3%=0.03) and\n   \n\n\nz\n\n\n\u00ce\u00b1\n2\n\n\n\n\n\n\nz\n\n\n\u00ce\u00b1\n2\n\n\n\n z0.05 = 1.645 because the confidence level is 90%. The acceptable error, e, is the difference between the actual population proportion p, and the sample proportion we expect to get from the sample.However, in order to find n, we need to know the estimated (sample) proportion p\u00e2\u0080\u00b2. Remember that q\u00e2\u0080\u00b2 = 1 \u00e2\u0080\u0093 p\u00e2\u0080\u00b2. But, we do not know p\u00e2\u0080\u00b2 yet. Since we multiply p\u00e2\u0080\u00b2 and q\u00e2\u0080\u00b2 together, we make them both equal to 0.5 because p\u00e2\u0080\u00b2q\u00e2\u0080\u00b2 = (0.5)(0.5) = 0.25 results in the largest possible product. (Try other products: (0.6)(0.4) = 0.24; (0.3)(0.7) = 0.21; (0.2)(0.8) = 0.16 and so on). The largest possible product gives us the largest n. This gives us a large enough sample so that we can be 90% confident that we are within three percentage points of the true population proportion. To calculate the sample size n, use the formula and make the substitutions.\n\n\nn=\n\n\nz\n2\n\n\np\n\u00e2\u0080\u00b2\n\n\nq\n\u00e2\u0080\u00b2\n\n\n\n\ne\n2\n\n\n\n\n\nn=\n\n\nz\n2\n\n\np\n\u00e2\u0080\u00b2\n\n\nq\n\u00e2\u0080\u00b2\n\n\n\n\ne\n2\n\n\n\n gives \n\nn=\n\n\n\n1.645\n\n2\n\n(0.5)(0.5)\n\n\n\n\n0.03\n\n2\n\n\n\n=751.7\n\n\nn=\n\n\n\n1.645\n\n2\n\n(0.5)(0.5)\n\n\n\n\n0.03\n\n2\n\n\n\n=751.7\nRound the answer to the next higher value. The sample size should be 752 cell phone customers aged 50+ in order to be 90%  confident that the estimated (sample) proportion is within three percentage points of the true population proportion of all customers aged 50+ who use text messaging on their cell phones.\n\n\n\n\nTry It \n8.9\n\n\n\n\nSuppose an internet marketing company wants to determine the current percentage of customers who click on ads on their smartphones. How many customers should the company survey in order to be 90% confident that the estimated proportion is within five percentage points of the true population proportion of customers who click on ads on their smartphones?\n  \n\n\n\n"}, {"chapter": "10.1", "title": "Comparing Two Independent Population Means", "mathml": "<math display=\"block\"><semantics><mrow>\n<mi>E</mi>\n<mo>(</mo>\n<msub><mi>\u00c2\u00b5</mi><mrow><msub><mover><mi>x</mi><mo>-</mo></mover><mn>1</mn></msub></mrow></msub>\n<mo>-</mo>\n<msub><mi>\u00c2\u00b5</mi><mrow><msub><mover><mi>x</mi><mo>-</mo></mover><mn>2</mn></msub></mrow></msub>\n<mo>)</mo>\n<mo>=</mo>\n<msub><mi>\u00c2\u00b5</mi><mn>1</mn></msub>\n<mo>-</mo>\n<msub><mi>\u00c2\u00b5</mi><mn>2</mn></msub>\n</mrow><annotation-xml encoding=\"MathML-Content\"><mi>E</mi><mo>(</mo><msub><mi>\u00c2\u00b5</mi><mrow><msub><mover><mi>x</mi><mo>-</mo></mover><mn>1</mn></msub></mrow></msub><mo>-</mo><msub><mi>\u00c2\u00b5</mi><mrow><msub><mover><mi>x</mi><mo>-</mo></mover><mn>2</mn></msub></mrow></msub><mo>)</mo><mo>=</mo><msub><mi>\u00c2\u00b5</mi><mn>1</mn></msub><mo>-</mo><msub><mi>\u00c2\u00b5</mi><mn>2</mn></msub></annotation-xml></semantics></math></div", "content": "\nThe comparison of two independent population means is very common and provides a way to test the hypothesis that the two groups differ from each other. Is the night shift less productive than the day shift, are the rates of return from fixed asset investments different from those from common stock investments, and so on? An observed difference between two sample means depends on both the means and the sample standard deviations. Very different means can occur by chance if there is great variation among the individual samples. The test statistic will have to account for this fact. The test comparing two independent population means with unknown and possibly unequal population standard deviations is called the Aspin-Welch t-test.  The degrees of freedom formula we will see later was developed by Aspin-Welch.When we developed the hypothesis test for the mean and proportions we began with the Central Limit Theorem. We recognized that a sample mean came from a distribution of sample means, and sample proportions came from the sampling distribution of sample proportions. This made our sample parameters, the sample means and sample proportions, into random variables. It was important for us to know the distribution that these random variables came from. The Central Limit Theorem gave us the answer: the normal distribution. Our Z and t statistics came from this theorem. This provided us with the solution to our question of how to measure the probability that a sample mean came from a distribution with a particular hypothesized value of the mean or proportion. In both cases that was the question: what is the probability that the mean (or proportion) from our sample data came from a population distribution with the hypothesized value we are interested in? Now we are interested in whether or not two samples have the same mean. Our question has not changed: Do these two samples come from the same population distribution? To approach this problem we create a new random variable. We recognize that we have two sample means, one from each set of data, and thus we have two random variables coming from two unknown distributions. To solve the problem we create a new random variable, the difference between the sample means. This new random variable also has a distribution and, again, the Central Limit Theorem tells us that this new distribution is normally distributed, regardless of the underlying distributions of the original data. A graph may help to understand this concept.  \nFigure \n10.2\n\nPictured are two distributions of data, X1 and X2, with unknown means and standard deviations. The second panel shows the sampling distribution of the newly created random variable (X-1-X-2X-1-X-2). This distribution is the theoretical distribution of many sample means from population 1 minus sample means from population 2. The Central Limit Theorem tells us that this theoretical sampling distribution of differences in sample means is normally distributed, regardless of the distribution of the actual population data shown in the top panel. Because the sampling distribution is normally distributed, we can develop a standardizing formula and calculate probabilities from the standard normal distribution in the bottom panel, the Z distribution. We have seen this same analysis before in Chapter 7  Figure 7.2 .The Central Limit Theorem, as before, provides us with the standard deviation of the sampling distribution, and further, that the expected value of the mean of the distribution of differences in sample means is equal to the differences in the population means.   Mathematically this can be stated:\nE\n(\n\u00c2\u00b5x-1\n-\n\u00c2\u00b5x-2\n)\n=\n\u00c2\u00b51\n-\n\u00c2\u00b52\nE(\u00c2\u00b5x-1-\u00c2\u00b5x-2)=\u00c2\u00b51-\u00c2\u00b52Because we do not know the population standard deviations, we estimate them using the two sample standard deviations from our independent samples. For the hypothesis test, we calculate the estimated standard deviation, or standard error, of the difference in sample means, \n\n\nX\n\u00c2\u00af\n\n1\n\n\n\nX\n\u00c2\u00af\n\n1\n \u00e2\u0080\u0093 \n\n\nX\n\u00c2\u00af\n\n2\n\n\n\nX\n\u00c2\u00af\n\n2\n.\nThe standard error is:\n\n\n\n(\n\ns\n1\n\n\n)\n2\n\n\n\n\nn\n1\n\n\n\n+\n\n\n(\n\ns\n2\n\n\n)\n2\n\n\n\n\nn\n2\n\n\n\n\n\n\n\n(\n\ns\n1\n\n\n)\n2\n\n\n\n\nn\n1\n\n\n\n+\n\n\n(\n\ns\n2\n\n\n)\n2\n\n\n\n\nn\n2\n\n\n\nWe remember that substituting the sample variance for the population variance when we did not have the population variance was the technique we used when building the confidence interval and the test statistic for the test of hypothesis for a single mean back in Confidence Intervals and Hypothesis Testing with One Sample. The test statistic (t-score) is calculated as follows:\n\ntc=\n\n\n\n(\n\nx\n\u00c2\u00af\n\n1\n\n\u00e2\u0080\u0093\n\nx\n\u00c2\u00af\n\n2\n\n)\u00e2\u0080\u0093\u00ce\u00b40\n\n\n\n\n\n\n\n\n(\ns\n1\n\n)\n\n2\n\n\n\n\nn\n1\n\n\n\n+\n\n\n\n(\ns\n2\n\n)\n\n2\n\n\n\n\nn\n2\n\n\n\n\n\n\n\n\ntc=\n\n\n(\n\nx\n\u00c2\u00af\n\n1\n\n\u00e2\u0080\u0093\n\nx\n\u00c2\u00af\n\n2\n\n)\u00e2\u0080\u0093\u00ce\u00b40\n\n\n\n\n\n\n\n\n(\ns\n1\n\n)\n\n2\n\n\n\n\nn\n1\n\n\n\n+\n\n\n\n(\ns\n2\n\n)\n\n2\n\n\n\n\nn\n2\n\n\n\n\n\n\n\nwhere:s1 and s2, the sample standard deviations, are estimates of \u00cf\u00831 and \u00cf\u00832, respectively and\n\u00cf\u00831 and \u00cf\u00832 are the unknown population standard deviations.\n\n\n\nx\n\u00c2\u00af\n\n1\n\n\n\nx\n\u00c2\u00af\n\n1\n\nand\n\n\n\nx\n\u00c2\u00af\n\n2\n\n\n\nx\n\u00c2\u00af\n\n2\n\nare the sample means. \u00ce\u00bc1 and \u00ce\u00bc2 are the unknown population means.\nThe number of degrees of freedom (df) requires a somewhat complicated calculation. The df are not always a whole number. The test statistic above is approximated by the Student's t-distribution with df as follows:\nThe standard error is:\n\ndf=\n\n\n\n(\n\n\n\n\n\n(\ns\n1\n\n)\n\n2\n\n\n\n\nn\n1\n\n\n\n+\n\n\n\n(\ns\n2\n\n)\n\n2\n\n\n\n\nn\n2\n\n\n\n\n)\n\n2\n\n\n\n(\n\n\n1\n\n\nn\n1\n\n\u00e2\u0080\u00931\n\n\n\n)\n\n(\n\n\n\n\n(\ns\n1\n\n)\n2\n\n\n\nn\n1\n\n\n\n\n)\n\n2\n\n+(\n\n\n1\n\n\nn\n2\n\n\u00e2\u0080\u00931\n\n\n\n)\n\n(\n\n\n\n\n\n(\ns\n2\n\n)\n\n2\n\n\n\n\nn\n2\n\n\n\n\n)\n\n2\n\n\n\n\n\ndf=\n\n\n\n(\n\n\n\n\n\n(\ns\n1\n\n)\n\n2\n\n\n\n\nn\n1\n\n\n\n+\n\n\n\n(\ns\n2\n\n)\n\n2\n\n\n\n\nn\n2\n\n\n\n\n)\n\n2\n\n\n\n(\n\n\n1\n\n\nn\n1\n\n\u00e2\u0080\u00931\n\n\n\n)\n\n(\n\n\n\n\n(\ns\n1\n\n)\n2\n\n\n\nn\n1\n\n\n\n\n)\n\n2\n\n+(\n\n\n1\n\n\nn\n2\n\n\u00e2\u0080\u00931\n\n\n\n)\n\n(\n\n\n\n\n\n(\ns\n2\n\n)\n\n2\n\n\n\n\nn\n2\n\n\n\n\n)\n\n2\n\n\n\nWhen both sample sizes n1 and n2 are 30 or larger, the Student's t approximation is very good. If each sample has more than 30 observations then the degrees of freedom can be calculated as n1 + n2 - 2. The format of the sampling distribution, differences in sample means,  specifies that the format of the null and alternative hypothesis is:\nH0\n:\n\u00c2\u00b51\n-\n\u00c2\u00b52\n=\n\u00ce\u00b40\nH0:\u00c2\u00b51-\u00c2\u00b52=\u00ce\u00b40\nHa\n:\n\u00c2\u00b51\n-\n\u00c2\u00b52\n\u00e2\u0089\u00a0\n\u00ce\u00b40\nHa:\u00c2\u00b51-\u00c2\u00b52\u00e2\u0089\u00a0\u00ce\u00b40where \u00ce\u00b40 is the hypothesized difference between the two means. If the question is simply \u00e2\u0080\u009cis there any difference between the means?\u00e2\u0080\u009d then \u00ce\u00b40 = 0 and the null and alternative hypotheses becomes:\nH0\n:\n\u00c2\u00b51\n=\n\u00c2\u00b52\nH0:\u00c2\u00b51=\u00c2\u00b52\nHa\n:\n\u00c2\u00b51\n\u00e2\u0089\u00a0\n\u00c2\u00b52\nHa:\u00c2\u00b51\u00e2\u0089\u00a0\u00c2\u00b52An example of when \u00ce\u00b40 might not be zero is when the comparison of the two groups requires a specific difference for the decision to be meaningful. Imagine that you are making a capital investment. You are considering changing from your current model machine to another. You measure the productivity of your machines by the speed they produce the product. It may be that a contender to replace the old model is faster in terms of product throughput, but is also more expensive. The second machine may also have more maintenance costs, setup costs, etc. The null hypothesis would be set up so that the new machine would have to be better than the old one by enough to cover these extra costs in terms of speed and cost of production. This form of the null and alternative hypothesis shows how valuable this particular hypothesis test can be. For most of our work we will be testing simple hypotheses asking if there is any difference between the two distribution means. \nExample \n10.1\n \n\nIndependent groupsThe Kona Iki Corporation produces coconut milk. They take coconuts and extract the milk inside by drilling a hole and pouring the milk into a vat for processing. They have both a day shift (called the B shift) and a night shift (called the G shift) to do this part of the process. They would like to know if the day shift and the night shift are equally efficient in processing the coconuts. A study is done sampling 9 shifts of the G shift and 16 shifts of the B shift. The results of the number of hours required to process 100 pounds of coconuts is presented in Table 10.1.  A study is done and data are collected, resulting in the data in Table 10.1.\n\n\nSample Size\nAverage Number of Hours to Process 100 Pounds of Coconuts\nSample Standard Deviation\n\n\n\nG Shift\n9\n2\n0.8660.866\n\n\nB Shift\n16\n3.2\n1.00\n\n\nTable \n10.1\n \n \n\n\nIs there a difference in the mean amount of time for each shift to process 100 pounds of coconuts? Test at the 5% level of significance.\n\n\n\nSolution\n1\n\n\nThe population standard deviations are not known and cannot be assumed to equal each other. Let g be the subscript for the G Shift and b be the subscript for the B Shift. Then, \u00ce\u00bcg is the population mean for G Shift and \u00ce\u00bcb is the population mean for B Shift. This is a test of two independent groups, two population means.Random variable:\n  \n\n\n\n\nX\n\u00c2\u00af\n\n\ng\n\n\u00e2\u0088\u0092\n\n\nX\n\u00c2\u00af\n\n\nb\n\n\n\n\n\n\nX\n\u00c2\u00af\n\n\ng\n\n\u00e2\u0088\u0092\n\n\nX\n\u00c2\u00af\n\n\nb\n\n\n = difference in the sample mean amount of time between the G Shift and the B Shift takes to process the coconuts.\n\nH0: \u00ce\u00bcg = \u00ce\u00bcb\u00e2\u0080\u0083\u00e2\u0080\u0083H0: \u00ce\u00bcg \u00e2\u0080\u0093 \u00ce\u00bcb = 0\n\nHa: \u00ce\u00bcg \u00e2\u0089\u00a0 \u00ce\u00bcb\u00e2\u0080\u0083\u00e2\u0080\u0083Ha: \u00ce\u00bcg \u00e2\u0080\u0093 \u00ce\u00bcb \u00e2\u0089\u00a0 0\nThe words \"the same\" tell you H0 has an \"=\". Since there are no other words to indicate Ha, is either faster or slower. This is a two tailed test.\nDistribution for the test:\nUse tdf where df is calculated using the df formula for independent groups, two population means above. Using a calculator, df is approximately 18.8462. \nGraph:\n\n\n\n\n\nFigure \n10.3\n\n\n\n\ntc\n=\n\n(X-1\u00e2\u0088\u0092X-2)\u00e2\u0088\u0092\u00ce\u00b40\n\nS12n1\n+\nS22n2\n\n\n\n=\n-3.01\n\n\n\nt\nc\n\n=\n\n(X-1\u00e2\u0088\u0092X-2)\u00e2\u0088\u0092\u00ce\u00b40\n\nS12n1\n+\nS22n2\n\n\n\n=\n-3.01\n\n\n\nWe next find the critical value on the t-table using the degrees of freedom from above. The critical value, 2.093, is found in the .025 column, this is \u00ce\u00b1/2, at 19 degrees of freedom. (The convention is to round up the degrees of freedom to make the conclusion more conservative.) Next we calculate the test statistic and mark this on the t-distribution graph. Make a decision: Since the calculated t-value is in the tail we cannot accept the null hypothesis that there is no difference between the two groups. The means are different.The graph has included the sampling distribution of the differences in the sample means to show how the t-distribution aligns with the sampling distribution data. We see in the top panel that the calculated difference in the two means is -1.2 and the bottom panel shows that this is 3.01 standard deviations from the mean. Typically we do not need to show the sampling distribution graph and can rely on the graph of the test statistic, the t-distribution in this case, to reach our conclusion.Conclusion: At the 5% level of significance, the sample data show there is sufficient evidence to conclude that the mean number of hours that the G Shift takes to process 100 pounds of coconuts is different from the B Shift (mean number of hours for the B Shift is greater than the mean number of hours for the G Shift).\n\n\n\nNOTE\n\n\nWhen the sum of the sample sizes is larger than 30 (n1 + n2 > 30) you can use the normal distribution to approximate the Student's t.\n\nExample \n10.2\n \n\nA study is done to determine if Company A retains its workers longer than Company B. It is believed that Company A has a higher retention than Company B. The study finds that in a sample of 11 workers at Company A their average time with the company is four years with a standard deviation of 1.5 years. A sample of 9 workers at Company B finds that the average time with the company was 3.5 years with a standard deviation of 1 year. Test this proposition at the 1% level of significance.\na. Is this a test of two means or two proportions?\n\n\n\n\nSolution\n1\n\n\na. two means because time is a continuous random variable.\n\n\n\n\nb. Are the populations standard deviations known or unknown?\n\n\n\nSolution\n2\n\n\nb. unknown\n\n\n\n\n\n\nc. Which distribution do you use to perform the test?\n\n\n\nSolution\n3\n\nc. Student's t\n\n\n\n\n\n\nd. What is the random variable?\n\n\n\n\nSolution\n4\n\n\nd. \n\n\n\nX\n\u00c2\u00af\n\nA\n\n-\n\nX\n\u00c2\u00af\n\nB\n\n\n\n\n\nX\n\u00c2\u00af\n\nA\n\n-\n\nX\n\u00c2\u00af\n\nB\n\n\n\n\n\n\n\ne. What are the null and alternate hypotheses? \n\n\n\n\nSolution\n5\n\n\ne. \n\n\n\nH\no\n\n:\n\u00ce\u00bc\nA\n\n\u00e2\u0089\u00a4\n\u00ce\u00bc\nB\n\n\n\n\nH\no\n\n:\n\u00ce\u00bc\nA\n\n\u00e2\u0089\u00a4\n\u00ce\u00bc\nB\n\n\n\n\n\n\n\nH\na\n\n:\n\u00ce\u00bc\nA\n\n>\n\u00ce\u00bc\nB\n\n\n\n\nH\na\n\n:\n\u00ce\u00bc\nA\n\n>\n\u00ce\u00bc\nB\n\n\n\n\n\n\n\n\n\n\n\n\nf. Is this test right-, left-, or two-tailed?\n\n\n\nSolution\n6\n\nf. right one-tailed test\n\n\n\n\nFigure \n10.4\n\n\n\n\n\n\n\n\ng. What is the value of the test statistic?\n\n\n\nSolution\n7\n\n\n\n\ntc\n=\n\n(X-1\u00e2\u0088\u0092X-2)\u00e2\u0088\u0092\u00ce\u00b40\n\nS12n1\n+\nS22n2\n\n\n\n=\n0.89\n\n\n\nt\nc\n\n=\n\n(X-1\u00e2\u0088\u0092X-2)\u00e2\u0088\u0092\u00ce\u00b40\n\nS12n1\n+\nS22n2\n\n\n\n=\n0.89\n\n\n\n\n\n\n\n\nh. Can you accept/reject the null hypothesis?\n\n\n\nSolution\n8\n\nh. Cannot reject the null hypothesis that there is no difference between the two groups. Test statistic is not in the tail. The critical value of the t-distribution is 2.764 with 10 degrees of freedom. This example shows how difficult it is to reject a null hypothesis with a very small sample. The critical values require very large test statistics to reach the tail.  \n\n\n\n\ni. Conclusion:\n\n\n\nSolution\n9\n\ni. At the 1% level of significance, from the sample data, there is not sufficient evidence to conclude that the retention of workers at Company A is longer than Company B, on average.\n\n\n\nExample \n10.3\n \n\n\nAn interesting research question is the effect, if any, that different types of teaching formats have on the grade outcomes of students. To investigate this issue one sample of students' grades was taken from a hybrid class and another sample taken from a standard lecture format class. Both classes were for the same subject. The mean course grade in percent for the 35 hybrid students is 74 with a standard deviation of 16. The mean grades of the 40 students form the standard lecture class was 76 percent with a standard deviation of 9. Test at 5% to see if there is any significant difference in the population mean grades between standard lecture course and hybrid class.\n  \n\n\n\nSolution\n1\n\n\nWe begin by noting that we have two groups, students from a hybrid class and students from a standard lecture format class. We also note that the random variable, what we are interested in, is students' grades, a continuous random variable. We could have asked the research question in a different way and had a binary random variable. For example, we could have studied the percentage of students with a failing grade, or with an A grade. Both of these would be binary and thus a test of proportions and not a test of means as is the case here. Finally, there is no presumption as to which format might lead to higher grades so the hypothesis is stated as a two-tailed test.\n\nH0: \u00c2\u00b51 = \u00c2\u00b52\nHa: \u00c2\u00b51 \u00e2\u0089\u00a0 \u00c2\u00b52\nAs would virtually always be the case, we do not know the population variances of the two distributions and thus our test statistic is:\ntc=(x\u00c2\u00af1\u00e2\u0088\u0092x\u00c2\u00af2)\u00e2\u0088\u0092\u00ce\u00b40s2n1+s2n2=(74\u00e2\u0088\u009276)\u00e2\u0088\u0092016235+9240=\u00e2\u0088\u00920.65tc=(x\u00c2\u00af1\u00e2\u0088\u0092x\u00c2\u00af2)\u00e2\u0088\u0092\u00ce\u00b40s2n1+s2n2=(74\u00e2\u0088\u009276)\u00e2\u0088\u0092016235+9240=\u00e2\u0088\u00920.65\n\nTo determine the critical value of the Student's t we need the degrees of freedom. For this case we use:\ndf = n1 + n2 - 2 = 35 + 40 -2 = 73. This is large enough to consider it the normal distribution thus ta/2 = 1.96. Again as always we determine if the calculated value is in the tail determined by the critical value. In this case we do not even need to look up the critical value: the calculated value of the difference in these two average grades is not even one standard deviation apart. Certainly not in the tail.\n\nConclusion: Cannot reject the null at \u00ce\u00b1=5%. Therefore, evidence does not exist to prove that the grades in hybrid and standard classes differ.\n\n\n\n\n"}, {"chapter": "10.3", "title": "Test for Differences in Means: Assuming Equal Population Variances", "mathml": "<math display=\"block\"><semantics><mrow><msub><mi>t</mi><mi>c</mi></msub><mo>=</mo><mfrac><mrow><mo>(</mo><msub><mover><mi>x</mi><mo>\u00c2\u00af</mo></mover><mn>1</mn></msub><mo>\u00e2\u0088\u0092</mo><msub><mover><mi>x</mi><mo>\u00c2\u00af</mo></mover><mn>2</mn></msub><mo>)</mo><mo>\u00e2\u0088\u0092</mo><msub><mi>\u00ce\u00b4</mi><mn>0</mn></msub></mrow><msqrt><mrow><mi>S</mi><mover><mi>p</mi><mn>2</mn></mover><mo>(</mo><mfrac><mn>1</mn><msub><mi>n</mi><mn>1</mn></msub></mfrac><mo>+</mo><mfrac><mn>1</mn><msub><mi>n</mi><mn>2</mn></msub></mfrac><mo>)</mo></mrow></msqrt></mfrac></mrow><annotation-xml encoding=\"MathML-Content\"><msub><mi>t</mi><mi>c</mi></msub><mo>=</mo><mfrac><mrow><mo>(</mo><msub><mover><mi>x</mi><mo>\u00c2\u00af</mo></mover><mn>1</mn></msub><mo>\u00e2\u0088\u0092</mo><msub><mover><mi>x</mi><mo>\u00c2\u00af</mo></mover><mn>2</mn></msub><mo>)</mo><mo>\u00e2\u0088\u0092</mo><msub><mi>\u00ce\u00b4</mi><mn>0</mn></msub></mrow><msqrt><mrow><mi>S</mi><mover><mi>p</mi><mn>2</mn></mover><mo>(</mo><mfrac><mn>1</mn><msub><mi>n</mi><mn>1</mn></msub></mfrac><mo>+</mo><mfrac><mn>1</mn><msub><mi>n</mi><mn>2</mn></msub></mfrac><mo>)</mo></mrow></msqrt></mfrac></annotation-xml></semantics></math></div", "content": "\n\n  Typically we can never expect to know any of the population parameters, mean, proportion, or standard deviation. When testing hypotheses concerning differences in means we are faced with the difficulty of two unknown variances that play a critical role in the test statistic. We have been substituting the sample variances just as we did when testing hypotheses for a single mean. And as we did before, we used a Student's t to compensate for this lack of information on the population variance. There may be situations, however, when we do not know the population variances, but we can assume that the two populations have the same variance. If this is true then the pooled sample variance will be smaller than the individual sample variances. This will give more precise estimates and reduce the probability of discarding a good null. The null and alternative hypotheses remain the same, but the test statistic changes to: tc=(x\u00c2\u00af1\u00e2\u0088\u0092x\u00c2\u00af2)\u00e2\u0088\u0092\u00ce\u00b40Sp2(1n1+1n2)tc=(x\u00c2\u00af1\u00e2\u0088\u0092x\u00c2\u00af2)\u00e2\u0088\u0092\u00ce\u00b40Sp2(1n1+1n2)where Sp2Sp2 is the pooled variance given by the formula:\nSp2=(n1\u00e2\u0088\u00921)s12-(n2\u00e2\u0088\u00921)s22n1+n2\u00e2\u0088\u00922Sp2=(n1\u00e2\u0088\u00921)s12-(n2\u00e2\u0088\u00921)s22n1+n2\u00e2\u0088\u00922\n\nExample \n10.5\n \n\n\nA drug trial is attempted using a real drug and a pill made of just sugar. 18 people are given the real drug in hopes of increasing the production of endorphins. The increase in endorphins is found to be on average 8 micrograms per person, and the sample standard deviation is 5.4 micrograms. 11 people are given the sugar pill, and their average endorphin increase is 4 micrograms with a standard deviation of 2.4. From previous research on endorphins it is determined that it can be assumed that the variances within the two samples can be assumed to be the same. Test at 5% to see if the population mean for the real drug had a significantly greater impact on the endorphins than the population mean with the sugar pill.\n  \n\n\n\nSolution\n1\n\n\nFirst we begin by designating one of the two groups Group 1 and the other Group 2. This will be needed to keep track of the null and alternative hypotheses. Let's set Group 1 as those who received the actual new medicine being tested and therefore Group 2 is those who received the sugar pill. We can now set up the null and alternative hypothesis as:\n\nH0: \u00c2\u00b51 \u00e2\u0089\u00a4 \u00c2\u00b52\n\nH1: \u00c2\u00b51 > \u00c2\u00b52\n\nThis is set up as a one-tailed test with the claim in the alternative hypothesis that the medicine will produce more endorphins than the sugar pill. We now calculate the test statistic which requires us to calculate the pooled variance, Sp2Sp2 using the formula above.\ntc=\n(x\u00c2\u00af1\u00e2\u0088\u0092x\u00c2\u00af2)\u00e2\u0088\u0092\u00ce\u00b40\nSp2(\n1n1\n+\n1n2\n)\n=\n\n(8\u00e2\u0088\u00924)\u00e2\u0088\u00920\n20.4933(\n118\n+\n111)\n\n=2.31tc=(x\u00c2\u00af1\u00e2\u0088\u0092x\u00c2\u00af2)\u00e2\u0088\u0092\u00ce\u00b40\nSp2(\n1n1\n+\n1n2\n)\n=\n(8\u00e2\u0088\u00924)\u00e2\u0088\u00920\n20.4933(\n118\n+\n111)\n=2.31\nt\u00ce\u00b1, allows us to compare the test statistic and the critical value.\n\n\n\nt\u00ce\u00b1=1.703 at df=n1+n2\u00e2\u0088\u00922=18+11\u00e2\u0088\u00922=27\n\n\nt\n\u00ce\u00b1\n\n=\n1.703\n\nat\n\nd\nf\n=\n\nn\n1\n\n+\n\nn\n2\n\n\u00e2\u0088\u0092\n2\n=\n18\n+\n11\n\u00e2\u0088\u0092\n2\n=\n27\n\n\n\nThe test statistic is clearly in the tail, 2.31 is larger than the critical value of 1.703, and therefore we cannot maintain the null hypothesis. Thus, we conclude that there is significant evidence at the 95% level of confidence that the new medicine produces the effect desired.\n\n\n\n\n"}, {"chapter": "10.4", "title": "Comparing Two Independent Population Proportions", "mathml": "<math display=\"block\"><semantics><mrow>\n<msub><mi>H</mi><mn>0</mn></msub>\n<mo>:</mo>\n<msub><mi>p</mi><mn>1</mn></msub>\n<mo>\u00e2\u0088\u0092</mo>\n<msub><mi>p</mi><mn>2</mn></msub>\n<mo>=</mo>\n<msub><mi>\u00f0\u009d\u009b\u00bf</mi><mn>0</mn></msub>\n</mrow><annotation-xml encoding=\"MathML-Content\"><msub><mi>H</mi><mn>0</mn></msub><mo>:</mo><msub><mi>p</mi><mn>1</mn></msub><mo>\u00e2\u0088\u0092</mo><msub><mi>p</mi><mn>2</mn></msub><mo>=</mo><msub><mi>\u00f0\u009d\u009b\u00bf</mi><mn>0</mn></msub></annotation-xml></semantics></math></div", "content": "\nWhen conducting a hypothesis test that compares two independent population proportions, the following characteristics should be present:\nThe two independent samples are random samples that are independent.\n The number of successes is at least five, and the number of failures is at least five, for each of the samples.\nGrowing literature states that the population must be at least ten or even perhaps 20 times the size of the sample. This keeps each population from being over-sampled and causing biased results.\nComparing two proportions, like comparing two means, is common. If two estimated proportions are different, it may be due to a difference in the populations or it may be due to chance in the sampling. A hypothesis test can help determine if a difference in the estimated proportions reflects a difference in the two population proportions.Like the case of differences in sample means, we construct a sampling distribution for differences in sample proportions: (pA'-pB')(pA'-pB')where \np\nA\n'\n\n=\n\nX\n\n\nA\n\n\n\nn\nA\n\n\n\n\np\nA\n'\n=\nX\n\n\nA\n\n\n\nn\nA\n\n\n\n and \n\np\nB\n'\n\n=\n\nX\n\n\nB\n\n\n\nn\nB\n\n\n\n\n\np\nB\n'\n=\nX\n\n\nB\n\n\n\nn\nB\n\n\n\n are the sample proportions for the two sets of data in question.  XA and XB are the number of successes in each sample group respectively, and nA and nB are the respective sample sizes from the two groups. Again we go the Central Limit theorem to find the distribution of this sampling distribution for the differences in sample proportions. And again we find that this sampling distribution, like the ones past, are normally distributed as proved by the Central Limit Theorem, as seen in Figure 10.5 .\nFigure \n10.5\n\nGenerally, the null hypothesis allows for the test of a difference of a particular value, \u00f0\u009d\u009b\u00bf0, just as we did for the case of differences in means.\nH0\n:\np1\n\u00e2\u0088\u0092\np2\n=\n\u00f0\u009d\u009b\u00bf0\nH0:p1\u00e2\u0088\u0092p2=\u00f0\u009d\u009b\u00bf0\nH1\n:\np1\n\u00e2\u0088\u0092\np2\n\u00e2\u0089\u00a0\n\u00f0\u009d\u009b\u00bf0\nH1:p1\u00e2\u0088\u0092p2\u00e2\u0089\u00a0\u00f0\u009d\u009b\u00bf0Most common, however, is the test that the two proportions are the same. That is,  \n\nH\n0\n\n:\n\np\nA\n\n=\n\np\nB\n\n\nH\n0\n:\np\nA\n=\np\nB\n\n\nH\na\n\n:\n\np\nA\n\n\u00e2\u0089\u00a0\n\np\nB\n\n\nH\na\n:\np\nA\n\u00e2\u0089\u00a0\np\nB\nTo conduct the test, we use a pooled proportion, pc. The pooled proportion is calculated as follows:\n\n\np\nc\n\n=\n\n\nx\nA\n\n+\nx\nB\n\n\n\n\nn\nA\n\n+\nn\nB\n\n\n\n\n\n\np\nc\n\n=\n\n\nx\nA\n\n+\nx\nB\n\n\n\n\nn\nA\n\n+\nn\nB\n\n\n\n\n\nThe test statistic (z-score) is:\nZc\n=\n\n\n(\n\np\nA\n\u00e2\u0080\u00b2\n\n\u00e2\u0088\u0092\n\np\nB\n\u00e2\u0080\u00b2\n\n)\n\u00e2\u0088\u0092\n\u00ce\u00b40\n\n\n\n\n\np\nc\n\n(1\u00e2\u0088\u0092\np\nc\n\n)(\n1\n\n\nn\nA\n\n\n\n+\n1\n\n\nn\nB\n\n\n\n)\n\n\n\n\nZc=\n\n(\n\np\nA\n\u00e2\u0080\u00b2\n\n\u00e2\u0088\u0092\n\np\nB\n\u00e2\u0080\u00b2\n\n)\n\u00e2\u0088\u0092\n\u00ce\u00b40\n\n\n\n\n\np\nc\n\n(1\u00e2\u0088\u0092\np\nc\n\n)(\n1\n\n\nn\nA\n\n\n\n+\n1\n\n\nn\nB\n\n\n\n)\n\n\n\n\nwhere \u00ce\u00b40 is the hypothesized differences between the two proportions and pc is the pooled variance from the formula above.\nExample \n10.6\n \n\n\n\n\nA bank has recently acquired a new branch and thus has customers in this new territory. They are interested in the default rate in their new territory. They wish to test the hypothesis that the default rate is different from their current customer base. They sample 200 files in area A, their current customers, and find that 20 have defaulted. In area B, the new customers, another sample of 200 files shows 12 have defaulted on their loans. At a 10% level of significance can we say that the default rates are the same or different?  \n\n\n\nSolution\n1\n\n\nThis is a test of proportions. We know this because the underlying random variable is binary, default or not default.  Further, we know it is a test of differences in proportions because we have two sample groups, the current customer base and the newly acquired customer base. Let A and B be the subscripts for the two customer groups. Then pA and pB are the two population proportions we wish to test. Random Variable:P\u00e2\u0080\u00b2A \u00e2\u0080\u0093 P\u00e2\u0080\u00b2B = difference in the proportions of customers who defaulted in the two groups.\n\n\n\n\nH\n0\n\n:\n\np\nA\n\n=\n\np\nB\n\n\n\n\nH\n0\n\n:\n\np\nA\n\n=\n\np\nB\n\n\n\n\n\n\n\n\n\nH\na\n\n:\n\np\nA\n\n\u00e2\u0089\u00a0\n\np\nB\n\n\n\n\nH\na\n\n:\n\np\nA\n\n\u00e2\u0089\u00a0\n\np\nB\n\n\n\n\nThe words \"is a difference\" tell you the test is two-tailed.\nDistribution for the test: Since this is a test of two binomial population proportions, the distribution is normal:\n\n\n\np\nc\n\n=\n\n\nx\nA\n\n+\nx\nB\n\n\n\n\nn\nA\n\n+\nn\nB\n\n\n\n=\n\n20+12\n\n\n200+200\n\n\n=0.08\u00e2\u0080\u00831\u00e2\u0080\u0093\np\nc\n\n=0.92\n\n\n\np\nc\n\n=\n\n\nx\nA\n\n+\nx\nB\n\n\n\n\nn\nA\n\n+\nn\nB\n\n\n\n=\n\n20+12\n\n\n200+200\n\n\n=0.08\u00e2\u0080\u00831\u00e2\u0080\u0093\np\nc\n\n=0.92\n\n\n(p\u00e2\u0080\u00b2A \u00e2\u0080\u0093 p\u00e2\u0080\u00b2B) = 0.04 follows an approximate normal distribution.Estimated proportion for group A: \n\n\n\np\n\u00e2\u0080\u00b2\n\nA\n\n=\n\n\nx\nA\n\n\n\n\nn\nA\n\n\n\n=\n\n20\n\n\n200\n\n\n=0.1\n\n\n\n\np\n\u00e2\u0080\u00b2\n\nA\n\n=\n\n\nx\nA\n\n\n\n\nn\nA\n\n\n\n=\n\n20\n\n\n200\n\n\n=0.1\n\nEstimated proportion for group B: \n\n\n\np\n\u00e2\u0080\u00b2\n\nB\n\n=\n\n\nx\nB\n\n\n\n\nn\nB\n\n\n\n=\n\n12\n\n\n200\n\n\n=0.06\n\n\n\n\np\n\u00e2\u0080\u00b2\n\nB\n\n=\n\n\nx\nB\n\n\n\n\nn\nB\n\n\n\n=\n\n12\n\n\n200\n\n\n=0.06\n\nThe estimated difference between the two groups is : p\u00e2\u0080\u00b2A  \u00e2\u0080\u0093 p\u00e2\u0080\u00b2B = 0.1 \u00e2\u0080\u0093 0.06 = 0.04.\n\n\n\n\nFigure \n10.6\n\n\n\n\nZc\n=\n\n(P\u00e2\u0080\u00b2A\u00e2\u0088\u0092P\u00e2\u0080\u00b2B)\u00e2\u0088\u0092\u00ce\u00b40\n\nPc(1\u00e2\u0088\u0092Pc)\n(1nA+1nB)\n\n\n=\n1.47\n\n\n\nZ\nc\n\n=\n\n(P\u00e2\u0080\u00b2A\u00e2\u0088\u0092P\u00e2\u0080\u00b2B)\u00e2\u0088\u0092\u00ce\u00b40\n\nPc(1\u00e2\u0088\u0092Pc)\n(1nA+1nB)\n\n\n=\n1.47\n\n\n\nThe calculated test statistic is 1.47 and is not in the tail of the distribution. Make a decision: Since the calculate test statistic is not in the tail of the distribution we cannot reject H0.Conclusion: At a 1% level of significance, from the sample data, there is not sufficient evidence to conclude that there is a difference between the proportions of customers who defaulted in the two groups.\n\n\n\n\n\nTry It \n10.6\n\n\nTwo types of valves are being tested to determine if there is a difference in pressure tolerances. Fifteen out of a random sample of 100 of Valve A cracked under 4,500 psi. Six out of a random sample of 100 of Valve B cracked under 4,500 psi. Test at a 5% level of significance.\n\n"}, {"chapter": "10.5", "title": "Two Population Means with Known Standard Deviations", "mathml": "<div data-type=\"title\" id=\"2\">The <span data-type=\"term\" group-by=\"t\" id=\"term153\">test statistic</span> (<em data-effect=\"italics\">z</em>-score) is:</div><math display=\"block\"><semantics><mrow>\n<mrow><msub>\n<mi>Z</mi><mi>c</mi></msub><mo>=</mo><mfrac>\n<mrow>\n<mo stretchy=\"false\">(</mo><msub>\n<mover accent=\"true\">\n<mi>x</mi>\n<mo>\u00e2\u0080\u0093</mo>\n</mover>\n<mn>1</mn>\n</msub>\n<mo>\u00e2\u0080\u0093</mo><msub>\n<mover accent=\"true\">\n<mi>x</mi>\n<mo>\u00e2\u0080\u0093</mo>\n</mover>\n<mn>2</mn>\n</msub>\n<mo stretchy=\"false\">)</mo><mo>\u00e2\u0080\u0093</mo><msub><mi>\u00ce\u00b4</mi><mn>0</mn></msub>\n</mrow>\n<mrow>\n<msqrt>\n<mrow>\n<mfrac>\n<mrow>\n<msup>\n<mrow>\n<mo stretchy=\"false\">(</mo><msub>\n<mi>\u00cf\u0083</mi>\n<mn>1</mn>\n</msub>\n<mo stretchy=\"false\">)</mo>\n</mrow>\n<mn>2</mn>\n</msup>\n</mrow>\n<mrow>\n<msub>\n<mi>n</mi>\n<mn>1</mn>\n</msub>\n</mrow>\n</mfrac>\n<mo>+</mo><mfrac>\n<mrow>\n<msup>\n<mrow>\n<mo stretchy=\"false\">(</mo><msub>\n<mi>\u00cf\u0083</mi>\n<mn>2</mn>\n</msub>\n<mo stretchy=\"false\">)</mo>\n</mrow>\n<mn>2</mn>\n</msup>\n</mrow>\n<mrow>\n<msub>\n<mi>n</mi>\n<mn>2</mn>\n</msub>\n</mrow>\n</mfrac>\n</mrow>\n</msqrt>\n</mrow>\n</mfrac>\n</mrow>\n</mrow><annotation-xml encoding=\"MathML-Content\"><mrow><msub>\n<mi>Z</mi><mi>c</mi></msub><mo>=</mo><mfrac>\n<mrow>\n<mo stretchy=\"false\">(</mo><msub>\n<mover accent=\"true\">\n<mi>x</mi>\n<mo>\u00e2\u0080\u0093</mo>\n</mover>\n<mn>1</mn>\n</msub>\n<mo>\u00e2\u0080\u0093</mo><msub>\n<mover accent=\"true\">\n<mi>x</mi>\n<mo>\u00e2\u0080\u0093</mo>\n</mover>\n<mn>2</mn>\n</msub>\n<mo stretchy=\"false\">)</mo><mo>\u00e2\u0080\u0093</mo><msub><mi>\u00ce\u00b4</mi><mn>0</mn></msub>\n</mrow>\n<mrow>\n<msqrt>\n<mrow>\n<mfrac>\n<mrow>\n<msup>\n<mrow>\n<mo stretchy=\"false\">(</mo><msub>\n<mi>\u00cf\u0083</mi>\n<mn>1</mn>\n</msub>\n<mo stretchy=\"false\">)</mo>\n</mrow>\n<mn>2</mn>\n</msup>\n</mrow>\n<mrow>\n<msub>\n<mi>n</mi>\n<mn>1</mn>\n</msub>\n</mrow>\n</mfrac>\n<mo>+</mo><mfrac>\n<mrow>\n<msup>\n<mrow>\n<mo stretchy=\"false\">(</mo><msub>\n<mi>\u00cf\u0083</mi>\n<mn>2</mn>\n</msub>\n<mo stretchy=\"false\">)</mo>\n</mrow>\n<mn>2</mn>\n</msup>\n</mrow>\n<mrow>\n<msub>\n<mi>n</mi>\n<mn>2</mn>\n</msub>\n</mrow>\n</mfrac>\n</mrow>\n</msqrt>\n</mrow>\n</mfrac>\n</mrow></annotation-xml></semantics></math>\n</div", "content": "\nEven though this situation is not likely (knowing the population standard deviations is very unlikely), the following example illustrates hypothesis testing for independent means with known population standard deviations. The sampling distribution for the difference between the means is normal in accordance with the central limit theorem. The random variable is \n\n\n\n\nX\n1\n\n\n\u00e2\u0080\u0093\n\n\u00e2\u0080\u0093\n\n\nX\n2\n\n\n\u00e2\u0080\u0093\n\n\n\n\n\n\nX\n1\n\n\n\u00e2\u0080\u0093\n\n\u00e2\u0080\u0093\n\n\nX\n2\n\n\n\u00e2\u0080\u0093\n\n. The normal distribution has the following format:\nThe standard deviation is:\n\n\n\n\n\n\n\n\n(\n\u00cf\u0083\n1\n\n)\n\n2\n\n\n\n\nn\n1\n\n\n\n+\n\n\n\n(\n\u00cf\u0083\n2\n\n)\n\n2\n\n\n\n\nn\n2\n\n\n\n\n\n\n\n\n\n\n\n\n\n(\n\u00cf\u0083\n1\n\n)\n\n2\n\n\n\n\nn\n1\n\n\n\n+\n\n\n\n(\n\u00cf\u0083\n2\n\n)\n\n2\n\n\n\n\nn\n2\n\n\n\n\n\n\n\nThe test statistic (z-score) is:\n\nZc=\n\n(\n\nx\n\u00e2\u0080\u0093\n\n1\n\n\u00e2\u0080\u0093\n\nx\n\u00e2\u0080\u0093\n\n2\n\n)\u00e2\u0080\u0093\u00ce\u00b40\n\n\n\n\n\n\n\n\n(\n\u00cf\u0083\n1\n\n)\n\n2\n\n\n\n\nn\n1\n\n\n\n+\n\n\n\n(\n\u00cf\u0083\n2\n\n)\n\n2\n\n\n\n\nn\n2\n\n\n\n\n\n\n\n\n\nZc=\n\n(\n\nx\n\u00e2\u0080\u0093\n\n1\n\n\u00e2\u0080\u0093\n\nx\n\u00e2\u0080\u0093\n\n2\n\n)\u00e2\u0080\u0093\u00ce\u00b40\n\n\n\n\n\n\n\n\n(\n\u00cf\u0083\n1\n\n)\n\n2\n\n\n\n\nn\n1\n\n\n\n+\n\n\n\n(\n\u00cf\u0083\n2\n\n)\n\n2\n\n\n\n\nn\n2\n\n\n\n\n\n\n\n\n\nExample \n10.7\n \n\n\nIndependent groups, population standard deviations known: The\n  mean lasting time of two competing floor waxes is to be compared. Twenty floors are randomly assigned to test each wax. Both populations have a normal distributions. The data are recorded in Table 10.3.\n\n\nWax\nSample mean number of months floor wax lasts\nPopulation standard deviation\n\n\n\n1\n3\n0.33\n\n\n2\n2.9\n0.36\n\n\nTable \n10.3\n \n \n\n\n\nDoes the data indicate that wax 1 is more effective than wax 2? Test at a 5% level of\nsignificance.\n\n\n\n\nSolution\n1\n\n\nThis is a test of two independent groups, two population means, population standard deviations known.\nRandom Variable: \n\n\n\nX\n\u00e2\u0080\u0093\n\n1\n\n\u00e2\u0080\u0093\u00a0\n\nX\n\u00e2\u0080\u0093\n\n2\n\n\n\n\n\nX\n\u00e2\u0080\u0093\n\n1\n\n\u00e2\u0080\u0093\u00a0\n\nX\n\u00e2\u0080\u0093\n\n2\n\n = difference in the mean number of months the competing floor waxes last.\n\n\n\n\nH\n0\n\n:\n\n\u00ce\u00bc\n1\n\n\u00e2\u0089\u00a4\n\n\u00ce\u00bc\n2\n\n\n\n\nH\n0\n\n:\n\n\u00ce\u00bc\n1\n\n\u00e2\u0089\u00a4\n\n\u00ce\u00bc\n2\n\n\n\n\n\n\n\n\n\nH\na\n\n:\n\n\u00ce\u00bc\n1\n\n>\n\n\u00ce\u00bc\n2\n\n\n\n\nH\na\n\n:\n\n\u00ce\u00bc\n1\n\n>\n\n\u00ce\u00bc\n2\n\n\n\n\nThe words \"is more effective\" says that wax 1 lasts longer than wax 2, on average. \"Longer\" is a \u00e2\u0080\u009c>\u00e2\u0080\u009d symbol and goes into Ha. Therefore, this is a right-tailed test.\nDistribution for the test: The population standard deviations are known so the distribution is normal. Using the formula for the test statistic we find the calculated value for the problem. \n\n\n\nZc\n=\n\n(\n\u00ce\u00bc1\n-\n\u00ce\u00bc2\n)\n-\n\u00ce\u00b40\n\n\n\n\u00cf\u008312\nn1\n\n+\n\u00cf\u008322\nn2\n\n\n\n\n=\n0.1\n\n\n\nZ\nc\n\n=\n\n(\n\u00ce\u00bc1\n-\n\u00ce\u00bc2\n)\n-\n\u00ce\u00b40\n\n\n\n\u00cf\u008312\nn1\n\n+\n\u00cf\u008322\nn2\n\n\n\n\n=\n0.1\n\n\n\n\n\n\n\nFigure \n10.7\n\nThe estimated difference between he two means is : \n\n\n\nX\n\u00e2\u0080\u0093\n\n1\n\n\n\n\n\nX\n\u00e2\u0080\u0093\n\n1\n\n \u00e2\u0080\u0093 \n\n\n\nX\n\u00e2\u0080\u0093\n\n2\n\n\n\n\n\nX\n\u00e2\u0080\u0093\n\n2\n\n = 3 \u00e2\u0080\u0093 2.9 = 0.1Compare calculated value and critical value and Z\u00ce\u00b1: We mark the calculated value on the graph and find the calculated value is not in the tail therefore we cannot reject the null hypothesis.Make a decision:  the calculated value of the test statistic is not in the tail, therefore you cannot reject H0.Conclusion: At the 5% level of significance, from the sample data, there is not\n sufficient evidence to conclude that the mean time wax 1 lasts is longer (wax 1 is more effective) than the mean time wax 2 lasts.\n\n\n\n\n\nTry It \n10.7\n\n\n\n\nThe means of the number of revolutions per minute of two competing engines are to be compared. Thirty engines are randomly assigned to be tested. Both populations have normal distributions. Table 10.4 shows the result. Do the data indicate that Engine 2 has higher RPM than Engine 1? Test at a 5% level of significance.EngineSample mean number of RPMPopulation standard deviation\n11,50050\n21,60060\n\nTable \n10.4\n \n \n\n\n\n\nExample \n10.8\n \n\nAn interested citizen wanted to know if Democratic U. S. senators are older than\n  Republican U.S. senators, on average. On May 26 2013, the mean age of 30 randomly selected Republican Senators was 61 years 247 days old (61.675 years) with a standard deviation of 10.17 years. The mean age of 30 randomly selected Democratic senators was 61 years 257 days old (61.704 years) with a standard deviation of 9.55 years.\n\n\nDo the data indicate that Democratic senators are older than Republican senators, on average? Test at a 5% level of significance.\n\n\n\n\nSolution\n1\n\n\nThis is a test of two independent groups, two population means. The population standard deviations are unknown, but the sum of the sample sizes is 30 + 30 = 60, which is greater than 30, so we can use the normal approximation to the Student\u00e2\u0080\u0099s-t distribution.  Subscripts: 1: Democratic senators 2: Republican senators\nRandom variable: \n\n\n\nX\n\u00e2\u0080\u0093\n\n1\n\n\u00a0\u00e2\u0080\u0093\u00a0\n\nX\n\u00e2\u0080\u0093\n\n2\n\n\n\n\n\nX\n\u00e2\u0080\u0093\n\n1\n\n\u00a0\u00e2\u0080\u0093\u00a0\n\nX\n\u00e2\u0080\u0093\n\n2\n\n = difference in the mean age of Democratic and Republican U.S. senators.\nH0:\u00ce\u00bc1\u00e2\u0089\u00a4\u00ce\u00bc2\nH0:\u00ce\u00bc1\u00e2\u0089\u00a4\u00ce\u00bc2 H0:\u00ce\u00bc1-\u00ce\u00bc2\u00e2\u0089\u00a40\nH0:\u00ce\u00bc1-\u00ce\u00bc2\u00e2\u0089\u00a40Ha:\u00ce\u00bc1>\u00ce\u00bc2\nHa:\u00ce\u00bc1>\u00ce\u00bc2 Ha:\u00ce\u00bc1-\u00ce\u00bc2>0\nHa:\u00ce\u00bc1-\u00ce\u00bc2>0The words \"older than\" translates as a \u00e2\u0080\u009c>\u00e2\u0080\u009d symbol and goes into Ha. Therefore, this is a right-tailed test.\n\n\n\n\nFigure \n10.8\n\nMake a decision:   The p-value is larger than 5%, therefore we cannot reject the null hypothesis. By calculating the test statistic we would find that the test statistic does not fall in the tail, therefore we cannot reject the null hypothesis. We reach the same conclusion using either method of a making this statistical decision.Conclusion: At the 5% level of significance, from the sample data, there is not sufficient evidence to conclude that the mean age of Democratic senators is greater than the mean age of the Republican senators.\n\n\n\n\n\n"}, {"chapter": "10.6", "title": "Matched or Paired Samples", "mathml": "<div data-type=\"title\" id=\"1\">The null and alternative hypotheses for this test are:</div><math display=\"block\"><semantics><mrow><msub><mi>H</mi><mn>0</mn></msub>\n<mo>:</mo>\n<msub><mi>\u00c2\u00b5</mi><mi>d</mi></msub><mo>=</mo><mn>0</mn>\n</mrow><annotation-xml encoding=\"MathML-Content\"><msub><mi>H</mi><mn>0</mn></msub><mo>:</mo><msub><mi>\u00c2\u00b5</mi><mi>d</mi></msub><mo>=</mo><mn>0</mn></annotation-xml></semantics></math></div", "content": "\nIn most cases of economic or business data we have little or no control over the process of how the data are gathered. In this sense the data are not the result of a planned controlled experiment. In some cases, however, we can develop data that are part of a controlled experiment. This situation occurs frequently in quality control situations. Imagine that the production rates of two machines built to the same design, but at different manufacturing plants, are being tested for differences in some production metric such as speed of output or meeting some production specification such as strength of the product. The test is the same in format to what we have been testing, but here we can have matched pairs for which we can test if differences exist. Each observation has its matched pair against which differences are calculated. First, the differences in the metric to be tested between the two lists of observations must be calculated, and this is typically labeled with the letter \"d.\" Then, the average of these matched differences, X\u00c2\u00afdX\u00c2\u00afd is calculated as is its standard deviation, Sd. We expect that the standard deviation of the differences of the matched pairs will be smaller than unmatched pairs because presumably fewer differences should exist because of the correlation between the two groups.When using a hypothesis test for matched or paired samples, the following characteristics may be present:Simple random sampling is used.\nSample sizes are often small.\nTwo measurements (samples) are drawn from the same pair of individuals or objects.\nDifferences are calculated from the matched or paired samples.\nThe differences form the sample that is used for the hypothesis test.\nEither the matched pairs have differences that come from a population that is normal or the number of differences is sufficiently large so that distribution of the sample mean of differences is approximately normal.\nIn a hypothesis test for matched or paired samples, subjects are matched in pairs and differences are calculated. The differences are the data. The population mean for the differences, \u00ce\u00bcd, is then tested using a Student's-t test for a single population mean with n \u00e2\u0080\u0093 1 degrees of freedom, where n is the number of differences, that is, the number of pairs not the number of observations.The null and alternative hypotheses for this test are:H0\n:\n\u00c2\u00b5d=0\nH0:\u00c2\u00b5d=0\nHa\n:\n\u00c2\u00b5d\u00e2\u0089\u00a00Ha:\u00c2\u00b5d\u00e2\u0089\u00a00The test statistic is:\n\ntc=\n\n\n\nx\n\u00e2\u0080\u0093\n\nd\n\n\u00e2\u0088\u0092\n\u00ce\u00bc\nd\n\n\n\n(\n\n\n\n\ns\nd\n\n\n\n\nn\n\n\n\n\n)\n\n\n\n\ntc=\n\n\n\nx\n\u00e2\u0080\u0093\n\nd\n\n\u00e2\u0088\u0092\n\u00ce\u00bc\nd\n\n\n\n(\n\n\n\n\ns\nd\n\n\n\n\nn\n\n\n\n\n)\n\n\n\nExample \n10.9\n \n\n\n\nA company has developed a training program for its entering employees because they have become concerned with the results of the six-month employee review. They hope that the training program can result in better six-month reviews. Each trainee constitutes a \u00e2\u0080\u009cpair\u00e2\u0080\u009d, the entering score the employee received when first entering the firm and the score given at the six-month review. The difference in the two scores were calculated for each employee and the means for before and after the training program was calculated. The sample mean before the training program was 20.4 and the sample mean after the training program was 23.9. The standard deviation of the differences in the two scores across the 20 employees was 3.8 points. Test at the 10% significance level the null hypothesis that the two population means are equal against the alternative that the training program helps improve the employees\u00e2\u0080\u0099 scores.   \n\n\n\nSolution\n1\n\n\nThe first step is to identify this as a two sample case: before the training and after the training. This differentiates this problem from simple one sample issues. Second, we determine that the two samples are \"paired.\" Each observation in the first sample has a paired observation in the second sample. This information tells us that the null and alternative hypotheses should be:\n\n\nH0\n:\n\u00c2\u00b5d\n\u00e2\u0089\u00a4\n0\nH0:\u00c2\u00b5d\u00e2\u0089\u00a40\n\n\nHa\n:\n\u00c2\u00b5d\n>\n0\nHa:\u00c2\u00b5d>0\n\nThis form reflects the implied claim that the training course improves scores; the test is one-tailed and the claim is in the alternative hypothesis. \nBecause the experiment was conducted as a matched paired sample rather than simply taking scores from people who took the training course those who didn't, we use the matched pair test statistic:\nTest Statistic: \ntc\n=\nX\u00c2\u00afd\u00e2\u0088\u0092\u00c2\u00b5d\nSdn\n=\n(23.9\u00e2\u0088\u009220.4)\u00e2\u0088\u00920\n(3.820)\n=\n4.12\nTest Statistic: tc=X\u00c2\u00afd\u00e2\u0088\u0092\u00c2\u00b5d\nSdn=(23.9\u00e2\u0088\u009220.4)\u00e2\u0088\u00920\n(3.820)=4.12\n\nIn order to solve this equation, the individual scores, pre-training course and post-training course need to be used to calculate the individual differences. These scores are then averaged and the average difference is calculated:\n\nX\u00c2\u00afd=x\u00c2\u00af1\u00e2\u0088\u0092x\u00c2\u00af2X\u00c2\u00afd=x\u00c2\u00af1\u00e2\u0088\u0092x\u00c2\u00af2\n\nFrom these differences we can calculate the standard deviation across the individual differences:\nSd=\n\u00ce\u00a3(di\u00e2\u0088\u0092X\u00c2\u00afd)2\nn\u00e2\u0088\u00921\n where \ndi=x1i\u00e2\u0088\u0092x2iSd=\u00ce\u00a3(di\u00e2\u0088\u0092X\u00c2\u00afd)2\nn\u00e2\u0088\u00921 where di=x1i\u00e2\u0088\u0092x2i\n\nWe can now compare the calculated value of the test statistic, 4.12, with the critical value. The critical value is a Student's t with degrees of freedom equal to the number of pairs, not observations, minus 1. In this case 20 pairs and at 90% confidence level ta/2 = \u00c2\u00b11.729 at df = 20 - 1 = 19. The calculated test statistic is most certainly in the tail of the distribution and thus we cannot accept the null hypothesis that there is no difference from the training program. Evidence seems indicate that the training aids employees in gaining higher scores.\n\n\n\n\nExample \n10.10\n \n\n\n A study was conducted to investigate the effectiveness of hypnotism in reducing pain. Results for randomly selected subjects are shown in Table 10.5. A lower score indicates less pain. The \"before\" value is matched to an \"after\" value and the differences are calculated. Are the sensory measurements, on average, lower after hypnotism? Test at a 5% significance level.\n\nSubject:\nA\nB\nC\nD\nE\nF\nG\nH\n\n\n\nBefore\n6.6\n6.5\n9.0\n10.3\n11.3\n8.1\n6.3\n11.6\n\n\nAfter\n6.8\n2.4\n7.4\n8.5\n8.1\n6.1\n3.4\n2.0\n\n\nTable \n10.5\n \n \n\n\n\n\n\n\nSolution\n1\n\n\nCorresponding \"before\" and \"after\" values form matched pairs. (Calculate \"after\" \u00e2\u0080\u0093 \"before.\")\n\n\n\nAfter data\nBefore data\nDifference\n\n\n\n\n6.8\n6.6\n0.2\n\n\n2.4\n6.5\n-4.1\n\n\n7.4\n9\n-1.6\n\n\n8.5\n10.3\n-1.8\n\n\n8.1\n11.3\n-3.2\n\n\n6.1\n8.1\n-2\n\n\n3.4\n6.3\n-2.9\n\n\n2\n11.6\n-9.6\n\n\n\nTable \n10.6\n \n \n\nThe data for the test are the differences: {0.2, \u00e2\u0080\u00934.1, \u00e2\u0080\u00931.6, \u00e2\u0080\u00931.8, \u00e2\u0080\u00933.2, \u00e2\u0080\u00932, \u00e2\u0080\u00932.9, \u00e2\u0080\u00939.6}The sample mean and sample standard deviation of the differences are: \n\n\n\nx\n\u00e2\u0080\u0093\n\nd\n\n=\n\u00e2\u0080\u00933.13\n\n\nx\n\u00e2\u0080\u0093\n\nd\n=\u00e2\u0080\u00933.13\nand\n\n\ns\nd\n\n=\n2.91\n\ns\nd\n=2.91\nVerify these values.Let \n\n\u00ce\u00bc\nd\n\n\n\u00ce\u00bc\nd\n be the population mean for the differences. We use the subscript dd to denote \"differences.\"\nRandom variable:\n\n\n\n\nX\n\u00e2\u0080\u0093\n\nd\n\n\n\n\n\nX\n\u00e2\u0080\u0093\n\nd\n\n = the mean difference of the sensory measurements\nH0: \u00ce\u00bcd \u00e2\u0089\u00a5 0\nThe null hypothesis is zero or positive, meaning that there is the same or more pain felt after hypnotism. That means the subject shows no improvement. \u00ce\u00bcd is the population mean of the differences.)\nHa: \u00ce\u00bcd < 0\nThe alternative hypothesis is negative, meaning there is less pain felt after hypnotism. That means the subject shows improvement. The score should be lower after hypnotism, so the difference ought to be negative to indicate improvement.\nDistribution for the test: The distribution is a Student's t with df = n \u00e2\u0080\u0093 1 = 8 \u00e2\u0080\u0093 1 = 7. Use t7. (Notice that the test is for a single population mean.)\nCalculate the test statistic and look up the critical value using the Student's-t distribution: The calculated value of the test statistic is 3.06 and the critical value of the t-distribution with 7 degrees of freedom at the 5% level of confidence is  1.895 with a one-tailed test.\n\n\n\n\nFigure \n10.9\n\n\n\n\n\nX\n\u00e2\u0080\u0093\n\nd\n\n\n\n\n\nX\n\u00e2\u0080\u0093\n\nd\n\n is the random variable for the differences.The sample mean and sample standard deviation of the differences are:\n\n\n\n\nx\n\u00e2\u0080\u0093\n\nd\n\n\n\n\n\nx\n\u00e2\u0080\u0093\n\nd\n\n = \u00e2\u0080\u00933.13\n\n\n\ns\n\u00e2\u0080\u0093\n\nd\n\n\n\n\n\ns\n\u00e2\u0080\u0093\n\nd\n\n = 2.91\nCompare the critical value for alpha against the calculated test statistic.\nThe conclusion from using the comparison of the calculated test statistic and the critical value will gives us the result. In this question the calculated test statistic is 3.06 and the critical value is 1.895. The test statistic is clearly in the tail and thus we cannot accept the null hypotheses that there is no difference between the two situations, hypnotized and not hypnotized.Make a decision: Cannot accept the null hypothesis, H0. This means that \u00ce\u00bcd < 0 and there is a statistically significant improvement.Conclusion: At a 5% level of significance, from the sample data, there is sufficient evidence to conclude that the sensory measurements, on average, are lower after hypnotism. Hypnotism appears to be effective in reducing pain.\n\n\n\n\nExample \n10.11\n \n\n\nA college football coach was interested in whether the college's strength development class increased his players' maximum lift (in pounds) on the bench press exercise. He asked four of his players to participate in a study. The amount of weight they could each lift was recorded before they took the strength development class. After completing the class, the amount of weight they could each lift was again measured. The data are as follows:\n\n\nWeight (in pounds)\nPlayer 1\nPlayer 2\nPlayer 3\nPlayer 4\n\n\n\nAmount of weight lifted prior to the class\n205\n241\n338\n368\n\n\nAmount of weight lifted after the class\n295\n252\n330\n360\n\n\nTable \n10.7\n \n \n\nThe coach wants to know if the strength development class makes his players stronger, on average.\nRecord the differences data. Calculate the differences by subtracting the amount of weight lifted prior to the class from the weight lifted after completing the class. The data for the differences are: {90, 11, -8, -8}. \n\n\n\nx\n\u00e2\u0080\u0093\n\nd\n\n\n\n\n\nx\n\u00e2\u0080\u0093\n\nd\n\n = 21.3, sd = 46.7Using the difference data, this becomes a test of a single mean.Define the random variable: \n\n\n\nX\n\u00e2\u0080\u0093\n\nd\n\n\n\n\n\nX\n\u00e2\u0080\u0093\n\nd\n\n mean difference in the maximum lift per player.The distribution for the hypothesis test is a student's t with 3 degrees of freedom.H0: \u00ce\u00bcd \u00e2\u0089\u00a4 0, Ha: \u00ce\u00bcd > 0\n\n\n\nFigure \n10.10\n\nCalculate the test statistic look up the critical value: Critical value of the test statistic is 0.91. The critical value of the student's t at 5% level of significance and 3 degrees of freedom is 2.353.Decision: If the level of significance is 5%, we cannot reject the null hypothesis, because the calculated value of the test statistic is not in the tail.What is the conclusion?\nAt a 5% level of significance, from the sample data, there is not sufficient evidence to conclude that the strength development class helped to make the players stronger, on average.\n\n"}, {"chapter": "11.1", "title": "Facts About the Chi-Square Distribution", "mathml": "<math display=\"block\"><semantics><mrow>\n<mrow>\n<mi>\u00cf\u0087</mi><mo>\u00e2\u0088\u00bc</mo><msubsup>\n<mi>\u00cf\u0087</mi>\n<mrow>\n<mi>d</mi><mi>f</mi>\n</mrow>\n<mn>2</mn>\n</msubsup>\n</mrow>\n</mrow><annotation-xml encoding=\"MathML-Content\"><mrow>\n<mi>\u00cf\u0087</mi><mo>\u00e2\u0088\u00bc</mo><msubsup>\n<mi>\u00cf\u0087</mi>\n<mrow>\n<mi>d</mi><mi>f</mi>\n</mrow>\n<mn>2</mn>\n</msubsup>\n</mrow></annotation-xml></semantics></math></div", "content": "\nThe notation for the chi-square distribution is:\n\n\u00cf\u0087\u00e2\u0088\u00bc\n\u00cf\u0087\n\ndf\n\n2\n\n\n\n\u00cf\u0087\u00e2\u0088\u00bc\n\u00cf\u0087\n\ndf\n\n2\n\nwhere df = degrees of freedom which depends on how chi-square is being used. (If you want to practice calculating chi-square probabilities then use df = n - 1. The degrees of freedom for the three major uses are each calculated differently.)For the \u00cf\u00872 distribution, the population mean is \u00ce\u00bc = df and the population standard deviation is \n\n\u00cf\u0083=\n\n2(df)\n\n\n\n\n\u00cf\u0083=\n\n2(df)\n\n\n.The random variable is shown as \u00cf\u00872.The random variable for a chi-square distribution with k degrees of freedom is the sum of k independent, squared standard normal variables.\n\u00cf\u00872 = (Z1)2 + (Z2)2 + ... + (Zk)2The curve is nonsymmetrical and skewed to the right.\nThere is a different chi-square curve for each df.\n\n\n\nFigure \n11.2\n\n\nThe test statistic for any test is always greater than or equal to zero.\nWhen df > 90, the chi-square curve approximates the normal distribution. For X ~ \n\n\n\u00cf\u0087\n\n1,000\n\n2\n\n\n\n\n\u00cf\u0087\n\n1,000\n\n2\n\n the mean, \u00ce\u00bc = df = 1,000 and the standard deviation, \u00cf\u0083 = \n\n\n\n2(1,000)\n\n\n\n\n\n\n2(1,000)\n\n\n = 44.7. Therefore, X ~ N(1,000, 44.7), approximately.\nThe mean, \u00ce\u00bc, is located just to the right of the peak.\n\n\n"}, {"chapter": "11.2", "title": "Test of a Single Variance", "mathml": "<math display=\"block\"><semantics><mrow>\n<msubsup><mi>\u00cf\u0087</mi><mi>c</mi><mn>2</mn></msubsup>\n<mo>=</mo>\n<mfrac>\n<mrow>\n<mo>(</mo><mi>n</mi><mo>-</mo><mn>1</mn><mo>)</mo><mspace></mspace>\n<msup><mi>s</mi><mn>2</mn></msup>\n</mrow>\n<mrow>\n<msubsup><mi>\u00cf\u0083</mi><mn>0</mn><mn>2</mn></msubsup>\n</mrow></mfrac></mrow><annotation-xml encoding=\"MathML-Content\"><msubsup><mi>\u00cf\u0087</mi><mi>c</mi><mn>2</mn></msubsup><mo>=</mo><mfrac>\n<mrow>\n<mo>(</mo><mi>n</mi><mo>-</mo><mn>1</mn><mo>)</mo><mspace></mspace>\n<msup><mi>s</mi><mn>2</mn></msup>\n</mrow>\n<mrow>\n<msubsup><mi>\u00cf\u0083</mi><mn>0</mn><mn>2</mn></msubsup>\n</mrow></mfrac></annotation-xml></semantics></math></div", "content": "\nThus far our interest has been exclusively on the population parameter \u00ce\u00bc or it's counterpart in the binomial, p. Surely the mean of a population is the most critical piece of information to have, but in some cases we are interested in the variability of the outcomes of some distribution. In almost all production processes quality is measured not only by how closely the machine matches the target, but also the variability of the process. If one were filling bags with potato chips not only would there be interest in the average weight of the bag, but also how much variation there was in the weights. No one wants to be assured that the average weight is accurate when their bag has no chips. Electricity voltage may meet some average level, but great variability, spikes, can cause serious damage to electrical machines, especially computers. I would not only like to have a high mean grade in my classes, but also low variation about this mean. In short, statistical tests concerning the variance of a distribution have great value and many applications.A test of a single variance assumes that the underlying distribution is normal. The null and alternative hypotheses are stated in terms of the population variance. The test statistic is:\n\u00cf\u0087c2\n=\n\n\n(n-1)\ns2\n\n\n\u00cf\u008302\n\u00cf\u0087c2=\n\n(n-1)\ns2\n\n\n\u00cf\u008302\nwhere:\n n = the total number of observations in the sample data \ns2 = sample variance \n\u00cf\u008302\u00cf\u008302 = hypothesized value of the population variance\nH0:\u00cf\u00832=\u00cf\u008302H0:\u00cf\u00832=\u00cf\u008302\nHa:\u00cf\u00832\u00e2\u0089\u00a0\u00cf\u008302Ha:\u00cf\u00832\u00e2\u0089\u00a0\u00cf\u008302 You may think of s as the random variable in this test. The number of degrees of freedom is df = n - 1. A test of a single variance may be right-tailed, left-tailed, or two-tailed. Example 11.1 will show you how to set up the null and alternative hypotheses. The null and alternative hypotheses contain statements about the population variance.\nExample \n11.1\n \n\n\n\n\nMath instructors are not only interested in how their students do on exams, on average, but how the exam scores vary. To many instructors, the variance (or standard deviation) may be more important than the average.\nSuppose a math instructor believes that the standard deviation for his final exam is five points. One of his\nbest students thinks otherwise. The student claims that the standard deviation is more than five points. If the student were to conduct a hypothesis test, what would the null and alternative hypotheses be?\n\n\n\nSolution\n1\n\n\nEven though we are given the population standard deviation, we can set up the test using the population variance as follows.\nH0: \u00cf\u00832 \u00e2\u0089\u00a4 52\n\nHa: \u00cf\u00832 > 52\n\n\n\n\n\n\nTry It \n11.1\n\n\n\n\nA SCUBA instructor wants to record the collective depths each of his students' dives during their checkout. He is interested in how the depths vary, even though everyone should have been at the same depth. He believes the standard deviation is three feet. His assistant thinks the standard deviation is less than three feet. If the instructor were to conduct a test, what would the null and alternative hypotheses be?\n\n\nExample \n11.2\n \n\nWith individual lines at its various windows, a post office finds that the standard deviation for waiting times for customers on Friday afternoon is 7.2 minutes. The post office experiments with a single, main waiting line and finds that for a random sample of 25 customers, the waiting times for customers have a standard deviation of 3.5 minutes on a Friday afternoon.\nWith a significance level of 5%, test the claim that a single line causes lower variation\namong waiting times for customers.\n  \n\n\n\nSolution\n1\n\n\nSince the claim is that a single line causes less variation, this is a test of a single variance.\nThe parameter is the population variance, \u00cf\u00832.\n  Random Variable: The sample standard deviation, s, is the random variable.\nLet s = standard deviation for the waiting times.H0: \u00cf\u00832 \u00e2\u0089\u00a5 7.22\nHa: \u00cf\u00832 < 7.22\nThe word \"less\" tells you this is a left-tailed test.Distribution for the test: \u00cf\u0087242\u00cf\u0087242, where:\nn = the number of customers sampled\ndf = n \u00e2\u0080\u0093 1 = 25 \u00e2\u0080\u0093 1 = 24\nCalculate the test statistic:\n\n\n\n\n\n\n\u00cf\u0087\nc\n2\n\n=\n\n(n\u00a0\u00e2\u0088\u0092\u00a01)\ns\n2\n\n\n\n\n\u00cf\u0083\n2\n\n\n\n=\n\n(25\u00a0\u00e2\u0088\u0092\u00a01)\n\n(3.5)\n\n2\n\n\n\n\n\n7.2\n\n2\n\n\n\n=5.67\n\n\n\n\n\n\u00cf\u0087\nc\n2\n\n=\n\n(n\u00a0\u00e2\u0088\u0092\u00a01)\ns\n2\n\n\n\n\n\u00cf\u0083\n2\n\n\n\n=\n\n(25\u00a0\u00e2\u0088\u0092\u00a01)\n\n(3.5)\n\n2\n\n\n\n\n\n7.2\n\n2\n\n\n\n=5.67\n\n\n\n\nwhere n = 25, s = 3.5, and \u00cf\u0083 = 7.2.\n\n\n\n\n\nFigure \n11.3\n\nThe graph of the Chi-square shows the distribution and marks the critical value with 24 degrees of freedom at 95% level of confidence, \u00ce\u00b1 = 0.05, 13.85. The critical value of 13.85 came from the Chi squared table which is read very much like the students t table. The difference is that the students t-distribution is symmetrical and the Chi squared distribution is not. At the top of the Chi squared table we see not only the familiar 0.05, 0.10, etc. but also 0.95, 0.975, etc. These are the columns used to find the left hand critical value. The graph also marks the calculated \u00cf\u00872 test statistic of 5.67. Comparing the test statistic with the critical value, as we have done with all other hypothesis tests, we reach the conclusion.Make a decision: Because the calculated test statistic is in the tail we cannot accept H0. This means that you reject \u00cf\u00832 \u00e2\u0089\u00a5 7.22. In other words, you do not think the variation in waiting times is 7.2 minutes or more; you think the variation in waiting times is less.Conclusion: At a 5% level of significance, from the data, there is sufficient evidence to\nconclude that a single line causes a lower variation among the waiting times or with a single\nline, the customer waiting times vary less than 7.2 minutes.\n\n\n\n\n\n\nExample \n11.3\n \n\nProfessor Hadley has a weakness for cream filled donuts, but he believes that some\n\nbakeries are not properly filling the donuts. A sample of 24 donuts reveals a mean\n\namount of filling equal to 0.04 cups, and the sample standard deviation is 0.11 cups.\n\nProfessor Hadley has an interest in the average quantity of filling, of course, but he is\n\nparticularly distressed if one donut is radically different from another. Professor Hadley\n\ndoes not like surprises.\n\nTest at 95% the null hypothesis that the population variance of donut filling is significantly different\n\nfrom the average amount of filling.\n  \n\n\n\nSolution\n1\n\n\nThis is clearly a problem dealing with variances. In this case we are testing a single sample rather than comparing two samples from different populations. The null and alternative hypotheses are thus:\nH0:\n\u00cf\u00832=0.04H0:\u00cf\u00832=0.04\nH0:\n\u00cf\u00832\u00e2\u0089\u00a00.04H0:\u00cf\u00832\u00e2\u0089\u00a00.04\n\nThe test is set up as a two-tailed test because Professor Hadley has shown concern with too much variation in filling as well as too little: his dislike of a surprise is any level of filling outside the expected average of 0.04 cups. The test statistic is calculated to be:\n\n\u00cf\u0087c2\n=\n(n\u00e2\u0088\u00921)s2\n\u00cf\u0083o2\n\n=\n(24\u00e2\u0088\u00921)0.112\n0.042\n\n=\n6.9575\n\u00cf\u0087c2=(n\u00e2\u0088\u00921)s2\n\u00cf\u0083o2\n=(24\u00e2\u0088\u00921)0.112\n0.042\n=6.9575\n\nThe calculated \u00cf\u00872\u00cf\u00872 test statistic, 6.96, is in the tail therefore at a 0.05 level of significance, we cannot accept the null hypothesis that the variance in the donut filling is equal to 0.04 cups. It seems that Professor Hadley is destined to meet disappointment with each bit.\n\n\n\n\n\n\n\nFigure \n11.4\n\n\nTry It \n11.3\n\n\n\n\nThe FCC conducts broadband speed tests to measure how much data per second passes between a consumer\u00e2\u0080\u0099s computer and the internet. As of August of 2012, the standard deviation of Internet speeds across Internet Service Providers (ISPs) was 12.2 percent. Suppose a sample of 15 ISPs is taken, and the standard deviation is 13.2. An analyst claims that the standard deviation of speeds is more than what was reported. State the null and alternative hypotheses, compute the degrees of freedom, the test statistic, sketch the graph of the distribution and mark the area associated with the level of confidence, and draw a conclusion. Test at the 1% significance level.\n\n\n"}, {"chapter": "11.3", "title": "Goodness-of-Fit Test", "mathml": "<math display=\"block\"><semantics><mrow>\n<mrow>\n<munder>\n<mi fontsize=\"2\">\u00ce\u00a3</mi>\n<mi>k</mi>\n</munder>\n<mfrac>\n<mrow>\n<msup>\n<mrow>\n<mo stretchy=\"false\">(</mo><mi>O</mi><mo>\u00e2\u0088\u0092</mo><mi>E</mi><mo stretchy=\"false\">)</mo>\n</mrow>\n<mn>2</mn>\n</msup>\n</mrow>\n<mi>E</mi>\n</mfrac>\n</mrow>\n</mrow><annotation-xml encoding=\"MathML-Content\"><mrow>\n<munder>\n<mi fontsize=\"2\">\u00ce\u00a3</mi>\n<mi>k</mi>\n</munder>\n<mfrac>\n<mrow>\n<msup>\n<mrow>\n<mo stretchy=\"false\">(</mo><mi>O</mi><mo>\u00e2\u0088\u0092</mo><mi>E</mi><mo stretchy=\"false\">)</mo>\n</mrow>\n<mn>2</mn>\n</msup>\n</mrow>\n<mi>E</mi>\n</mfrac>\n</mrow></annotation-xml></semantics></math></div", "content": "\nIn this type of hypothesis test, you determine whether the data \"fit\" a particular distribution or not. For example, you may suspect your unknown data fit a binomial distribution. You use a chi-square test (meaning the distribution for the hypothesis test is chi-square) to determine if there is a fit or not. The null and the alternative hypotheses for this test may be written in sentences or may be stated as equations or inequalities.The test statistic for a goodness-of-fit test is:\n\n\n\n\u00ce\u00a3\nk\n\n\n\n\n\n(O\u00e2\u0088\u0092E)\n\n2\n\n\nE\n\n\n\n\n\u00ce\u00a3\nk\n\n\n\n\n\n(O\u00e2\u0088\u0092E)\n\n2\n\n\nE\n\nwhere:\nO = observed values (data)\nE = expected values (from theory)\nk = the number of different data cells or categories\nThe observed values are the data values and the expected values are the values you would expect to get if the null hypothesis were true. There are n terms of the form \n\n\n\n\n\n(O\u00e2\u0088\u0092E)\n\n2\n\n\nE\n\n\n\n\n\n\n\n(O\u00e2\u0088\u0092E)\n\n2\n\n\nE\n\n.\nThe number of degrees of freedom is df = (number of categories \u00e2\u0080\u0093 1).The goodness-of-fit test is almost always right-tailed. If the observed values and the corresponding expected values are not close to each other, then the test statistic can get very large and will be way out in the right tail of the chi-square curve.\nNote\n\n\nThe number of expected values inside each cell needs to be at least five in order to use this test.\nExample \n11.4\n \n\nAbsenteeism of college students from math classes is a major concern to math instructors because missing class appears to increase the drop rate. Suppose that a study was done to determine if the actual student absenteeism rate follows faculty perception. The faculty expected that a group of 100 students would miss class according to Table 11.1.\n\nNumber of absences per term\nExpected number of students\n\n\n\n0\u00e2\u0080\u00932\n50\n\n\n3\u00e2\u0080\u00935\n30\n\n\n6\u00e2\u0080\u00938\n12\n\n\n9\u00e2\u0080\u009311\n6\n\n\n12+\n2\n\n\nTable \n11.1\n \n \n\nA random survey across all mathematics courses was then done to determine the actual number (observed) of absences in a course. The chart in Table 11.2 displays the results of that survey.\n\n\nNumber of absences per term\nActual number of students\n\n\n\n0\u00e2\u0080\u00932\n35\n\n\n3\u00e2\u0080\u00935\n40\n\n\n6\u00e2\u0080\u00938\n20\n\n\n9\u00e2\u0080\u009311\n1\n\n\n12+\n4\n\n\nTable \n11.2\n \n \n\n\nDetermine the null and alternative hypotheses needed to conduct a goodness-of-fit test.H0: Student absenteeism fits faculty perception.The alternative hypothesis is the opposite of the null hypothesis.Ha: Student absenteeism does not fit faculty perception.\n\na. Can you use the information as it appears in the charts to conduct the goodness-of-fit test? \n\n\n\nSolution\n1\n\n\na. No. Notice that the expected number of absences for the \"12+\" entry is less than five (it is two). Combine that group with the \"9\u00e2\u0080\u009311\" group to create new tables where the number of students for each entry are at least five. The new results are in Table 11.3 and Table 11.4.\n\n\n\n\nNumber of absences per term\nExpected number of students\n\n\n\n\n0\u00e2\u0080\u00932\n50\n\n\n3\u00e2\u0080\u00935\n30\n\n\n6\u00e2\u0080\u00938\n12\n\n\n9+\n8\n\n\n\nTable \n11.3\n \n \n\n\n\n\nNumber of absences per term\nActual number of students\n\n\n\n\n0\u00e2\u0080\u00932\n35\n\n\n3\u00e2\u0080\u00935\n40\n\n\n6\u00e2\u0080\u00938\n20\n\n\n9+\n5\n\n\n\nTable \n11.4\n \n \n\n\n\n\n\n\n\nb. What is the number of degrees of freedom (df)?\n\n\n\nSolution\n2\n\n\nb. There are four \"cells\" or categories in each of the new tables.df = number of cells \u00e2\u0080\u0093 1 = 4 \u00e2\u0080\u0093 1 = 3\n\n\n\nTry It \n11.4\n\n\n\n\nA factory manager needs to understand how many products are defective versus how many are produced. The number of expected defects is listed in Table 11.5.\nNumber produced Number defective\n0\u00e2\u0080\u0093100 5\n101\u00e2\u0080\u0093200 6\n201\u00e2\u0080\u0093300 7\n301\u00e2\u0080\u0093400 8\n401\u00e2\u0080\u0093500 10\n\nTable \n11.5\n \n \n\n\nA random sample was taken to determine the actual number of defects. Table 11.6 shows the results of the survey.\n\n\nNumber produced\nNumber defective\n\n\n\n0\u00e2\u0080\u0093100\n5\n\n\n101\u00e2\u0080\u0093200\n7\n\n\n201\u00e2\u0080\u0093300\n8\n\n\n301\u00e2\u0080\u0093400\n9\n\n\n401\u00e2\u0080\u0093500\n11\n\n\nTable \n11.6\n \n \n\n\nState the null and alternative hypotheses needed to conduct a goodness-of-fit test, and state the degrees of freedom.\n\n\nExample \n11.5\n \n\n\nEmployers want to know which days of the week employees are absent in a five-day work week. Most employers would like to believe that employees are absent equally during the week. Suppose a random sample of 60 managers were asked on which day of the week they had the highest number of employee absences. The results were distributed as in Table 11.7. For the population of employees, do the days for the highest number of absences occur with equal frequencies during a five-day work week? Test at a 5% significance level.\n\n\n\nMonday\nTuesday\nWednesday\nThursday\nFriday\n\n\n\nNumber of absences\n15\n12\n9\n9\n15\n\n\nTable \n11.7\n \nDay of the Week Employees were Most Absent\n \n\n\n\n\n\nSolution\n1\n\n\nThe null and alternative hypotheses are:H0: The absent days occur with equal frequencies, that is, they fit a uniform distribution.\nHa: The absent days occur with unequal frequencies, that is, they do not fit a uniform distribution.\nIf the absent days occur with equal frequencies, then, out of 60 absent days (the total in the sample: 15 + 12 + 9 + 9 + 15 = 60), there would be 12 absences on Monday, 12 on Tuesday, 12 on Wednesday, 12 on Thursday, and 12 on Friday. These numbers are the expected (E) values. The values in the table are the observed (O) values or data.This time, calculate the \u00cf\u00872 test statistic by hand. Make a chart with the following headings  and fill in the columns:Expected (E) values (12, 12, 12, 12, 12)\nObserved (O) values (15, 12, 9, 9, 15)\n(O \u00e2\u0080\u0093 E)\n(O \u00e2\u0080\u0093 E)2\n\n\n\n\n\n\n\n(O\u00a0\u00e2\u0080\u0093\u00a0E)\n\n2\n\n\nE\n\n\n\n\n\n\n\n(O\u00a0\u00e2\u0080\u0093\u00a0E)\n\n2\n\n\nE\n\n\nNow add (sum) the last column. The sum is three. This is the \u00cf\u00872 test statistic.\nThe calculated test statistics is 3 and the critical value of the \u00cf\u00872 distribution at 4 degrees of freedom the 0.05 level of confidence is 9.48. This value is found in the \u00cf\u00872 table at the 0.05 column  on the degrees of freedom row 4.The degrees of freedom are the number of cells \u00e2\u0080\u0093 1 = 5 \u00e2\u0080\u0093 1 = 4Next, complete a graph like the following one with the proper labeling and shading. (You should shade the right tail.)\n\n\n\n\n\nFigure \n11.5\n\n\n\n\n\n\u00cf\u0087c2\n=\n\n\n\u00ce\u00a3\nk\n\n\n\n\n\n(O\u00e2\u0088\u0092E)\n\n2\n\n\nE\n\n\n=\n3\n\n\n\n\u00cf\u0087\nc\n2\n\n=\n\n\n\u00ce\u00a3\nk\n\n\n\n\n\n(O\u00e2\u0088\u0092E)\n\n2\n\n\nE\n\n\n=\n3\n\n\n\nThe decision is not to reject the null hypothesis because the calculated value of the test statistic is not in the tail of the distribution.Conclusion: At a 5% level of significance, from the sample data, there is not sufficient evidence to conclude that the absent days do not occur with equal frequencies.\n\n\n\nTry It \n11.5\n\n\n\nTeachers want to know which night each week their students are doing most of their homework. Most teachers think that students do homework equally throughout the week. Suppose a random sample of 56 students were asked on which night of the week they did the most homework. The results were distributed as in Table 11.8.Sunday Monday Tuesday Wednesday Thursday Friday SaturdayNumber of students 11 8 10 7 10 5 5\nTable \n11.8\n \n \n\nFrom the population of students, do the nights for the highest number of students doing the majority of their homework occur with equal frequencies during a week? What type of hypothesis test should you use?\n\nExample \n11.6\n \n\nOne study indicates that the number of televisions that American families have is distributed (this is the given distribution for the American population) as in Table 11.9.\n\nNumber of Televisions\nPercent\n\n\n\n0\n10\n\n\n1\n16\n\n\n2\n55\n\n\n3\n11\n\n\n4+\n8\n\n\nTable \n11.9\n \n \n\n\nThe table contains expected (E) percents.\nA random sample of 600 families in the far western United States resulted in the data in Table 11.10.\n\nNumber of Televisions\nFrequency\n\n\n\n0\n66\n\n\n1\n119\n\n\n2\n340\n\n\n3\n60\n\n\n4+\n15\n\n\n\nTotal = 600\n\n\nTable \n11.10\n \n \n\nThe table contains observed (O) frequency values.\nAt the 1% significance level, does it appear that the distribution \"number of televisions\" of far western United States families is different from the distribution for the American population as a whole?\n\n\n\n\nSolution\n1\n\n\nThis problem asks you to test whether the far western United States families distribution fits the distribution of the American families. This test is always right-tailed.\nThe first table contains expected percentages. To get expected (E) frequencies, multiply the percentage by 600. The expected frequencies are shown in Table 11.11.\n\n\nNumber of televisions\nPercent\nExpected frequency\n\n\n\n\n0\n10\n(0.10)(600) = 60\n\n\n1\n16\n(0.16)(600) = 96\n\n\n2\n55\n(0.55)(600) = 330\n\n\n3\n11\n(0.11)(600) = 66\n\n\nover 3\n8\n(0.08)(600) = 48\n\n\n\nTable \n11.11\n \n \n\nTherefore, the expected frequencies are 60, 96, 330, 66, and 48. H0: The \"number of televisions\" distribution of far western United States families is the same as the \"number of televisions\" distribution of the American population.Ha: The \"number of televisions\" distribution of far western United States families is different from the \"number of televisions\" distribution of the American population.Distribution for the test: \n\n\n\u00cf\u0087\n4\n2\n\n\n\n\n\u00cf\u0087\n4\n2\n\n where df = (the number of cells) \u00e2\u0080\u0093 1 = 5 \u00e2\u0080\u0093 1 = 4.Calculate the test statistic: \u00cf\u00872 = 29.65\n\nGraph:\n\n\n\n\n\nFigure \n11.6\n\nThe graph of the Chi-square shows the distribution and marks the critical value with four degrees of freedom at 99% level of confidence, \u00ce\u00b1 = .01, 13.277. The graph also marks the calculated chi squared test statistic of 29.65. Comparing the test statistic with the critical value, as we have done with all other hypothesis tests, we reach the conclusion. Make a decision: Because the test statistic is in the tail of the distribution we cannot accept the null hypothesis. This means you reject the belief that the distribution for the far western states is the same as that of the American population as a whole.Conclusion: At the 1% significance level, from the data, there is sufficient evidence to conclude that the \"number of televisions\" distribution for the far western United States is different from the \"number of televisions\" distribution for the American population as a whole.\n\n\n\nTry It \n11.6\n\n\n\n\nThe expected percentage of the number of pets students have in their homes is distributed (this is the given distribution for the student population of the United States) as in Table 11.12.Number of pets Percent\n0 18\n1 25\n2 30\n3 18\n4+ 9\n\nTable \n11.12\n \n \n\nA random sample of 1,000 students from the Eastern United States resulted in the data in Table 11.13.\nNumber of pets Frequency\n\n0  210\n1 240\n2 320\n3 140\n4+ 90\n\nTable \n11.13\n \n \n\nAt the 1% significance level, does it appear that the distribution \u00e2\u0080\u009cnumber of pets\u00e2\u0080\u009d of students in the Eastern United States is different from the distribution for the United States student population as a whole? \n\n\nExample \n11.7\n \n\n\n\nSuppose you flip two coins 100 times. The results are 20 HH, 27 HT, 30 TH, and 23 TT. Are the coins fair? Test at a 5% significance level.\n\n\n\nSolution\n1\n\n\nThis problem can be set up as a goodness-of-fit problem. The sample space for flipping two fair coins is {HH, HT, TH, TT}. Out of 100 flips, you would expect 25 HH, 25 HT, 25 TH, and 25 TT. This is the expected distribution from the binomial probability distribution. The question, \"Are the coins fair?\" is the same as saying, \"Does the distribution of the coins (20 HH, 27 HT, 30 TH, 23 TT) fit the expected distribution?\"Random Variable: Let X = the number of heads in one flip of the two coins. X takes on the values 0, 1, 2. (There are 0, 1, or 2 heads in the flip of two coins.) Therefore, the number of cells is three. Since X = the number of heads, the observed frequencies are 20 (for two heads), 57 (for one head), and 23 (for zero heads or both tails). The expected frequencies are 25 (for two heads), 50 (for one head), and 25 (for zero heads or both tails). This test is right-tailed.H0: The coins are fair.\n Ha: The coins are not fair.\n Distribution for the test:\n\n\n\u00cf\u0087\n2\n2\n\n\n\u00cf\u0087\n2\n2\n\nwhere\n    df = 3 \u00e2\u0080\u0093 1 = 2.Calculate the test statistic: \u00cf\u00872 = 2.14\nGraph:\n\n\n\n\n\n\nFigure \n11.7\n\nThe graph of the Chi-square shows the distribution and marks the critical value with two degrees of freedom at 95% level of confidence, \u00ce\u00b1 = 0.05, 5.991. The graph also marks the calculated \u00cf\u00872 test statistic of 2.14. Comparing the test statistic with the critical value, as we have done with all other hypothesis tests, we reach the conclusion.\nConclusion: There is insufficient evidence to conclude that the coins are not fair: we cannot reject the null hypothesis that the coins are fair.\n\n\n\n"}, {"chapter": "11.4", "title": "Test of Independence", "mathml": "<math display=\"block\"><semantics><mrow>\n<mrow>\n<munder>\n<mi fontsize=\"2\">\u00ce\u00a3</mi>\n<mrow>\n<mo stretchy=\"false\">(</mo><mi>i</mi><mo>\u00e2\u008b\u0085</mo><mi>j</mi><mo stretchy=\"false\">)</mo>\n</mrow>\n</munder>\n<mfrac>\n<mrow>\n<msup>\n<mrow>\n<mo stretchy=\"false\">(</mo><mi>O</mi><mo>\u00e2\u0080\u0093</mo><mi>E</mi><mo stretchy=\"false\">)</mo>\n</mrow>\n<mn>2</mn>\n</msup>\n</mrow>\n<mi>E</mi>\n</mfrac>\n</mrow>\n</mrow><annotation-xml encoding=\"MathML-Content\"><mrow>\n<munder>\n<mi fontsize=\"2\">\u00ce\u00a3</mi>\n<mrow>\n<mo stretchy=\"false\">(</mo><mi>i</mi><mo>\u00e2\u008b\u0085</mo><mi>j</mi><mo stretchy=\"false\">)</mo>\n</mrow>\n</munder>\n<mfrac>\n<mrow>\n<msup>\n<mrow>\n<mo stretchy=\"false\">(</mo><mi>O</mi><mo>\u00e2\u0080\u0093</mo><mi>E</mi><mo stretchy=\"false\">)</mo>\n</mrow>\n<mn>2</mn>\n</msup>\n</mrow>\n<mi>E</mi>\n</mfrac>\n</mrow></annotation-xml></semantics></math></div", "content": "\nTests of independence involve using a contingency table of observed (data) values.\nThe test statistic for a test of independence is similar to that of a goodness-of-fit test:\n\n\n\u00ce\u00a3\n\n(i\u00e2\u008b\u0085j)\n\n\n\n\n\n\n(O\u00e2\u0080\u0093E)\n\n2\n\n\nE\n\n\n\n\n\u00ce\u00a3\n\n(i\u00e2\u008b\u0085j)\n\n\n\n\n\n\n(O\u00e2\u0080\u0093E)\n\n2\n\n\nE\n\nwhere:\nO = observed values\nE = expected values\ni = the number of rows in the table\nj = the number of columns in the table\nThere are i\u00e2\u008b\u0085 ji\u00e2\u008b\u0085 j terms of the form \n\n\n\n\n\n(O\u00e2\u0080\u0093E)\n\n2\n\n\nE\n\n\n\n\n\n\n\n(O\u00e2\u0080\u0093E)\n\n2\n\n\nE\n\n. A test of independence determines whether two factors are independent or not. You first encountered the term independence in 3.2 Independent and Mutually Exclusive Events earlier. As a review, consider the following example.\nNote\n\n\nThe expected value inside each cell needs to be at least five in order for you to use this test.\nExample \n11.8\n \n\n\nSuppose A = a speeding violation in the last year and B = a cell phone user while driving. If A and B are independent then P(A \u00e2\u0088\u00a9\u00e2\u0088\u00a9 B) = P(A)P(B). A \u00e2\u0088\u00a9\u00e2\u0088\u00a9 B is the event that a driver received a speeding violation last year and also used a cell phone while driving. Suppose, in a study of drivers who received speeding violations in the last year, and who used cell phone while driving, that 755 people were surveyed. Out of the 755, 70 had a speeding violation and 685 did not; 305 used cell phones while driving and 450 did not.Let y = expected number of drivers who used a cell phone while driving and received speeding violations.\nIf A and B are independent, then P(A \u00e2\u0088\u00a9\u00e2\u0088\u00a9 B) = P(A)P(B). By substitution,\n\n\ny\n\n755\n\n\n=(\n\n\n\n70\n\n\n755\n\n\n\n)(\n\n\n\n305\n\n\n755\n\n\n\n)\n\n\n\ny\n\n755\n\n\n=(\n\n\n\n70\n\n\n755\n\n\n\n)(\n\n\n\n305\n\n\n755\n\n\n\n)\n\n\nSolve for y: y = \n\n\n\n(70)(305)\n\n\n755\n\n\n=28.3\n\n\n\n\n(70)(305)\n\n\n755\n\n\n=28.3\n About 28 people from the sample are expected to use cell phones while driving and to receive speeding violations.In a test of independence, we state the null and alternative hypotheses in words. Since the contingency table consists of two factors, the null hypothesis states that the factors are independent and the alternative hypothesis states that they are not independent (dependent). If we do a test of independence using the example, then the null hypothesis is:H0H0: Being a cell phone user while driving and receiving a speeding violation are independent events; in other words, they have no effect on each other.If the null hypothesis were true, we would expect about 28 people to use cell phones while driving and to receive a speeding violation.\nThe test of independence is always right-tailed because of the calculation of the test statistic. If the expected and observed values are not close together, then the test statistic is very large and way out in the right tail of the chi-square curve, as it is in a goodness-of-fit.The number of degrees of freedom for the test of independence is:df = (number of columns - 1)(number of rows - 1)\nThe following formula calculates the expected number (E):\nE=(row total)(column total)total number surveyedE=(row total)(column total)total number surveyed\n\nTry It \n11.8\n\n\n\n\nA sample of 300 students is taken. Of the students surveyed, 50 were music students, while 250 were not. Ninety-seven of the 300 surveyed were on the honor roll, while 203 were not. If we assume being a music student and being on the honor roll are independent events, what is the expected number of music students who are also on the honor roll?\n\n\nExample \n11.9\n \n\n\nA volunteer group, provides from one to nine hours each week with disabled senior citizens. The program recruits among community college students, four-year college students, and nonstudents. In Table 11.14 is a sample of the adult volunteers and the number of hours they volunteer per week. \n\nType of volunteer\n1\u00e2\u0080\u00933 Hours\n4\u00e2\u0080\u00936 Hours\n7\u00e2\u0080\u00939 Hours\nRow total\n\n\n\nCommunity college students\n111\n96\n48\n255\n\n\nFour-year college students\n96\n133\n61\n290\n\n\nNonstudents\n91\n150\n53\n294\n\n\nColumn total\n298\n379\n162\n839\n\n\nTable \n11.14\n \nNumber of Hours Worked Per Week by Volunteer Type (Observed)\n \nThe table contains observed (O) values (data). \n\n\n Is the number of hours volunteered independent of the type of volunteer? \n\n\n\n\nSolution\n1\n\n\nThe observed table and the question at the end of the problem, \"Is the number of hours volunteered independent of the type of volunteer?\" tell you this is a test of independence. The two factors are number of hours volunteered and type of volunteer. This test is always right-tailed.\nH0: The number of hours volunteered is independent of the type of volunteer.Ha: The number of hours volunteered is dependent on the type of volunteer.The expected results are in Table 11.15.\n \n\nType of volunteer 1-3 Hours 4-6 Hours 7-9 Hours\n\n\nCommunity college students90.57115.1949.24\nFour-year college students103.00131.0056.00\nNonstudents104.42132.8156.77\n\n\nTable \n11.15\n \nNumber of Hours Worked Per Week by Volunteer Type (Expected)\n \nThe table contains expected (E) values (data).\n\nFor example, the calculation for the expected frequency for the top left cell is \n\n\nE=\n\n(row\u00a0total)(column\u00a0total)\n\n\ntotal\u00a0number\u00a0surveyed\n\n\n=\n\n(\n\n255\n\n)(\n\n298\n\n)\n\n\n839\n\n\n=90.57\n\n\nE=\n\n(row\u00a0total)(column\u00a0total)\n\n\ntotal\u00a0number\u00a0surveyed\n\n\n=\n\n(\n\n255\n\n)(\n\n298\n\n)\n\n\n839\n\n\n=90.57\n\n\nCalculate the test statistic: \u00cf\u00872 = 12.99 (calculator or computer)\nDistribution for the test:\n\n\n\n\n\u00cf\u0087\n4\n2\n\n\n\n\n\u00cf\u0087\n4\n2\n\n\n\n\n\ndf = (3 columns \u00e2\u0080\u0093 1)(3 rows \u00e2\u0080\u0093 1) = (2)(2) = 4\nGraph:\n\n\n\n\n\n\nFigure \n11.8\n\nThe graph of the Chi-square shows the distribution and marks the critical value with four degrees of freedom at 95% level of confidence, \u00ce\u00b1 = 0.05, 9.488. The graph also marks the calculated \u00cf\u0087c2\u00cf\u0087c2 test statistic of 12.99. Comparing the test statistic with the critical value, as we have done with all other hypothesis tests, we reach the conclusion. Make a decision: Because the calculated test statistic is in the tail we cannot accept H0. This means that the factors are not independent.Conclusion: At a 5% level of significance, from the data, there is sufficient evidence to conclude that the number of hours volunteered and the type of volunteer are dependent on one another.\nFor the example in Table 11.15, if there had been another type of volunteer, teenagers, what would the degrees of freedom be?\n\n\n\nTry It \n11.9\n\n\n\nThe Bureau of Labor Statistics gathers data about employment in the United States. A sample is taken to calculate the number of U.S. citizens working in one of several industry sectors over time. Table 11.16 shows the results: \n\n\nIndustry sector\n2000\n2010\n2020\nTotal\n\n\n\nNonagriculture wage and salary\n13,243\n13,044\n15,018\n41,305\n\n\nGoods-producing, excluding agriculture\n2,457\n1,771\n1,950\n6,178\n\n\nServices-providing\n10,786\n11,273\n13,068\n35,127\n\n\nAgriculture, forestry, fishing, and hunting\n240\n214\n201\n655\n\n\nNonagriculture self-employed and unpaid family worker\n931\n894\n972\n2,797\n\n\nSecondary wage and salary jobs in agriculture and private household industries\n14\n11\n11\n36\n\n\nSecondary jobs as a self-employed or unpaid family worker\n196\n144\n152\n492\n\n\nTotal\n27,867\n27,351\n31,372\n86,590\n\n\nTable \n11.16\n \n \n\nWe want to know if the change in the number of jobs is independent of the change in years. State the null and alternative hypotheses and the degrees of freedom.\n\n\nExample \n11.10\n \n\n\nDe Anza College is interested in the relationship between anxiety level and the need to succeed in school. A random sample of 400 students took a test that measured anxiety level and need to succeed in school. Table 11.17 shows the results. De Anza College wants to know if anxiety level and need to succeed in school are independent events.\n\nNeed to succeed in school\nHigh anxiety\nMed-high anxiety\nMedium anxiety\nMed-low anxiety\nLow anxiety\nRow total\n\n\n\nHigh need\n35\n42\n53\n15\n10\n155\n\n\nMedium need\n18\n48\n63\n33\n31\n193\n\n\nLow need\n4\n5\n11\n15\n17\n52\n\n\nColumn total\n57\n95\n127\n63\n58\n400\n\n\nTable \n11.17\n \nNeed to Succeed in School vs. Anxiety Level\n \n\n\na. How many high anxiety level students are expected to have a high need to succeed in school? \n\n\n\nSolution\n1\n\n\na. The column total for a high anxiety level is 57. The row total for high need to succeed in school is 155. The sample size or total surveyed is 400. \n\n\n\nE\n=\n\n(row total)(column total)\ntotal surveyed\n\n=\n\n\n155\n\u00e2\u008b\u0085\n57\n\n400\n\n=\n22.09\n\n\nE\n=\n\n(row total)(column total)\ntotal surveyed\n\n=\n\n\n155\n\u00e2\u008b\u0085\n57\n\n400\n\n=\n22.09\n\n\n\nThe expected number of students who have a high anxiety level and a high need to succeed in school is about 22.\n\n\n\nb. If the two variables are independent, how many students do you expect to have a low need to succeed in school and a med-low level of anxiety? \n\n\n\nSolution\n2\n\n\nb. The column total for a med-low anxiety level is 63. The row total for a low need to succeed in school is 52. The sample size or total surveyed is 400.\n\n\n\n\nc. E=(row total)(column total)\ntotal surveyedE=(row total)(column total)\ntotal surveyed = ________\n\n\n\n\nSolution\n3\n\n\nc. E=(row total)(column total)total surveyed = 8.19E=(row total)(column total)total surveyed = 8.19 \n\n\n\n\nd. The expected number of students who have a med-low anxiety level and a low need to\n  succeed in school is about ________.\n\n\n\n\nSolution\n4\n\n\nd. 8\n\n\n\n\n"}, {"chapter": "12.1", "title": "Test of Two Variances", "mathml": "<math display=\"block\"><semantics><mrow>\n<msub><mi>H</mi><mn>0</mn></msub>\n<mo>:</mo>\n<mfrac>\n<mrow><msup><mrow><msub><mi>\u00cf\u0083</mi><mn>1</mn></msub></mrow><mn>2</mn></msup>\n</mrow><mrow>\n<msup><mrow><msub><mi>\u00cf\u0083</mi><mn>2</mn></msub></mrow><mn>2</mn></msup></mrow>\n</mfrac>\n<mo>=</mo>\n<msub><mi>\u00ce\u00b4</mi><mn>0</mn></msub>\n</mrow><annotation-xml encoding=\"MathML-Content\"><msub><mi>H</mi><mn>0</mn></msub><mo>:</mo><mfrac>\n<mrow><msup><mrow><msub><mi>\u00cf\u0083</mi><mn>1</mn></msub></mrow><mn>2</mn></msup>\n</mrow><mrow>\n<msup><mrow><msub><mi>\u00cf\u0083</mi><mn>2</mn></msub></mrow><mn>2</mn></msup></mrow>\n</mfrac><mo>=</mo><msub><mi>\u00ce\u00b4</mi><mn>0</mn></msub></annotation-xml></semantics></math></div", "content": "\nThis chapter introduces a new probability density function, the F distribution. This distribution is used for many applications including ANOVA and for testing equality across multiple means. We begin with the F distribution and the test of hypothesis of differences in variances. It is often desirable to compare two variances rather than two averages. For instance, college administrators would like two college professors grading exams to have the same variation in their grading. In order for a lid to fit a container, the variation in the lid and the container should be approximately the same. A supermarket might be interested in the variability of check-out times for two checkers. In finance, the variance is a measure of risk and thus an interesting question would be to test the hypothesis that two different investment portfolios have the same variance, the volatility.In order to perform a F test of two variances, it is important that the following are true:\nThe populations from which the two samples are drawn are approximately normally distributed.\nThe two populations are independent of each other.\n\nUnlike most other hypothesis tests in this book, the F test for equality of two variances is very sensitive to deviations from normality. If the two distributions are not normal, or close, the test can give a biased result for the test  statistic.Suppose we sample randomly from two independent normal populations. Let \n\n\n\u00cf\u0083\n1\n2\n\n\n\n\n\u00cf\u0083\n1\n2\n\n and \n\n\n\u00cf\u0083\n2\n2\n\n\n\n\n\u00cf\u0083\n2\n2\n\n be the unknown population variances and \n\n\ns\n1\n2\n\n\n\n\ns\n1\n2\n\n and \n\n\ns\n2\n2\n\n\n\n\ns\n2\n2\n\n be the sample variances. Let the\nsample sizes be n1 and n2. Since we are interested in comparing the two sample variances, we use the F ratio:\n\nF=\n\n[ \n\n\n\n\n\ns\n1\n\n\n2\n\n\n\n\n\n\n\u00cf\u0083\n1\n\n\n2\n\n\n\n ]\n\n\n[ \n\n\n\n\n\ns\n2\n\n\n2\n\n\n\n\n\n\n\u00cf\u0083\n2\n\n\n2\n\n\n\n ]\n\n\n\n\nF=\n\n[ \n\n\n\n\n\ns\n1\n\n\n2\n\n\n\n\n\n\n\u00cf\u0083\n1\n\n\n2\n\n\n\n ]\n\n\n[ \n\n\n\n\n\ns\n2\n\n\n2\n\n\n\n\n\n\n\u00cf\u0083\n2\n\n\n2\n\n\n\n ]\n\n\nF has the distribution F ~ F(n1 \u00e2\u0080\u0093 1, n2 \u00e2\u0080\u0093 1)\nwhere n1 \u00e2\u0080\u0093 1 are the degrees of freedom for the numerator and n2 \u00e2\u0080\u0093 1 are the degrees of freedom for the denominator.\nIf the null hypothesis is \n\n\n\u00cf\u0083\n1\n2\n\n=\n\u00cf\u0083\n2\n2\n\n\n\n\n\u00cf\u0083\n1\n2\n\n=\n\u00cf\u0083\n2\n2\n\n, then the F Ratio, test statistic, becomes \n\nFc=\n\n[ \n\n\n\n\n\ns\n1\n\n\n2\n\n\n\n\n\n\n\u00cf\u0083\n1\n\n\n2\n\n\n\n ]\n\n\n[ \n\n\n\n\n\ns\n2\n\n\n2\n\n\n\n\n\n\n\u00cf\u0083\n2\n\n\n2\n\n\n\n ]\n\n\n=\n\n\n\n\ns\n1\n\n\n2\n\n\n\n\n\n\ns\n2\n\n\n2\n\n\n\n\n\nFc=\n\n[ \n\n\n\n\n\ns\n1\n\n\n2\n\n\n\n\n\n\n\u00cf\u0083\n1\n\n\n2\n\n\n\n ]\n\n\n[ \n\n\n\n\n\ns\n2\n\n\n2\n\n\n\n\n\n\n\u00cf\u0083\n2\n\n\n2\n\n\n\n ]\n\n\n=\n\n\n\n\ns\n1\n\n\n2\n\n\n\n\n\n\ns\n2\n\n\n2\n\n\n\n\nThe various forms of the hypotheses tested are:\n\nTwo-Tailed Test\nOne-Tailed Test\nOne-Tailed Test\n\n\n\nH0: \u00cf\u008312 = \u00cf\u008322 \nH0: \u00cf\u008312 \u00e2\u0089\u00a4 \u00cf\u008322\nH0: \u00cf\u008312 \u00e2\u0089\u00a5 \u00cf\u008322\n\n\nH1: \u00cf\u008312 \u00e2\u0089\u00a0 \u00cf\u008322\nH1: \u00cf\u008312 > \u00cf\u008322\nH1: \u00cf\u008312 < \u00cf\u008322\n\n\nTable \n12.1\n \n \n\nA more general form of the null and alternative hypothesis for a two tailed test would be :\nH0\n:\n\n\u00cf\u008312\n\n\u00cf\u008322\n\n=\n\u00ce\u00b40\nH0:\n\u00cf\u008312\n\n\u00cf\u008322\n=\u00ce\u00b40\nHa\n:\n\n\u00cf\u008312\n\n\u00cf\u008322\n\n\u00e2\u0089\u00a0\n\u00ce\u00b40\nHa:\n\u00cf\u008312\n\n\u00cf\u008322\n\u00e2\u0089\u00a0\u00ce\u00b40Where if \u00ce\u00b40 = 1 it is a simple test of the hypothesis that the two variances are equal. This form of the hypothesis does have the benefit of allowing for tests that are more than for simple differences and can accommodate tests for specific differences as we did for differences in means and differences in proportions. This form of the hypothesis also shows the relationship between the F distribution and the \u00cf\u00872 : the F is a ratio of two chi squared distributions a distribution we saw in the last chapter. This is helpful in determining the degrees of freedom of the resultant F distribution.   If the two populations have equal variances, then \n\n\ns\n1\n2\n\n\n\n\ns\n1\n2\n\n and \n\n\ns\n2\n2\n\n\n\n\ns\n2\n2\n\n are close in value and the test statistic, \n\nFc=\n\n\n\n\ns\n1\n\n\n2\n\n\n\n\n\n\ns\n2\n\n\n2\n\n\n\n\n\nFc=\n\n\n\n\ns\n1\n\n\n2\n\n\n\n\n\n\ns\n2\n\n\n2\n\n\n\n is close to one. But if the two population variances are very different, \n\n\ns\n1\n2\n\n\n\n\ns\n1\n2\n\n and \n\n\ns\n2\n2\n\n\n\n\ns\n2\n2\n\n tend to be very different, too. Choosing \n\n\ns\n1\n2\n\n\n\n\ns\n1\n2\n\n as the larger sample variance causes the ratio \n\n\n\n\n\n\ns\n1\n\n\n2\n\n\n\n\n\n\ns\n2\n\n\n2\n\n\n\n\n\n\n\n\n\n\ns\n1\n\n\n2\n\n\n\n\n\n\ns\n2\n\n\n2\n\n\n\n to be greater than one. If \n\n\ns\n1\n2\n\n\n\n\ns\n1\n2\n\n and \n\n\ns\n2\n2\n\n\n\n\ns\n2\n2\n\n are far apart, then \n\nFc=\n\n\n\n\ns\n1\n\n\n2\n\n\n\n\n\n\ns\n2\n\n\n2\n\n\n\n\n\nFc=\n\n\n\n\ns\n1\n\n\n2\n\n\n\n\n\n\ns\n2\n\n\n2\n\n\n\n is a large number.Therefore, if F is close to one, the evidence favors the null hypothesis (the two population variances are equal). But if F is much larger than one, then the evidence is against the null hypothesis. In essence, we are asking if the calculated F statistic, test statistic,  is significantly different from one.To determine the critical points we have to find F\u00ce\u00b1,df1,df2. See Appendix A for the F table. This F table has values for various levels of significance from 0.1 to 0.001 designated as \"p\" in the first column. To find the critical value choose the desired significance level and follow down and across to find the critical value at the intersection of the two different degrees of freedom. The F distribution has two different degrees of freedom, one associated with the numerator, df1, and one associated with the denominator, df2 and to complicate matters the F distribution is not symmetrical and changes the degree of skewness as the degrees of freedom change. The degrees of freedom in the numerator is n1-1, where n1 is the sample size for group 1, and the degrees of freedom in the denominator is n2-1, where n2 is the sample size for group 2.  F\u00ce\u00b1,df1,df2 will give the critical value on the upper end of the F distribution. To find the critical value for the lower end of the distribution, reverse the degrees of freedom and divide the F-value from the table into one.Upper tail critical value : F\u00ce\u00b1,df1,df2\nLower tail critical value : 1/F\u00ce\u00b1,df2,df1\nWhen the calculated value of  F is between the critical values, not in the tail, we cannot reject the null hypothesis that the two variances came from a population with the same variance. If the calculated F-value is in either tail we cannot accept the null hypothesis just as we have been doing for all of the previous tests of hypothesis.An alternative way of finding the critical values of the F distribution makes the use of the F-table easier. We note in the F-table that all the values of F are greater than one therefore the critical F value for the left hand tail will always be less than one because to find the critical value on the left tail we divide an F value into the number one as shown above.  We also note that if the sample variance in the numerator of the test statistic is larger than the sample variance in the denominator, the resulting F value will be greater than one. The shorthand method for this test is thus to be sure that the larger of the two sample variances is placed in the numerator to calculate the test statistic. This will mean that only the right hand tail critical value will have to be found in the F-table.\nExample \n12.1\n \n\n\n\n\nTwo college instructors are interested in whether or not there is any variation in the way they grade math exams. They each grade the same set of 10 exams. The first instructor's grades have a variance of 52.3. The second instructor's grades have a variance of 89.9. Test the claim that the first instructor's variance is smaller. (In most colleges, it is desirable for the variances of exam grades to be nearly the same among instructors.) The level of significance is 10%.\n\n\n\nSolution\n1\n\n\nLet 1 and 2 be the subscripts that indicate the first and second instructor, respectively.\nn1 = n2 = 10.H0: \n\n\n\u00cf\u0083\n1\n2\n\n\u00e2\u0089\u00a5\n\u00cf\u0083\n2\n2\n\n\n\n\n\u00cf\u0083\n1\n2\n\n\u00e2\u0089\u00a5\n\u00cf\u0083\n2\n2\n\n and Ha: \n\n\n\u00cf\u0083\n1\n2\n\n\u00a0<\u00a0\n\u00cf\u0083\n2\n2\n\n\n\n\n\u00cf\u0083\n1\n2\n\n\u00a0<\u00a0\n\u00cf\u0083\n2\n2\n\nCalculate the test statistic: By the null hypothesis \n\n(\n\u00cf\u0083\n1\n2\n\n\u00a0\u00e2\u0089\u00a5\u00a0\n\u00cf\u0083\n2\n2\n\n)\n\n\n(\n\u00cf\u0083\n1\n2\n\n\u00a0\u00e2\u0089\u00a5\u00a0\n\u00cf\u0083\n2\n2\n\n)\n, the F statistic is:\n\n\n\n\nFc\n=\n\n\n\n\ns\n2\n\n\n2\n\n\n\n\n\n\ns\n1\n\n\n2\n\n\n\n=\n\n89.9\n\n\n52.3\n\n\n=1.719\n\n\n\n\nFc\n=\n\n\n\n\ns\n2\n\n\n2\n\n\n\n\n\n\ns\n1\n\n\n2\n\n\n\n=\n\n89.9\n\n\n52.3\n\n\n=1.719\n\n\n\n\nCritical value for the test: F9,9 = 5.35 where n1 \u00e2\u0080\u0093 1 = 9 and n2 \u00e2\u0080\u0093 1 = 9.\n\n\n\n\nFigure \n12.2\n\nMake a decision: Since the calculated F value is not in the tail we cannot reject H0.Conclusion: With a 10% level of significance, from the data, there is insufficient evidence to conclude that the variance in grades for the first instructor is smaller.\n\n\n\n\n\nTry It \n12.1\n\n\n\n\nThe New York Choral Society divides male singers up into four categories from highest voices to lowest: Tenor1, Tenor2, Bass1, Bass2. In the table are heights of the men in the Tenor1 and Bass2 groups. One suspects that taller men will have lower voices, and that the variance of height may go up with the lower voices as well. Do we have good evidence that the variance of the heights of singers in each of these two groups (Tenor1 and Bass2) are different?\n\n\nTenor1\nBass 2\nTenor 1\nBass 2\nTenor 1\nBass 2\n\n\n\n69\n72\n67\n72\n68\n67\n\n\n72\n75\n70\n74\n67\n70\n\n\n71\n67\n65\n70\n64\n70\n\n\n66\n75\n72\n66\n\n69\n\n\n76\n74\n70\n68\n\n72\n\n\n74\n72\n68\n75\n\n71\n\n\n71\n72\n64\n68\n\n74\n\n\n66\n74\n73\n70\n\n75\n\n\n68\n72\n66\n72\n\n\n\n\nTable \n12.2\n \n \n\n\n\n\n"}, {"chapter": "13.1", "title": "The Correlation Coefficient r", "mathml": "<math display=\"block\"><semantics><mrow>\n<mi>r</mi>\n<mo>=</mo>\n<mfrac>\n<mrow>\n<mfrac><mn>1</mn><mrow><mi>n</mi><mo>\u00e2\u0088\u0092</mo><mn>1</mn></mrow></mfrac>\n<mo>\u00ce\u00a3</mo><mo>(</mo><msub><mi>X</mi><mrow><mn>1</mn><mi>i</mi></mrow></msub>\n<mo>\u00e2\u0088\u0092</mo>\n<msub><mrow><mover><mi>X</mi><mo>\u00e2\u0080\u0093</mo></mover></mrow><mn>1</mn></msub><mo>)</mo><mo>(</mo><msub><mi>X</mi><mrow><mn>2</mn><mi>i</mi></mrow></msub>\n<mo>\u00e2\u0088\u0092</mo>\n<msub><mrow><mover><mi>X</mi><mo>\u00e2\u0080\u0093</mo></mover></mrow><mn>2</mn></msub><mo>)</mo>\n</mrow>\n<mrow>\n<msub><mi>s</mi><mrow><msub><mi>x</mi><mn>1</mn></msub></mrow></msub><msub><mi>s</mi><mrow><msub><mi>x</mi><mn>2</mn></msub></mrow></msub>\n</mrow>\n</mfrac>\n</mrow><annotation-xml encoding=\"MathML-Content\"><mi>r</mi><mo>=</mo><mfrac>\n<mrow>\n<mfrac><mn>1</mn><mrow><mi>n</mi><mo>\u00e2\u0088\u0092</mo><mn>1</mn></mrow></mfrac>\n<mo>\u00ce\u00a3</mo><mo>(</mo><msub><mi>X</mi><mrow><mn>1</mn><mi>i</mi></mrow></msub>\n<mo>\u00e2\u0088\u0092</mo>\n<msub><mrow><mover><mi>X</mi><mo>\u00e2\u0080\u0093</mo></mover></mrow><mn>1</mn></msub><mo>)</mo><mo>(</mo><msub><mi>X</mi><mrow><mn>2</mn><mi>i</mi></mrow></msub>\n<mo>\u00e2\u0088\u0092</mo>\n<msub><mrow><mover><mi>X</mi><mo>\u00e2\u0080\u0093</mo></mover></mrow><mn>2</mn></msub><mo>)</mo>\n</mrow>\n<mrow>\n<msub><mi>s</mi><mrow><msub><mi>x</mi><mn>1</mn></msub></mrow></msub><msub><mi>s</mi><mrow><msub><mi>x</mi><mn>2</mn></msub></mrow></msub>\n</mrow>\n</mfrac></annotation-xml></semantics></math></div", "content": "\nAs we begin this section we note that the type of data we will be working with has changed. Perhaps unnoticed, all the data we have been using is for a single variable. It may be from two samples, but it is still a univariate variable. The type of data described in the examples above and for any model of cause and effect is bivariate data \u00e2\u0080\u0094 \"bi\" for two variables. In reality, statisticians use multivariate data, meaning many variables. For our work we can classify data into three broad categories, time series data, cross-section data, and panel data. We met the first two very early on. Time series data measures a single unit of observation; say a person, or a company or a country, as time passes. What are measured will be at least two characteristics, say the person\u00e2\u0080\u0099s income, the quantity of a particular good they buy and the price they paid. This would be three pieces of information in one time period, say 1985. If we followed that person across time we would have those same pieces of information for 1985,1986, 1987, etc. This would constitute a times series data set. If we did this for 10 years we would have 30 pieces of information concerning this person\u00e2\u0080\u0099s consumption habits of this good for the past decade and we would know their income and the price they paid. A second type of data set is for cross-section data. Here the variation is not across time for a single unit of observation, but across units of observation during one point in time. For a particular period of time we would gather the price paid, amount purchased, and income of many individual people. A third type of data set is panel data. Here a panel of units of observation is followed across time. If we take our example from above we might follow 500 people, the unit of observation, through time, ten years, and observe their income, price paid and quantity of the good purchased. If we had 500 people and data for ten years for price, income and quantity purchased we would have 15,000 pieces of information. These types of data sets are very expensive to construct and maintain. They do, however, provide a tremendous amount of information that can be used to answer very important questions. As an example, what is the effect on the labor force participation rate of women as their family of origin, mother and father, age? Or are there differential effects on health outcomes depending upon the age at which a person started smoking? Only panel data can give answers to these and related questions because we must follow multiple people across time. The work we do here however will not be fully appropriate for data sets such as these. Beginning with a set of data with two independent variables we ask the question: are these related? One way to visually answer this question is to create a scatter plot of the data. We could not do that before when we were doing descriptive statistics because those data were univariate. Now we have bivariate data so we can plot in two dimensions. Three dimensions are possible on a flat piece of paper, but become very hard to fully conceptualize. Of course, more than three dimensions cannot be graphed although the relationships can be measured mathematically.  To provide mathematical precision to the measurement of what we see we use the correlation coefficient.  The correlation tells us something about the co-movement of two variables, but nothing about why this movement occurred. Formally, correlation analysis assumes that both variables being analyzed are independent variables. This means that neither one causes the movement in the other. Further, it means that neither variable is dependent on the other, or for that matter, on any other variable. Even with these limitations, correlation analysis can yield some interesting results.The correlation coefficient, \u00cf\u0081 (pronounced rho), is the mathematical statistic for a population that provides us with a measurement of the strength of a linear relationship between the two variables. For a sample of data, the statistic, r, developed by Karl Pearson in the early 1900s, is an estimate of the population correlation and is defined mathematically as:\nr\n=\n\n\n1n\u00e2\u0088\u00921\n\u00ce\u00a3(X1i\n\u00e2\u0088\u0092\nX\u00e2\u0080\u00931)(X2i\n\u00e2\u0088\u0092\nX\u00e2\u0080\u00932)\n\n\nsx1sx2\n\n\nr=\n\n1n\u00e2\u0088\u00921\n\u00ce\u00a3(X1i\n\u00e2\u0088\u0092\nX\u00e2\u0080\u00931)(X2i\n\u00e2\u0088\u0092\nX\u00e2\u0080\u00932)\n\n\nsx1sx2\n\nOR\nr\n=\n\n\n\u00ce\u00a3X1iX2i\n\u00e2\u0088\u0092\nnX\u00e2\u0080\u00931\u00e2\u0088\u0092X\u00e2\u0080\u00932\n\n\n\n(\u00ce\u00a3X12i\u00e2\u0088\u0092nX\u00e2\u0080\u009312)\n(\u00ce\u00a3X22i\u00e2\u0088\u0092nX\u00e2\u0080\u009322)\n\n\n\nr=\n\n\u00ce\u00a3X1iX2i\n\u00e2\u0088\u0092\nnX\u00e2\u0080\u00931\u00e2\u0088\u0092X\u00e2\u0080\u00932\n\n\n\n(\u00ce\u00a3X12i\u00e2\u0088\u0092nX\u00e2\u0080\u009312)\n(\u00ce\u00a3X22i\u00e2\u0088\u0092nX\u00e2\u0080\u009322)\n\n\nwhere sx1 and sx2 are the standard deviations of the two independent variables X1 and X2, X\u00e2\u0080\u00931X\u00e2\u0080\u00931 and X\u00e2\u0080\u00932X\u00e2\u0080\u00932 are the sample means of the two variables, and X1i and X2i  are the individual observations of X1 and X2. The correlation coefficient r ranges in value from -1 to 1. The second equivalent formula is often used because it may be computationally easier. As scary as these formulas look they are really just the ratio of the covariance between the two variables and the product of their two standard deviations. That is to say, it is a measure of relative variances.In practice all correlation and regression analysis will be provided through computer software designed for these purposes. Anything more than perhaps one-half a dozen observations creates immense computational problems. It was because of this fact that correlation, and even more so, regression, were not widely used research tools until after the advent of \u00e2\u0080\u009ccomputing machines\u00e2\u0080\u009d. Now the computing power required to analyze data using regression packages is deemed almost trivial by comparison to just a decade ago. To visualize any linear relationship that may exist review the plot of a scatter diagrams of the standardized data. Figure 13.2 presents several scatter diagrams and the calculated value of r. In panels (a) and (b) notice that the data generally trend together, (a) upward and (b) downward. Panel (a) is an example of a positive correlation and panel (b) is an example of a negative correlation, or relationship. The sign of the correlation coefficient tells us if the relationship is a positive or negative (inverse) one. If all the values of X1 and X2 are on a straight line the correlation coefficient will be either 1 or -1 depending on whether the line has a positive or negative slope and the closer to one or negative one the stronger the relationship between the two variables.  BUT ALWAYS REMEMBER THAT THE CORRELATION COEFFICIENT DOES NOT TELL US THE SLOPE.\n\nFigure \n13.2\n\nRemember, all the correlation coefficient tells us is whether or not the data are linearly related. In panel (d) the variables obviously have some type of very specific relationship to each other, but the correlation coefficient is zero, indicating no linear relationship exists. If you suspect a linear relationship between X1 and X2 then r can measure how strong the linear relationship is.What the VALUE of r tells us:The value of r is always between \u00e2\u0080\u00931 and +1: \u00e2\u0080\u00931 \u00e2\u0089\u00a4 r \u00e2\u0089\u00a4 1.\nThe size of the correlation r indicates the strength of the linear relationship between X1 and X2. Values of r close to \u00e2\u0080\u00931 or\nto +1 indicate a stronger linear relationship between  X1 and X2.\n\nIf r = 0 there is absolutely no linear relationship between X1 and X2 (no linear correlation).\nIf r = 1, there is perfect positive correlation. If r = \u00e2\u0080\u00931, there is perfect negative correlation. In both these cases, all of the original data points lie on a straight line: ANY straight line no matter what the slope. Of course, in the real world, this will not generally happen.What the SIGN of r tells usA positive value of r means that when X1 increases, X2 tends to increase and when X1 decreases, X2  tends to decrease (positive correlation).\nA negative value of r means that when X1 increases, X2 tends to decrease and when X1 decreases, X2 tends to increase (negative correlation).\n\n\nNote\n\nStrong correlation does not suggest that X1 causes X2 or X2 causes X1. We say \"correlation does not imply causation.\"\n"}, {"chapter": "13.3", "title": "Linear Equations", "mathml": "<math display=\"block\"><semantics><mrow>\n<mi>y</mi>\n<mo>=</mo>\n<mi>a</mi>\n<mo>+</mo>\n<mtext>bx</mtext>\n</mrow><annotation-xml encoding=\"MathML-Content\"><mi>y</mi><mo>=</mo><mi>a</mi><mo>+</mo><mtext>bx</mtext></annotation-xml></semantics></math>\n</div", "content": "\nLinear regression for two variables is based on a linear equation with one independent\nvariable. The equation has the form:\n\ny\n=\na\n+\nbx\ny=a+bx\n\nwhere a and b are constant numbers.The variable x is the independent variable, and y is the dependent variable. Another way to think about this equation is a statement of cause and effect. The X variable is the cause and the Y variable is the hypothesized effect. Typically, you choose a value to substitute for the independent variable and then solve for the dependent variable.\nExample \n13.1\n \n\n\nThe following examples are linear equations.\ny=3+2xy=3+2x\ny=\u00e2\u0080\u00930.01+1.2xy=\u00e2\u0080\u00930.01+1.2x\nThe graph of a linear equation of the form y = a + bx is a straight line. Any line that is not vertical can be described by this equation.\nExample \n13.2\n \n\n\nGraph the equation y = \u00e2\u0080\u00931 + 2x.\n\n\n\nFigure \n13.3\n\n\n\nTry It \n13.2\n\n\n\n\nIs the following an example of a linear equation? Why or why not?\n\n\n\nFigure \n13.4\n\n\n\n\n\nExample \n13.3\n \n\n\nAaron's Word Processing Service (AWPS) does word processing. The rate for services is $32 per hour plus a $31.50 one-time charge. The total cost to a customer depends on the number of hours it takes to complete the job.\n\n\nFind the equation that expresses the total cost in terms of the number of hours required to complete the job.\n\n\n\nSolution\n1\n\n\nLet x = the number of hours it takes to get the job done.\nLet y = the total cost to the customer.The $31.50 is a fixed cost. If it takes x hours to complete the job, then (32)(x) is the cost of the word processing only. The total cost is: y = 31.50 + 32x\n\n\n\n\nSlope and Y-Intercept of a Linear EquationFor the linear equation y = a + bx, b = slope and a = y-intercept. From algebra recall that the slope is a number that describes the steepness of a line, and the y-intercept is the y coordinate of the point (0, a) where the line crosses the y-axis. From calculus the slope is the first derivative of the function. For a linear function the slope is dy / dx = b where we can read the mathematical expression as \"the change in y (dy) that results from a change in x (dx) = b * dx\".\n\n\nFigure \n13.5\n \nThree possible graphs of y = a + bx. (a) If b > 0, the line slopes upward to the right. (b) If b = 0, the line is horizontal. (c) If b < 0, the line slopes downward to the right.\n\n\nExample \n13.4\n \n\n\nSvetlana tutors to make extra money for college. For each tutoring session, she charges a one-time fee of $25 plus $15 per hour of tutoring. A linear equation that expresses the total amount of money Svetlana earns for each session she tutors is y = 25 + 15x.\n\n\nWhat are the independent and dependent variables? What is the y-intercept and what is the slope? Interpret them using complete sentences.\n\n\n\n\nSolution\n1\n\n\nThe independent variable (x) is the number of hours Svetlana tutors each session. The dependent variable (y) is the amount, in dollars, Svetlana earns for each session.\nThe y-intercept is 25 (a = 25). At the start of the tutoring session, Svetlana charges a one-time fee of $25 (this is when x = 0). The slope is 15 (b = 15).  For each session, Svetlana earns $15 for each hour she tutors.\n\n\n\n\n\n"}, {"chapter": "13.4", "title": "The Regression Equation", "mathml": "<math display=\"block\"><semantics><mrow>\n<msub><mi>y</mi><mi>i</mi></msub>\n<mo>=</mo>\n<msub><mi>\u00ce\u00b2</mi><mn>0</mn></msub>\n<mo>+</mo>\n<msub><mi>\u00ce\u00b2</mi><mn>1</mn></msub>\n<msub><mi>X</mi><mrow><mn>1</mn><mi>i</mi></mrow></msub>\n<mo>+</mo>\n<msub><mi>\u00ce\u00b2</mi><mn>2</mn></msub>\n<msub><mi>X</mi><mrow><mn>2</mn><mi>i</mi></mrow></msub>\n<mo>+</mo>\n<mo>\u00e2\u008b\u00af</mo>\n<mo>+</mo>\n<msub><mi>\u00ce\u00b2</mi><mi>k</mi></msub>\n<msub><mi>X</mi><mrow><mi>k</mi><mi>i</mi></mrow></msub>\n<mo>+</mo>\n<msub><mi>\u00ce\u00b5</mi><mi>i</mi></msub>\n</mrow><annotation-xml encoding=\"MathML-Content\"><msub><mi>y</mi><mi>i</mi></msub><mo>=</mo><msub><mi>\u00ce\u00b2</mi><mn>0</mn></msub><mo>+</mo><msub><mi>\u00ce\u00b2</mi><mn>1</mn></msub><msub><mi>X</mi><mrow><mn>1</mn><mi>i</mi></mrow></msub><mo>+</mo><msub><mi>\u00ce\u00b2</mi><mn>2</mn></msub><msub><mi>X</mi><mrow><mn>2</mn><mi>i</mi></mrow></msub><mo>+</mo><mo>\u00e2\u008b\u00af</mo><mo>+</mo><msub><mi>\u00ce\u00b2</mi><mi>k</mi></msub><msub><mi>X</mi><mrow><mi>k</mi><mi>i</mi></mrow></msub><mo>+</mo><msub><mi>\u00ce\u00b5</mi><mi>i</mi></msub></annotation-xml></semantics></math></div", "content": "\nRegression analysis is a statistical technique that can test the hypothesis that a variable is dependent upon one or more other variables. Further, regression analysis can provide an estimate of the magnitude of the impact of a change in one variable on another. This last feature, of course, is all important in predicting future values.Regression analysis is based upon a functional relationship among variables and further, assumes that the relationship is linear. This linearity assumption is required because, for the most part, the theoretical statistical properties of non-linear estimation are not well worked out yet by the mathematicians and econometricians. This presents us with some difficulties in economic analysis because many of our theoretical models are nonlinear. The marginal cost curve, for example, is decidedly nonlinear as is the total cost function, if we are to believe in the effect of specialization of labor and the Law of Diminishing Marginal Product. There are techniques for overcoming some of these difficulties, exponential and logarithmic transformation of the data for example, but at the outset we must recognize that standard ordinary least squares (OLS) regression analysis will always use a linear function to estimate what might be a nonlinear relationship.The general linear regression model can be stated by the equation:\nyi\n=\n\u00ce\u00b20\n+\n\u00ce\u00b21\nX1i\n+\n\u00ce\u00b22\nX2i\n+\n\u00e2\u008b\u00af\n+\n\u00ce\u00b2k\nXki\n+\n\u00ce\u00b5i\nyi=\u00ce\u00b20+\u00ce\u00b21X1i+\u00ce\u00b22X2i+\u00e2\u008b\u00af+\u00ce\u00b2kXki+\u00ce\u00b5iwhere \u00ce\u00b20 is the intercept, \u00ce\u00b2i's are the slope between Y and the appropriate Xi, and \u00ce\u00b5 (pronounced epsilon), is the error term that captures errors in measurement of Y and the effect on Y of any variables missing from the equation that would contribute to \nexplaining variations in Y. This equation is the theoretical population equation and therefore uses Greek letters. The equation we will estimate will have the Roman equivalent symbols. This is parallel to how we kept track of the population \tparameters and sample parameters before. The symbol for the population mean was \u00c2\u00b5 and for the sample mean X\u00e2\u0080\u0093X\u00e2\u0080\u0093 and for the population standard deviation was \u00cf\u0083 and for the sample standard deviation was s. The equation that will be estimated with a sample of data for two independent variables will thus be:  \n\nyi\n=\nb0\n+\nb1\nx1i\n+\nb2\nx2i\n+\nei\nyi=b0+b1x1i+b2x2i+eiAs with our earlier work with probability distributions, this model works only if certain assumptions hold. These are that the Y is normally distributed, the errors are also normally distributed with a mean of zero and a constant standard deviation, and that the error terms are independent of the size of X and independent of each other.Assumptions of the Ordinary Least Squares Regression ModelEach of these assumptions needs a bit more explanation. If one of these assumptions fails to be true, then it will have an effect on the quality of the estimates. Some of the failures of these assumptions can be fixed while others result in estimates that quite simply provide no insight into the questions the model is trying to answer or worse, give biased estimates.\nThe independent variables, xixi , are all measured without error, and are fixed numbers that are independent of the error term. This assumption is saying in effect that Y is deterministic, the result of a fixed component \u00e2\u0080\u009cX\u00e2\u0080\u009d and a random error component \u00e2\u0080\u009c\u00cf\u00b5.\u00e2\u0080\u009d\nThe error term is a random variable with a mean of zero and a constant variance. The meaning of this is that the variances of the independent variables are independent of the value of the variable. Consider the relationship between personal income and the quantity of a good purchased as an example of a case where the variance is dependent upon the value of the independent variable, income. It is plausible that as income increases the variation around the amount purchased will also increase simply because of the flexibility provided with higher levels of income. The assumption is for constant variance with respect to the magnitude of the independent variable called homoscedasticity. If the assumption fails, then it is called heteroscedasticity. Figure 13.6 shows the case of homoscedasticity where all three distributions have the same variance around the predicted value of Y regardless of the magnitude of X.\nWhile the independent variables are all fixed values they are from a probability distribution that is normally distributed. This can be seen in Figure 13.6 by the shape of the distributions placed on the predicted line at the expected value of the relevant value of Y.\nThe independent variables are independent of Y, but are also assumed to be independent of the other X variables. The model is designed to estimate the effects of independent variables on some dependent variable in accordance with a proposed theory. The case where some or more of the independent variables are correlated is not unusual. There may be no cause and effect relationship among the independent variables, but nevertheless they move together. Take the case of a simple supply curve where quantity supplied is theoretically related to the price of the product and the prices of inputs. There may be multiple inputs that may over time move together from general inflationary pressure. The input prices will therefore violate this assumption of regression analysis. This condition is called multicollinearity, which will be taken up in detail later.\nThe error terms are uncorrelated with each other. This situation arises from an effect on one error term from another error term. While not exclusively a time series problem, it is here that we most often see this case. An X variable in time period one has an effect on the Y variable, but this effect then has an effect in the next time period. This effect gives rise to a relationship among the error terms. This case is called autocorrelation, \u00e2\u0080\u009cself-correlated.\u00e2\u0080\u009d The error terms are now not independent of each other, but rather have their own effect on subsequent error terms.\nFigure 13.6 shows the case where the assumptions of the regression model are being satisfied. The estimated line is y^=a+bx.y^=a+bx. Three values of X are shown. A normal distribution is placed at each point where X equals the estimated line and the associated error at each value of Y. Notice that the three distributions are normally distributed around the point on the line, and further, the variation, variance, around the predicted value is constant indicating homoscedasticity from assumption 2. Figure 13.6 does not show all the assumptions of the regression model, but it helps visualize these important ones.\nFigure \n13.6\n\n\nFigure \n13.7\n\nThis is the general form that is most often called the multiple regression model.  So-called \"simple\" regression analysis has only one independent (right-hand) variable rather than many independent variables. Simple regression is just a special case of multiple regression. There is some value in beginning with simple regression: it is easy to graph in two dimensions, difficult to graph in three dimensions, and impossible to graph in more than three dimensions. Consequently, our graphs will be for the simple regression case. Figure 13.7 presents the regression problem in the form of a scatter plot graph of the data set where it is hypothesized that Y is dependent upon the single independent variable X.A basic relationship from Macroeconomic Principles is the consumption function. This theoretical relationship states that as a person's income rises, their consumption rises, but by a smaller amount than the rise in income. If Y is consumption and X is income in the equation below Figure 13.7, the regression problem is, first, to establish that this relationship exists, and second, to determine the impact of a change in income on a person's consumption. The parameter \u00ce\u00b21 was called the Marginal Propensity to Consume in Macroeconomics Principles.Each \"dot\" in Figure 13.7 represents the consumption and income of different individuals at some point in time. This was called cross-section data earlier; observations on variables at one point in time across different people or other units of measurement. This analysis is often done with time series data, which would be the consumption and income of one individual or country at different points in time. For macroeconomic problems it is common to use times series aggregated data for a whole country. For this particular theoretical concept these data are readily available in the annual report of the President\u00e2\u0080\u0099s Council of Economic Advisors.The regression problem comes down to determining which straight line would best represent the data in Figure 13.8. Regression analysis is sometimes called \"least squares\" analysis because the method of determining which line best \"fits\" the data is to minimize the sum of the squared residuals of a line put through the data. \nFigure \n13.8\n \n\n\nPopulation Equation: C = \u00ce\u00b20 + \u00ce\u00b21 Income + \u00ce\u00b5\n\n\nEstimated Equation:   C = b0 + b1 Income + e\n\n\nThis figure shows the assumed relationship between consumption and income from macroeconomic theory. Here the data are plotted as a scatter plot and an estimated straight line has been drawn. From this graph we can see an error term, e1. Each data point also has an error term. Again, the error term is put into the equation to capture effects on consumption that are not caused by income changes. Such other effects might be a person\u00e2\u0080\u0099s savings or wealth, or periods of unemployment. We will see how by minimizing the sum of these errors we can get an estimate for the slope and intercept of this line.Consider the graph below. The notation has returned to that for the more general model rather than the specific case of the Macroeconomic consumption function in our example.   \nFigure \n13.9\n\nThe \u00c5\u00b7 is read \"y hat\" and is the estimated value of y. (In Figure 13.8 C^C^ represents the estimated value of consumption because it is on the estimated line.) It is the value of y obtained using the regression line. \u00c5\u00b7 is not generally equal to y from the data.The term y0-\u00c5\u00b70=e0y0-\u00c5\u00b70=e0 is called the \"error\" or residual. It is not an error in the sense of a mistake. The error term was put into the estimating equation to capture missing variables and errors in measurement that may have occurred in the dependent variables. The absolute value of a residual measures the vertical distance between the actual value of y and the estimated value of y. In other words, it measures the vertical distance between the actual data point and the predicted point on the line as can be seen on the graph at point X0.If the observed data point lies above the line, the residual is positive, and the line underestimates the actual data value for y.If the observed data point lies below the line, the residual is negative, and the line overestimates that actual data value for y.In the graph, y0-\u00c5\u00b70=e0y0-\u00c5\u00b70=e0 is the residual for the point shown. Here the point lies above the line and the residual is positive.\nFor each data point the residuals, or errors, are calculated yi \u00e2\u0080\u0093 \u00c5\u00b7i = ei for i = 1, 2, 3, ..., n where n is the sample size. Each |e| is a vertical distance.\n\nThe sum of the errors squared is the term obviously called Sum of Squared Errors (SSE).Using calculus, you can determine the straight line that has the parameter values of b0 and b1 that minimizes the SSE. When you make the SSE a minimum, you have determined the points that are on the line of best fit. It turns out that the line of best fit has the equation:\n\u00c5\u00b7\n=\nb0\n+\nb1\nx\n\u00c5\u00b7=b0+b1x\nwhere \n\nb0\n=\ny\u00e2\u0080\u0093\n\u00e2\u0088\u0092\nb1\nx\u00e2\u0080\u0093\nb0=y\u00e2\u0080\u0093\u00e2\u0088\u0092b1x\u00e2\u0080\u0093\n\nand\n\n\nb1\n=\n\n\n\u00ce\u00a3(x\u00e2\u0088\u0092x\u00e2\u0080\u0093)\n(y\u00e2\u0088\u0092y\u00e2\u0080\u0093)\n\n\n\u00ce\u00a3(x\u00e2\u0088\u0092x\u00e2\u0080\u0093)2\n\n\n=\ncov(x,y)sx2\nb1=\n\n\u00ce\u00a3(x\u00e2\u0088\u0092x\u00e2\u0080\u0093)\n(y\u00e2\u0088\u0092y\u00e2\u0080\u0093)\n\n\n\u00ce\u00a3(x\u00e2\u0088\u0092x\u00e2\u0080\u0093)2\n\n=cov(x,y)sx2The sample means of the x values and the y values are x\u00e2\u0080\u0093x\u00e2\u0080\u0093 and y\u00e2\u0080\u0093y\u00e2\u0080\u0093, respectively. The best fit line always passes through the\npoint (x\u00e2\u0080\u0093x\u00e2\u0080\u0093, y\u00e2\u0080\u0093y\u00e2\u0080\u0093) called the points of means.\nThe slope b can also be written as:\n\nb\n1\n\n=\n\nr\ny,x\n\n(\n\n\n\ns\ny\n\n\n\n\ns\nx\n\n\n\n)\n\nb\n1\n=\nr\ny,x\n(\n\n\ns\ny\n\n\n\n\ns\nx\n\n\n)where sy = the standard deviation of the y values and sx = the standard deviation\nof the x values and r is the correlation coefficient between x and y.\nThese equations are called the Normal Equations and come from another very important mathematical finding called the Gauss-Markov Theorem without which we could not do regression analysis. The Gauss-Markov Theorem tells us that the estimates we get from using the ordinary least squares (OLS) regression method will result in estimates that have some very important properties. In the Gauss-Markov Theorem it was proved that a least squares line is BLUE, which is, Best, Linear, Unbiased, Estimator. Best is the statistical property that an estimator is the one with the minimum variance. Linear refers to the property of the type of line being estimated. An unbiased estimator is one whose estimating function has an expected mean equal to the mean of the population. (You will remember that the expected value of \u00c2\u00b5x\u00e2\u0080\u0093\u00c2\u00b5x\u00e2\u0080\u0093 was equal to the population mean \u00c2\u00b5 in accordance with the Central Limit Theorem. This is exactly the same concept here). Both Gauss and Markov were giants in the field of mathematics, and Gauss in physics too, in the 18th century and early 19th century. They barely overlapped chronologically and never in geography, but Markov\u00e2\u0080\u0099s work on this theorem was based extensively on the earlier work of Carl Gauss. The extensive applied value of this theorem had to wait until the middle of this last century. Using the OLS method we can now find the estimate of the error variance which is the variance of the squared errors, e2. This is sometimes called the standard error of the estimate. (Grammatically this is probably best said as the estimate of the error\u00e2\u0080\u0099s variance) The formula for the estimate of the error variance is:\n\ns\ne\n2\n\n=\n\n\n\u00ce\u00a3\n\n\n(\n\ny\ni\n\n\u00e2\u0088\u0092\n\n\u00c5\u00b7\ni\n\n)\n\n2\n\n\n\nn\n\u00e2\u0088\u0092\nk\n\n\n=\n\n\n\u00ce\u00a3\n\n\n\ne\ni\n\n\n2\n\n\n\nn\n\u00e2\u0088\u0092\nk\n\n\n\ns\ne\n2\n=\n\n\u00ce\u00a3\n\n\n(\n\ny\ni\n\n\u00e2\u0088\u0092\n\n\u00c5\u00b7\ni\n\n)\n\n2\n\n\n\nn\n\u00e2\u0088\u0092\nk\n\n=\n\n\u00ce\u00a3\n\n\n\ne\ni\n\n\n2\n\n\n\nn\n\u00e2\u0088\u0092\nk\n\nwhere \u00c5\u00b7 is the predicted value of y and y is the observed value, and thus the term (yi\u00e2\u0088\u0092\u00c5\u00b7i)2(yi\u00e2\u0088\u0092\u00c5\u00b7i)2  is the squared errors that are to be minimized to find the estimates of the regression line parameters. This is really just the variance of the error terms and follows our regular variance formula. One important note is that here we are dividing by (n-k)(n-k), which is the degrees of freedom. The degrees of freedom of a regression equation will be the number of observations, n, reduced by the number of estimated parameters, which includes the intercept as a parameter. The variance of the errors is fundamental in testing hypotheses for a regression. It tells us just how \u00e2\u0080\u009ctight\u00e2\u0080\u009d the dispersion is about the line. As we will see shortly, the greater the dispersion about the line, meaning the larger the variance of the errors, the less probable that the hypothesized independent variable will be found to have a significant effect on the dependent variable. In short, the theory being tested will more likely fail if the variance of the error term is high. Upon reflection this should not be a surprise. As we tested hypotheses about a mean we observed that large variances reduced the calculated test statistic and thus it failed to reach the tail of the distribution. In those cases, the null hypotheses could not be rejected. If we cannot reject the null hypothesis in a regression problem, we must conclude that the hypothesized independent variable has no effect on the dependent variable. A way to visualize this concept is to draw two scatter plots of x and y data along a predetermined line. The first will have little variance of the errors, meaning that all the data points will move close to the line. Now do the same except the data points will have a large estimate of the error variance, meaning that the data points are scattered widely along the line. Clearly the confidence about a relationship between x and y is effected by this difference between the estimate of the error variance.  \tTesting the Parameters of the LineThe whole goal of the regression analysis was to test the hypothesis that the dependent variable, Y, was in fact dependent upon the values of the independent variables as asserted by some foundation theory, such as the consumption function example. Looking at the estimated equation under Figure 13.8, we see that this amounts to determining the values of b0 and b1. Notice that again we are using the convention of Greek letters for the population parameters and Roman letters for their estimates. \nThe regression analysis output provided by the computer software will produce an estimate of b0 and b1, and any other b's for other independent variables that were included in the estimated equation. The issue is how good are these estimates? In order to test a hypothesis concerning any estimate, we have found that we need to know the underlying sampling distribution. It should come as no surprise at his stage in the course that the answer is going to be the normal distribution. This can be seen by remembering the assumption that the error term in the population, \u00ce\u00b5, is normally distributed. If the error term is normally distributed and the variance of the estimates of the equation parameters, b0 and b1, are determined by the variance of the error term, it follows that the variances of the parameter estimates are also normally distributed. And indeed this is just the case. We can see this by the creation of the test statistic for the test of hypothesis for the slope parameter, \u00ce\u00b21 in our consumption function equation. To test whether or not Y does indeed depend upon X, or in our example, that consumption depends upon income, we need only test the hypothesis that \u00ce\u00b21 equals zero. This hypothesis would be stated formally as:H0:\u00ce\u00b21=0\nH0:\u00ce\u00b21=0\nHa:\u00ce\u00b21\u00e2\u0089\u00a00Ha:\u00ce\u00b21\u00e2\u0089\u00a00If we cannot reject the null hypothesis, we must conclude that our theory has no validity. If we cannot reject the null hypothesis that \u00ce\u00b21 = 0 then b1, the coefficient of Income, is zero and zero times anything is zero. Therefore the effect of Income on Consumption is zero. There is no relationship as our theory had suggested. Notice that we have set up the presumption, the null hypothesis, as \"no relationship\". This puts the burden of proof on the alternative hypothesis. In other words, if we are to validate our claim of finding a relationship, we must do so with a level of significance greater than 90, 95, or 99 percent. The status quo is ignorance, no relationship exists, and to be able to make the claim that we have actually added to our body of knowledge we must do so with significant probability of being correct. John Maynard Keynes got it right and thus was born Keynesian economics starting with this basic concept in 1936.\nThe test statistic for this test comes directly from our old friend the standardizing formula:\n\nt\nc\n\n=\n\n\n\nb\n1\n\n\u00e2\u0088\u0092\n\n\u00ce\u00b2\n1\n\n\n\n\nS\n\n\nb\n1\n\n\n\n\n\n\nt\nc\n=\n\n\nb\n1\n\n\u00e2\u0088\u0092\n\n\u00ce\u00b2\n1\n\n\n\n\nS\n\n\nb\n1\n\n\n\n\nwhere b1 is the estimated value of the slope of the regression line, \u00ce\u00b21 is the hypothesized value of beta, in this case zero, and  \nS\n\n\nb\n1\n\n\n\nS\n\n\nb\n1\n\n\n is the standard deviation of the estimate of b1. In this case we are asking how many standard deviations is the estimated slope away from the hypothesized slope. This is exactly the same question we asked before with respect to a hypothesis about a mean: how many standard deviations is the estimated mean, the sample mean, from the hypothesized mean?The test statistic is written as a student's t-distribution, but if the sample size is larger enough so that the degrees of freedom are greater than 30 we may again use the normal distribution. To see why we can use the student's t or normal distribution we have only to look at  \nS\n\n\nb\n1\n\n\n\nS\n\n\nb\n1\n\n\n,the formula for the standard deviation of the estimate of b1:\n\n\nS\n\n\nb\n1\n\n\n\n=\n\n\n\nS\ne\n2\n\n\n\n\n\n\n(\n\nx\ni\n\n\u00e2\u0088\u0092\n\nx\n\u00e2\u0080\u0093\n\n)\n\n2\n\n\n\n\n\nS\n\n\nb\n1\n\n\n=\n\n\nS\ne\n2\n\n\n\n\n\n\n(\n\nx\ni\n\n\u00e2\u0088\u0092\n\nx\n\u00e2\u0080\u0093\n\n)\n\n2\n\n\n\n\nor\n\nS\n\n\nb\n1\n\n\n\n=\n\n\n\nS\ne\n2\n\n\n\n(\nn\n\u00e2\u0088\u0092\n1\n)\n\nS\nx\n2\n\n\n\n\nS\n\n\nb\n1\n\n\n=\n\n\nS\ne\n2\n\n\n\n(\nn\n\u00e2\u0088\u0092\n1\n)\n\nS\nx\n2\n\n\n\nWhere Se is the estimate of the error variance and S2x is the variance of x values of the coefficient of the independent variable being tested. We see that Se, the estimate of the error variance, is part of the computation. Because the estimate of the error variance is based on the assumption of normality of the error terms, we can conclude that the sampling distribution of the b's, the coefficients of our hypothesized regression line, are also normally distributed.One last note concerns the degrees of freedom of the test statistic, \u00ce\u00bd=n-k. Previously we subtracted 1 from the sample size to determine the degrees of freedom in a student's t problem.  Here we must subtract one degree of freedom for each parameter estimated in the equation. For the example of the consumption function we lose 2 degrees of freedom, one for b0b0, the intercept, and one for b1, the slope of the consumption function. The degrees of freedom would be n - k - 1, where k is the number of independent variables and the extra one is lost because of the intercept. If we were estimating an equation with three independent variables, we would lose 4 degrees of freedom: three for the independent variables, k, and one more for the intercept.The decision rule for acceptance or rejection of the null hypothesis follows exactly the same form as in all our previous test of hypothesis. Namely, if the calculated value of t (or Z) falls into the tails of the distribution, where the tails are defined by \u00ce\u00b1 ,the required significance level in the test, we cannot accept the null hypothesis. If on the other hand, the calculated value of the test statistic is within the critical region, we cannot reject the null hypothesis.  If we conclude that we cannot accept the null hypothesis, we are able to state with (1-\u00ce\u00b1)(1-\u00ce\u00b1) level of confidence that the slope of the line is given by b1. This is an extremely important conclusion. Regression analysis not only allows us to test if a cause and effect relationship exists, we can also determine the magnitude of that relationship, if one is found to exist. It is this feature of regression analysis that makes it so valuable. If models can be developed that have statistical validity, we are then able to simulate the effects of changes in variables that may be under our control with some degree of probability , of course. For example, if advertising is demonstrated to effect sales, we can determine the effects of changing the advertising budget and decide if the increased sales are worth the added expense.Multicollinearity\nOur discussion earlier indicated that like all statistical models, the OLS regression model has important assumptions attached. Each assumption, if violated, has an effect on the ability of the model to provide useful and meaningful estimates. The Gauss-Markov Theorem has assured us that the OLS estimates are unbiased and minimum variance, but this is true only under the assumptions of the model. Here we will look at the effects on OLS estimates if the independent variables are correlated. The other assumptions and the methods to mitigate the difficulties they pose if they are found to be violated are examined in Econometrics courses. We take up multicollinearity because it is so often prevalent in Economic models and it often leads to frustrating results.\n\nThe OLS model assumes that all the independent variables are independent of each other. This assumption is easy to test for a particular sample of data with simple correlation coefficients. Correlation, like much in statistics, is a matter of degree: a little is not good, and a lot is terrible.\nThe goal of the regression technique is to tease out the independent impacts of each of a set of independent variables on some hypothesized dependent variable. If two 2 independent variables are interrelated, that is, correlated, then we cannot isolate the effects on Y of one from the other. In an extreme case where x1x1 is a linear combination of x2x2, correlation equal to one, both variables move in identical ways with Y. In this case it is impossible to determine the variable that is the true cause of the effect on Y. (If the two variables were actually perfectly correlated, then mathematically no regression results could actually be calculated.)The normal equations for the coefficients show the effects of multicollinearity on the\n\ncoefficients.\nb1=\n\n\nsy(rx1y\n-\nrx1x2\nrx2y)\n\n\nsx1\n(\n1-\nr\nx1x2\n2)\n\nb1=\n\nsy(rx1y\n-\nrx1x2\nrx2y)\n\n\nsx1\n(\n1-\nr\nx1x2\n2)\n\nb2=\n\n\nsy(rx2y\n-\nrx1x2\nrx1y)\n\n\nsx2\n(\n1-\nr\nx1x2\n2)\n\nb2=\n\nsy(rx2y\n-\nrx1x2\nrx1y)\n\n\nsx2\n(\n1-\nr\nx1x2\n2)\n\nb0=\ny--b1x-1-b2x-2\nb0=y--b1x-1-b2x-2The correlation between x1x1 and x2x2, rx1x22\nrx1x22, appears in the denominator of both the estimating\n\nformula for b1b1 and b2b2. If the assumption of independence holds, then this term is zero.\n\nThis indicates that there is no effect of the correlation on the coefficient. On the\n\nother hand, as the correlation between the two independent variables increases the\n\ndenominator decreases, and thus the estimate of the coefficient increases. The\n\ncorrelation has the same effect on both of the coefficients of these two variables. In\n\nessence, each variable is \u00e2\u0080\u009ctaking\u00e2\u0080\u009d part of the effect on Y that should be attributed to\n\nthe collinear variable. This results in biased estimates.Multicollinearity has a further deleterious impact on the OLS estimates. The\n\ncorrelation between the two independent variables also shows up in the formulas\n\nfor the estimate of the variance for the coefficients.\n\nsb12\n=\n\n\nse2\n\n(n-1)sx12(1-\nrx1x22)\n\n\nsb12=\n\nse2\n\n(n-1)sx12(1-\nrx1x22)\n\n\nsb22\n=\n\n\nse2\n\n(n-1)sx22(1-\nrx1x22)\n\n\nsb22=\n\nse2\n\n(n-1)sx22(1-\nrx1x22)\n\nHere again we see the correlation between x1x1 and x2x2 in the denominator of the estimates of the variance for the coefficients for both variables. If the correlation is zero as assumed in the regression model, then the formula collapses to the familiar ratio of the variance of the errors to the variance of the relevant independent variable. If however the two independent variables are correlated, then the variance of the estimate of the coefficient increases. This results in a smaller t-value for the test of hypothesis of the coefficient. In short, multicollinearity results in failing to reject the null hypothesis that the X variable has no impact on Y when in fact X does have a statistically significant impact on Y. Said another way, the large standard errors of the estimated coefficient created by multicollinearity suggest statistical insignificance even when the hypothesized relationship is strong.How Good is the Equation?In the last section we concerned ourselves with testing the hypothesis that the dependent variable did indeed depend upon the hypothesized independent variable or variables. It may be that we find an independent variable that has some effect on the dependent variable, but it may not be the only one, and it may not even be the most important one. Remember that the error term was placed in the model to capture the effects of any missing independent variables. It follows that the error term may be used to give a measure of the \"goodness of fit\" of the equation taken as a whole in explaining the variation of the dependent variable, Y.\nThe multiple correlation coefficient, also called the coefficient of multiple determination or the coefficient of determination, is given by the formula:\nR2\n=\n\nSSR\nSST\n\nR2=\nSSR\nSST\nwhere SSR is the regression sum of squares, the squared deviation of the predicted value of y from the mean value of y(\u00c5\u00b7\u00e2\u0088\u0092y\u00e2\u0080\u0093)(\u00c5\u00b7\u00e2\u0088\u0092y\u00e2\u0080\u0093),  and SST is the total sum of squares which is the total squared deviation of the dependent variable, y, from its mean value, including the error term, SSE, the sum of squared errors. Figure 13.10 shows how the total deviation of the dependent variable, y, is partitioned into these two pieces. \nFigure \n13.10\n\nFigure 13.10 shows the estimated regression line and a single observation, x1. Regression analysis tries to explain the variation of the data about the mean value of the dependent variable, y. The question is, why do the observations of y vary from the average level of y? The value of y at observation x1 varies from the mean of y by the difference (yi\u00e2\u0088\u0092y\u00e2\u0080\u0093yi\u00e2\u0088\u0092y\u00e2\u0080\u0093). The sum of these differences squared is SST, the sum of squares total. The actual value of y at x1 deviates from the estimated value, \u00c5\u00b7, by the difference between the estimated value and the actual value, (yi\u00e2\u0088\u0092\u00c5\u00b7yi\u00e2\u0088\u0092\u00c5\u00b7). We recall that this is the error term, e, and the sum of these errors is SSE, sum of squared errors. The deviation of the predicted value of y, \u00c5\u00b7, from the mean value of y is (\u00c5\u00b7\u00e2\u0088\u0092y\u00e2\u0080\u0093\u00c5\u00b7\u00e2\u0088\u0092y\u00e2\u0080\u0093) and is the SSR, sum of squares regression. It is called \u00e2\u0080\u009cregression\u00e2\u0080\u009d because it is the deviation explained by the regression. (Sometimes the SSR is called SSM for sum of squares mean because it measures the deviation from the mean value of the dependent variable, y, as shown on the graph.).Because the SST = SSR + SSE we see that the multiple correlation coefficient is the percent of the variance, or deviation in y from its mean value, that is explained by the equation when taken as a whole. R2 will vary between zero and 1, with zero indicating that none of the variation in y was explained by the equation and a value of 1 indicating that 100% of the variation in y was explained by the equation. For time series studies expect a high R2 and for cross-section data expect low R2. While a high R2 is desirable, remember that it is the tests of the hypothesis concerning the existence of a relationship between a set of independent variables and a particular dependent variable that was the motivating factor in using the regression model. It is validating a cause and effect relationship developed by some theory that is the true reason that we chose the regression analysis. Increasing the number of independent variables will have the effect of increasing R2. To account for this effect the proper measure of the coefficient of determination is the R\u00e2\u0080\u00932R\u00e2\u0080\u00932, adjusted for degrees of freedom, to keep down mindless addition of independent variables. There is no statistical test for the R2 and thus little can be said about the model using R2 with our characteristic confidence level. Two models that have the same size of SSE, that is sum of squared errors, may have very different R2  if the competing models have different SST, total sum of squared deviations. The goodness of fit of the two models is the same; they both have the same sum of squares unexplained, errors squared, but because of the larger total sum of squares on one of the models the R2 differs. Again, the real value of regression as a tool is to examine hypotheses developed from a model that predicts certain relationships among the variables. These are tests of hypotheses on the coefficients of the model and not a game of maximizing R2.Another way to test the general quality of the overall model is to test the coefficients\n\nas a group rather than independently. Because this is multiple regression (more\n\nthan one X), we use the F-test to determine if our coefficients collectively affect Y.\n\nThe hypothesis is: \nHo:\u00ce\u00b21=\u00ce\u00b22=\u00e2\u0080\u00a6=\u00ce\u00b2i=0Ho:\u00ce\u00b21=\u00ce\u00b22=\u00e2\u0080\u00a6=\u00ce\u00b2i=0Ha:Ha: \"at least one of the \u00ce\u00b2i is not equal to 0\"If the null hypothesis cannot be rejected, then we conclude that none of the\n\nindependent variables contribute to explaining the variation in Y. Reviewing Figure 13.10 we see that SSR, the explained sum of squares, is a measure of just how much of\n\nthe variation in Y is explained by all the variables in the model. SSE, the sum of the\n\nerrors squared, measures just how much is unexplained. It follows that the ratio of\n\nthese two can provide us with a statistical test of the model as a whole.\n\nRemembering that the F distribution is a ratio of Chi squared distributions and that\n\nvariances are distributed according to Chi Squared, and the sum of squared errors\n\nand the sum of squares are both variances, we have the test statistic for this\n\nhypothesis as:\nFc=\n\n\n(\nSSR\nk\n\n)\n\n\n(\nSSE\nn\u00e2\u0088\u0092k\u00e2\u0088\u00921\n\n)\n\n\nFc=\n\n(\nSSR\nk\n\n)\n\n\n(\nSSE\nn\u00e2\u0088\u0092k\u00e2\u0088\u00921\n\n)\n\nwhere n is the number of observations and k is the number of independent variables. It can be shown that this is equivalent to:\nFc\n=\n\n\nn\u00e2\u0088\u0092k\u00e2\u0088\u00921\n\nk\n\n\u00c2\u00b7\n\nR2\n1\u00e2\u0088\u0092R2\n\nFc=\n\nn\u00e2\u0088\u0092k\u00e2\u0088\u00921\n\nk\n\u00c2\u00b7\nR2\n1\u00e2\u0088\u0092R2\nFigure 13.10 where R2 is the coefficient of determination which is also a\n\nmeasure of the \u00e2\u0080\u009cgoodness\u00e2\u0080\u009d of the model.As with all our tests of hypothesis, we reach a conclusion by comparing the\n\ncalculated F statistic with the critical value given our desired level of confidence. If\n\nthe calculated test statistic, an F statistic in this case, is in the tail of the distribution,\n\nthen we cannot accept the null hypothesis. By not being able to accept the null\n\nhypotheses we conclude that this specification of this model has validity, because at\n\nleast one of the estimated coefficients is significantly different from zero.An alternative way to reach this conclusion is to use the p-value comparison rule.\n\nThe p-value is the area in the tail, given the calculated F statistic. In essence, the\n\ncomputer is finding the F value in the table for us. The computer regression output\n\nfor the calculated F statistic is typically found in the ANOVA table section labeled\n\n\u00e2\u0080\u009csignificance F\". How to read the output of an Excel regression is presented below. This is the probability of NOT accepting a false null hypothesis. If\n\nthis probability is less than our pre-determined alpha error, then the conclusion is\n\nthat we cannot accept the null hypothesis.Dummy VariablesThus far the analysis of the OLS regression technique assumed that the independent variables in the models tested were continuous random variables. There are, however, no restrictions in the regression model against independent variables that are binary. This opens the regression model for testing hypotheses concerning categorical variables such as gender, race, region of the country, before a certain data, after a certain date and innumerable others. These categorical variables take on only two values, 1 and 0, success or failure, from the binomial probability distribution.\nThe form of the equation becomes:\n\u00c5\u00b7=b0+b2x2+b1x1\u00c5\u00b7=b0+b2x2+b1x1\nFigure \n13.11\n\nwhere x2=0,1x2=0,1.  X2 is the dummy variable and X1 is some continuous random variable. The constant, b0, is the y-intercept, the value where the line crosses the y-axis. When the value of X2 = 0, the estimated line crosses at b0. When the value of X2 = 1 then the estimated line crosses at b0 + b2. In effect the dummy variable causes the estimated line to shift either up or down by the size of the effect of the characteristic captured by the dummy variable. Note that this is a simple parallel shift and does not affect the impact of the other independent variable; X1.This variable is a continuous random variable and predicts different values of y at different values of X1 holding constant the condition of the dummy variable. An example of the use of a dummy variable is the work estimating the impact of gender on salaries. There is a full body of literature on this topic and dummy variables are used extensively. For this example the salaries of elementary and secondary school teachers for a particular state is examined. Using a homogeneous job category, school teachers, and for a single state reduces many of the variations that naturally effect salaries such as differential physical risk, cost of living in a particular state, and other working conditions. The estimating equation in its simplest form specifies salary as a function of various teacher characteristic that economic theory would suggest could affect salary. These would include education level as a measure of potential productivity, age and/or experience to capture on-the-job training, again as a measure of productivity. Because the data are for school teachers employed in a public school districts rather than workers in a for-profit company, the school district\u00e2\u0080\u0099s average revenue per average daily student attendance is included as a measure of ability to pay.  The results of the regression analysis using data on 24,916 school teachers are presented below. \n\nVariable\nRegression Coefficients (b) \nStandard Errors of the estimates for teacher's earnings function (sb)\n\n\n\nIntercept\n4269.9\n\n\n\nGender (male = 1)\n632.38\n13.39\n\n\nTotal Years of Experience\n52.32\n1.10\n\n\nYears of Experience in Current District\n29.97\n1.52\n\n\nEducation\n629.33\n13.16\n\n\nTotal Revenue per ADA\n90.24\n3.76\n\n\nR\u00e2\u0080\u00932R\u00e2\u0080\u00932\n.725\n\n\n\nn\n24,916\n\n\n\nTable \n13.1\n \nEarnings Estimate for Elementary and Secondary School Teachers\n \n\nThe coefficients for all the independent variables are significantly different from zero as indicated by the standard errors. Dividing the standard errors of each coefficient results in a t-value greater than 1.96 which is the required level for 95% significance. The binary variable, our dummy variable of interest in this analysis, is gender where male is given a value of 1 and female given a value of 0. The coefficient is significantly different from zero with a dramatic t-statistic of 47 standard deviations. We thus cannot accept the null hypothesis that the coefficient is equal to zero. Therefore we conclude that there is a premium paid male teachers of $632 after holding constant experience, education and the wealth of the school district in which the teacher is employed. It is important to note that these data are from some time ago and the $632 represents a six percent salary premium at that time. A graph of this example of dummy variables is presented below. \nFigure \n13.12\n\nIn two dimensions, salary is the dependent variable on the vertical axis and total years of experience was chosen for the continuous independent variable on horizontal axis. Any of the other independent variables could have been chosen to illustrate the effect of the dummy variable. The relationship between total years of experience has a slope of $52.32 per year of experience and the estimated line has an intercept of $4,269 if the gender variable is equal to zero, for female. If the gender variable is equal to 1, for male, the coefficient for the gender variable is added to the intercept and thus the relationship between total years of experience and salary is shifted upward parallel as indicated on the graph. Also marked on the graph are various points for reference. A female school teacher with 10 years of experience receives a salary of $4,792 on the basis of her experience only, but this is still $109 less than a male teacher with zero years of experience.A more complex interaction between a dummy variable and the dependent variable can also be estimated. It may be that the dummy variable has more than a simple shift effect on the dependent variable, but also interacts with one or more of the other continuous independent variables. While not tested in the example above, it could be hypothesized that the impact of gender on salary was not a one-time shift, but impacted the value of additional years of experience on salary also. That is, female school teacher\u00e2\u0080\u0099s salaries were discounted at the start, and further did not grow at the same rate from the effect of experience as for male school teachers. This would show up as a different slope for the relationship between total years of experience for males than for females. If this is so then females school teachers would not just start behind their male colleagues (as measured by the shift in the estimated regression line), but would fall further and further behind as time and experienced increased. The graph below shows how this hypothesis can be tested with the use of dummy variables and an interaction variable. \nFigure \n13.13\n\nThe estimating equation shows how the slope of X1, the continuous random variable experience, contains two parts, b1 and b3. This occurs because of the new variable X2 X1, called the interaction variable, was created to allow for an effect on the slope of X1 from changes in X2, the binary dummy variable.  Note that when the dummy variable, X2 = 0 the interaction variable has a value of 0, but when X2 = 1 the interaction variable has a value of X1. The coefficient b3 is an estimate of the difference in the coefficient of X1 when X2 = 1 compared to when X2 = 0. In the example of teacher\u00e2\u0080\u0099s salaries, if there is a premium paid to male teachers that affects the rate of increase in salaries from experience, then the rate at which male teachers\u00e2\u0080\u0099 salaries rises would be b1 + b3 and the rate at which female teachers\u00e2\u0080\u0099 salaries rise would be simply b1. This hypothesis can be tested with the hypothesis:\n\nH\n0\n\n:\n\n\u00ce\u00b2\n3\n\n=\n0\n|\n\n\u00ce\u00b2\n1\n\n=\n0\n,\n\n\u00ce\u00b2\n2\n\n=\n0\n\nH\n0\n:\n\u00ce\u00b2\n3\n=0|\n\u00ce\u00b2\n1\n=0,\n\u00ce\u00b2\n2\n=0\n\n\nH\na\n\n:\n\n\u00ce\u00b2\n3\n\n\u00e2\u0089\u00a0\n0\n|\n\n\u00ce\u00b2\n1\n\n\u00e2\u0089\u00a0\n0\n,\n\n\u00ce\u00b2\n2\n\n\u00e2\u0089\u00a0\n0\n\nH\na\n:\n\u00ce\u00b2\n3\n\u00e2\u0089\u00a00|\n\u00ce\u00b2\n1\n\u00e2\u0089\u00a00,\n\u00ce\u00b2\n2\n\u00e2\u0089\u00a00\nThis is a t-test using the test statistic for the parameter \u00ce\u00b23. If we cannot accept the null hypothesis that \u00ce\u00b23=0 we conclude there is a difference between the rate of increase for the group for whom the value of the binary variable is set to 1, males in this example. This estimating equation can be combined with our earlier one that tested only a parallel shift in the estimated line. The earnings/experience functions in Figure 13.13 are drawn for this case with a shift in the earnings function and a difference in the slope of the function with respect to total years of experience.\nExample \n13.5\n \n\n\nA random sample of 11 statistics students produced the following data, where x is the third exam score out of 80, and y is the final exam score out of 200. Can you predict the final exam score of a randomly selected student if you know the third exam score?\n\nx (third exam score)\ny (final exam score)\n\n\n\n65\n175\n\n\n67\n133\n\n\n71\n185\n\n\n71\n163\n\n\n66\n126\n\n\n75\n198\n\n\n67\n153\n\n\n70\n163\n\n\n71\n159\n\n\n69\n151\n\n\n69\n159\n\n\nTable \n13.2\n \n \nTable showing the scores on the final exam based on scores from the third exam.\n\n\n\nFigure \n13.14\n \nScatter plot showing the scores on the final exam based on scores from the third exam.\n\n\n"}, {"chapter": "13.5", "title": "Interpretation of Regression Coefficients: Elasticity and Logarithmic Transformation", "mathml": "<math display=\"block\"><semantics><mrow><mtext>Price elasticity:</mtext><mspace width=\"0.1em\"></mspace><msub><mtext>\u00ce\u00b7</mtext><mtext>p</mtext></msub><mo>=</mo><mfrac><mrow><mo>(</mo><mtext>%\u00e2\u0088\u0086Q</mtext><mo>)</mo></mrow><mrow><mo>(</mo><mtext>%\u00e2\u0088\u0086P</mtext><mo>)</mo></mrow></mfrac></mrow><annotation-xml encoding=\"MathML-Content\"><mtext>Price elasticity:</mtext><mspace width=\"0.1em\"></mspace><msub><mtext>\u00ce\u00b7</mtext><mtext>p</mtext></msub><mo>=</mo><mfrac><mrow><mo>(</mo><mtext>%\u00e2\u0088\u0086Q</mtext><mo>)</mo></mrow><mrow><mo>(</mo><mtext>%\u00e2\u0088\u0086P</mtext><mo>)</mo></mrow></mfrac></annotation-xml></semantics></math> </div", "content": "\nAs we have seen, the coefficient of an equation estimated using OLS regression analysis provides an estimate of the slope of a straight line that is assumed be the relationship between the dependent variable and at least one independent variable. From the calculus, the slope of the line is the first derivative and tells us the magnitude of the impact of a one unit change in the XX variable upon the value of the YY variable measured in the units of the YY variable.  As we saw in the case of dummy variables, this can show up as a parallel shift in the estimated line or even a change in the slope of the line through an interactive variable. Here we wish to explore the concept of elasticity and how we can use a regression analysis to estimate the various elasticities in which economists have an interest.The concept of elasticity is borrowed from engineering and physics where it is used to measure a material\u00e2\u0080\u0099s responsiveness to a force, typically a physical force such as a stretching/pulling force. It is from here that we get the term an \u00e2\u0080\u009celastic\u00e2\u0080\u009d band. In economics, the force in question is some market force such as a change in price or income. Elasticity is measured as a percentage change/response in both engineering applications and in economics. The value of measuring in percentage terms is that the units of measurement do not play a role in the value of the measurement and thus allows direct comparison between elasticities. As an example, if the price of gasoline increased say 50 cents from an initial price of $3.00 and generated a decline in monthly consumption for a consumer from 50 gallons to 48 gallons we calculate the elasticity to be 0.25. The price elasticity is the percentage change in quantity resulting from some percentage change in price. A 16 percent increase in price has generated only a 4 percent decrease in demand: 16% price change \u00e2\u0086\u0092 4% quantity change or .04/.16 = .25.  This is called an inelastic demand meaning a small response to the price change. This comes about because there are few if any real substitutes for gasoline; perhaps public transportation, a bicycle or walking. Technically, of course, the percentage change in demand from a price increase is a decline in demand thus price elasticity is a negative number. The common convention, however, is to talk about elasticity as the absolute value of the number. Some goods have many substitutes: pears for apples for plums, for grapes, etc. etc. The elasticity for such goods is larger than one and are called elastic in demand. Here a small percentage change in price will induce a large percentage change in quantity demanded. The consumer will easily shift the demand to the close substitute.While this discussion has been about price changes, any of the independent variables in a demand equation will have an associated elasticity. Thus, there is an income elasticity that measures the sensitivity of demand to changes in income: not much for the demand for food, but very sensitive for yachts. If the demand equation contains a term for substitute goods, say candy bars in a demand equation for cookies, then the responsiveness of demand for cookies from changes in prices of candy bars can be measured. This is called the cross-price elasticity of demand and to an extent can be thought of as brand loyalty from a marketing view. How responsive is the demand for Coca-Cola to changes in the price of Pepsi?      Now imagine the demand for a product that is very expensive. Again, the measure of elasticity is in percentage terms thus the elasticity can be directly compared to that for gasoline: an elasticity of 0.25 for gasoline conveys the same information as an elasticity of 0.25 for $25,000 car. Both goods are considered by the consumer to have few substitutes and thus have inelastic demand curves, elasticities less than one. The mathematical formulae for various elasticities are:Price elasticity:\u00ce\u00b7p=(%\u00e2\u0088\u0086Q)(%\u00e2\u0088\u0086P)Price elasticity:\u00ce\u00b7p=(%\u00e2\u0088\u0086Q)(%\u00e2\u0088\u0086P) Where \u00ce\u00b7 is the Greek small case letter eta used to designate elasticity. \u00e2\u0088\u0086 is read as \u00e2\u0080\u009cchange\u00e2\u0080\u009d.Income elasticity:\u00ce\u00b7Y=(%\u00e2\u0088\u0086Q)(%\u00e2\u0088\u0086Y)Income elasticity:\u00ce\u00b7Y=(%\u00e2\u0088\u0086Q)(%\u00e2\u0088\u0086Y) Where Y is used as the symbol for income.Cross-Price elasticity:\u00ce\u00b7p1=(%\u00e2\u0088\u0086Q1)(%\u00e2\u0088\u0086P2)Cross-Price elasticity:\u00ce\u00b7p1=(%\u00e2\u0088\u0086Q1)(%\u00e2\u0088\u0086P2) Where P2 is the price of the substitute good.Examining closer the price elasticity we can write the formula as:\u00ce\u00b7p=(%\u00e2\u0088\u0086Q)(%\u00e2\u0088\u0086P)=dQdP(PQ)=b(PQ)\u00ce\u00b7p=(%\u00e2\u0088\u0086Q)(%\u00e2\u0088\u0086P)=dQdP(PQ)=b(PQ) Where bb is the estimated coefficient for price in the OLS regression.The first form of the equation demonstrates the principle that elasticities are measured in percentage terms. Of course, the ordinary least squares coefficients provide an estimate of the impact of a unit change in the independent variable, X, on the dependent variable measured in units of Y. These coefficients are not elasticities, however, and are shown in the second way of writing the formula for elasticity as (dQdP)(dQdP), the derivative of the estimated demand function which is simply the slope of the regression line. Multiplying the slope times PQPQ provides an elasticity measured in percentage terms. Along a straight-line demand curve the percentage change, thus elasticity, changes continuously as the scale changes, while the slope, the estimated regression coefficient, remains constant. Going back to the demand for gasoline. A change in price from $3.00 to $3.50 was a 16 percent increase in price. If the beginning price were $5.00 then the same 50\u00c2\u00a2 increase would be only a 10 percent increase generating a different elasticity. Every straight-line demand curve has a range of elasticities starting at the top left, high prices, with large elasticity numbers, elastic demand, and decreasing as one goes down the demand curve, inelastic demand. In order to provide a meaningful estimate of the elasticity of demand the convention is to estimate the elasticity at the point of means. Remember that all OLS regression lines will go through the point of means. At this point is the greatest weight of the data used to estimate the coefficient. The formula to estimate an elasticity when an OLS demand curve has been estimated becomes: \u00ce\u00b7p=b(P\u00e2\u0088\u0092Q\u00e2\u0088\u0092)\u00ce\u00b7p=b(P\u00e2\u0088\u0092Q\u00e2\u0088\u0092)Where P\u00e2\u0088\u0092P\u00e2\u0088\u0092 and Q\u00e2\u0088\u0092Q\u00e2\u0088\u0092 are the mean values of these data used to estimate bb, the price coefficient.The same method can be used to estimate the other elasticities for the demand function by using the appropriate mean values of the other variables; income and price of substitute goods for example.Logarithmic Transformation of the DataOrdinary least squares estimates typically assume that the population relationship among the variables is linear thus of the form presented in The Regression Equation. In this form the interpretation of the coefficients is as discussed above; quite simply the coefficient provides an estimate of the impact of a one unit change in X on Y measured in units of Y. It does not matter just where along the line one wishes to make the measurement because it is a straight line with a constant slope thus constant estimated level of impact per unit change. It may be, however, that the analyst wishes to estimate not the simple unit measured impact on the Y variable, but the magnitude of the percentage impact on Y of a one unit change in the X variable. Such a case might be how a unit change in experience, say one year, effects not the absolute amount of a worker\u00e2\u0080\u0099s wage, but the percentage impact on the worker\u00e2\u0080\u0099s wage. Alternatively, it may be that the question asked is the unit measured impact on Y of a specific percentage increase in X. An example may be \u00e2\u0080\u009cby how many dollars will sales increase if the firm spends X percent more on advertising?\u00e2\u0080\u009d The third possibility is the case of elasticity discussed above. Here we are interested in the percentage impact on quantity demanded for a given percentage change in price, or income or perhaps the price of a substitute good. All three of these cases can be estimated by transforming the data to logarithms before running the regression. The resulting coefficients will then provide a percentage change measurement of the relevant variable.  \nTo summarize, there are four cases: Unit \u00e2\u0088\u0086X \u00e2\u0086\u0092 Unit \u00e2\u0088\u0086Y (Standard OLS case)\nUnit \u00e2\u0088\u0086X \u00e2\u0086\u0092 %\u00e2\u0088\u0086Y\n%\u00e2\u0088\u0086X \u00e2\u0086\u0092 Unit \u00e2\u0088\u0086Y\n%\u00e2\u0088\u0086X \u00e2\u0086\u0092 %\u00e2\u0088\u0086Y (elasticity case)Case 1: The ordinary least squares case begins with the linear model developed above:Y=a+bXY=a+bXwhere the coefficient of the independent variable b=dYdXb=dYdX is the slope of a straight line and thus measures the impact of a unit change in X on Y measured in units of Y.Case 2: The underlying estimated equation is: log(Y)=a+bXlog(Y)=a+bXThe equation is estimated by converting the Y values to logarithms and using OLS techniques to estimate the coefficient of the X variable, b. This is called a semi-log estimation. Again, differentiating both sides of the equation allows us to develop the interpretation of the X coefficient b: d(logY)=bdXd(logY)=bdXdYY=bdXdYY=bdX Multiply by 100 to covert to percentages and rearranging terms gives:100b=%\u00e2\u0088\u0086YUnit \u00e2\u0088\u0086X100b=%\u00e2\u0088\u0086YUnit \u00e2\u0088\u0086X100b100b is thus the percentage change in Y resulting from a unit change in X.Case 3: In this case the question is \u00e2\u0080\u009cwhat is the unit change in Y resulting from a percentage change in X?\u00e2\u0080\u009d What is the dollar loss in revenues of a five percent increase in price or what is the total dollar cost impact of a five percent increase in labor costs? The estimated equation for this case would be:Y=a+Blog(X)Y=a+Blog(X)Here the calculus differential of the estimated equation is:dY=bd(logX)dY=bd(logX)dY=bdXXdY=bdXXDivide by 100 to get percentage and rearranging terms gives:b100=dY100dXX=Unit \u00e2\u0088\u0086Y%\u00e2\u0088\u0086Xb100=dY100dXX=Unit \u00e2\u0088\u0086Y%\u00e2\u0088\u0086XTherefore, b100b100 is the increase in Y measured in units from a one percent increase in X.Case 4: This is the elasticity case where both the dependent and independent variables are converted to logs before the OLS estimation. This is known as the log-log case or double log case, and provides us with direct estimates of the elasticities of the independent variables. The estimated equation is:logY=a+blogXlogY=a+blogXDifferentiating we have: d(logY)=bd(logX)d(logY)=bd(logX)d(logX)=b1XdXd(logX)=b1XdXthus:\n1Y\ndY=b\n1X\ndX\nOR\ndYY\n=b\ndXX\nOR\nb=\ndYdX(XY)1YdY=b1XdXORdYY=bdXXORb=dYdX(XY)and b=%\u00e2\u0088\u0086Y%\u00e2\u0088\u0086Xb=%\u00e2\u0088\u0086Y%\u00e2\u0088\u0086X our definition of elasticity. We conclude that we can directly estimate the elasticity of a variable through double log transformation of the data. The estimated coefficient is the elasticity. It is common to use double log transformation of all variables in the estimation of demand functions to get estimates of all the various elasticities of the demand curve.\n"}, {"chapter": "13.6", "title": "Predicting with a Regression Equation", "mathml": "<math display=\"block\"><semantics><mrow>\n<mi>\u00c5\u00b7</mi>\n<mo>=</mo>\n<msub>\n<mi>b</mi>\n<mn>0</mn>\n</msub>\n<mo>+</mo>\n<mi>b</mi>\n<mo>,</mo>\n<msub>\n<mi>X</mi>\n<mrow>\n<mn>1</mn>\n<mi>i</mi>\n</mrow>\n</msub>\n<mo>+</mo>\n<mo>\u00e2\u008b\u00af</mo>\n<mo>+</mo>\n<msub>\n<mi>b</mi>\n<mi>k</mi>\n</msub>\n<msub>\n<mi>X</mi>\n<mrow>\n<mi>k</mi>\n<mi>i</mi>\n</mrow>\n</msub>\n</mrow><annotation-xml encoding=\"MathML-Content\"><mi>\u00c5\u00b7</mi><mo>=</mo><msub>\n<mi>b</mi>\n<mn>0</mn>\n</msub><mo>+</mo><mi>b</mi><mo>,</mo><msub>\n<mi>X</mi>\n<mrow>\n<mn>1</mn>\n<mi>i</mi>\n</mrow>\n</msub><mo>+</mo><mo>\u00e2\u008b\u00af</mo><mo>+</mo><msub>\n<mi>b</mi>\n<mi>k</mi>\n</msub><msub>\n<mi>X</mi>\n<mrow>\n<mi>k</mi>\n<mi>i</mi>\n</mrow>\n</msub></annotation-xml></semantics></math>\n</div", "content": "\n\nOne important value of an estimated regression equation is its ability to predict the effects on Y of a change in one or more values of the independent variables. The value of this is obvious. Careful policy cannot be made without estimates of the effects that may result. Indeed, it is the desire for particular results that drive the formation of most policy. Regression models can be, and have been, invaluable aids in forming such policies.  \nThe Gauss-Markov theorem assures us that the point estimate of the impact  on the dependent variable derived by putting in the equation the hypothetical values of the independent variables one wishes to simulate will result in an estimate of the dependent variable which is minimum variance and unbiased. That is to say that from this equation comes the best unbiased point estimate of y given the values of x.\n\u00c5\u00b7\n=\n\nb\n0\n\n+\nb\n,\n\nX\n\n1\ni\n\n\n+\n\u00e2\u008b\u00af\n+\n\nb\nk\n\n\nX\n\nk\ni\n\n\n\u00c5\u00b7=\nb\n0\n+b,\nX\n\n1\ni\n\n+\u00e2\u008b\u00af+\nb\nk\n\nX\n\nk\ni\n\n\nRemember that point estimates do not carry a particular level of probability, or level of confidence, because points have no \u00e2\u0080\u009cwidth\u00e2\u0080\u009d above which there is an area to measure. This was why we developed confidence intervals for the mean and proportion earlier. The same concern arises here also. There are actually two different approaches to the issue of developing estimates of changes in the independent variable, or variables, on the dependent variable. The first approach wishes to measure the expected mean value of y from a specific change in the value of x: this specific value implies the expected value. Here the question is: what is the mean impact on y that would result from multiple hypothetical experiments on y at this specific value of x. Remember that there is a variance around the estimated parameter of x and thus each experiment will result in a bit of a different estimate of the predicted value of y. The second approach to estimate the effect of a specific value of x on y treats the event as a single experiment: you choose x and multiply it times the coefficient and that provides a single estimate of y. Because this approach acts as if there were a single experiment the variance that exists in the parameter estimate is larger than the variance associated with the expected value approach. The conclusion is that we have two different ways to predict the effect of values of the independent variable(s) on the dependent variable and thus we have two different intervals. Both are correct answers to the question being asked, but there are two different questions. To avoid confusion, the first case where we are asking for the expected value of the mean of the estimated y, is called a confidence interval as we have named this concept before. The second case, where we are asking for the estimate of the impact on the dependent variable y of a single experiment using a value of x, is called the prediction interval. The test statistics for these two interval measures within which the estimated value of y will fall are:Confidence Interval for Expected Value of Mean Value of y for x=xp\n\u00c5\u00b7\n=\n\u00c2\u00b1\n\nt\n\n\n\u00ce\u00b1\n2\n\n\n\n\ns\ne\n\n(\n\n\n1\nn\n\n+\n\n\n\n\n(\n\nx\np\n\n-\n\nx\n\u00e2\u0080\u0093\n\n)\n\n2\n\n\n\nsx\n\n\n\n)\n\u00c5\u00b7=\u00c2\u00b1\nt\n\n\n\u00ce\u00b1\n2\n\n\n\ns\ne\n(\n\n1\nn\n\n+\n\n\n\n\n(\n\nx\np\n\n-\n\nx\n\u00e2\u0080\u0093\n\n)\n\n2\n\n\n\nsx\n\n\n)\nPrediction Interval for an Individual y for x = xp\n\u00c5\u00b7\n=\n\u00c2\u00b1\n\nt\n\n\n\u00ce\u00b1\n2\n\n\n\n\ns\ne\n\n(\n\n1\n+\n\n1\nn\n\n+\n\n\n\n\n(\n\nx\np\n\n-\n\nx\n\u00e2\u0080\u0093\n\n)\n\n2\n\n\n\nsx\n\n\n\n)\n\u00c5\u00b7=\u00c2\u00b1\nt\n\n\n\u00ce\u00b1\n2\n\n\n\ns\ne\n(\n1\n+\n\n1\nn\n\n+\n\n\n\n\n(\n\nx\np\n\n-\n\nx\n\u00e2\u0080\u0093\n\n)\n\n2\n\n\n\nsx\n\n\n)\nWhere se is the standard deviation of the error term and sx is the standard deviation of the x variable. The mathematical computations of these two test statistics are complex.  Various computer regression software packages provide programs within the regression functions to provide answers to inquires of estimated predicted values of y given various values chosen for the x variable(s). It is important to know just which interval is being tested in the computer package because the difference in the size of the standard deviations will change the size of the interval estimated. This is shown in Figure 13.15.\nFigure \n13.15\n \nPrediction and confidence intervals for regression equation; 95% confidence level.\n\nFigure 13.15 shows visually the difference the standard deviation makes in the size of the estimated intervals. The confidence interval, measuring the expected value of the dependent variable, is smaller than the prediction interval for the same level of confidence. The expected value method assumes that the experiment is conducted multiple times rather than just once as in the other method. The logic here is similar, although not identical, to that discussed when developing the relationship between the sample size and the confidence interval using the Central Limit Theorem. There, as the number of experiments increased, the distribution narrowed and the confidence interval became tighter around the expected value of the mean.It is also important to note that the intervals around a point estimate are highly dependent upon the range of data used to estimate the equation regardless of which approach is being used for prediction. Remember that all regression equations go through the point of means, that is, the mean value of y and the mean values of all independent variables in the equation. As the value of x chosen to estimate the associated value of y is further from the point of means the width of the estimated interval around the point estimate increases. Choosing values of x beyond the range of the data used to estimate the equation possess even greater danger of creating estimates with little use; very large intervals, and risk of error.  Figure 13.16 shows this relationship. \nFigure \n13.16\n \nConfidence interval for an individual value of x, Xp, at 95% level of confidence\n\nFigure 13.16 demonstrates the concern for the quality of the estimated interval whether it is a prediction interval or a confidence interval. As the value chosen to predict y, Xp in the graph, is further from the central weight of the data, X\u00e2\u0080\u0093X\u00e2\u0080\u0093, we see the interval expand in width even while holding constant the level of confidence. This shows that the precision of any estimate will diminish as one tries to predict beyond the largest weight of the data and most certainly will degrade rapidly for predictions beyond the range of the data. Unfortunately, this is just where most predictions are desired. They can be made, but the width of the confidence interval may be so large as to render the prediction useless. Only actual calculation and the particular application can determine this, however.\nExample \n13.6\n \n\n\nRecall the third exam/final exam example .We found the equation of the best-fit line for the final exam grade as a function of the grade on the third-exam. We can now use the least-squares regression line for prediction. Assume the coefficient for X was determined to be significantly different from zero. Suppose you want to estimate, or predict, the mean final exam score of statistics students who received 73 on the third exam. The exam scores (x-values) range from 65 to 75. Since 73 is between the x-values 65 and 75, we feel comfortable to substitute x = 73 into the equation. Then:\n\n\ny\n^\n\n=\u00e2\u0088\u0092173.51+4.83(73)=179.08\n\n\n\ny\n^\n\n=\u00e2\u0088\u0092173.51+4.83(73)=179.08\n\n\nWe predict that statistics students who earn a grade of 73 on the third exam will earn a grade of 179.08 on the final exam, on average.\n\na. What would you predict the final exam score to be for a student who scored a 66 on the third exam?  \n\n\n\nSolution\n1\n\n\na. 145.27\n\n\n\n\n\nb. What would you predict the final exam score to be for a student who scored a 90 on the third exam?\n  \n\n\n\nSolution\n2\n\n\nb. The x values in the data are between 65 and 75. Ninety is outside of the domain of the observed x values in the data (independent variable), so you cannot reliably predict the final exam score for this student. (Even though it is possible to enter 90 into the equation for x and calculate a corresponding y value, the y value that you get will have a confidence interval that may not be meaningful.)\n\n\nTo understand really how unreliable the prediction can be outside of the observed x values observed in the data, make the substitution x = 90 into the equation.\n\n\n\ny\n^\n\n=\n\u00e2\u0080\u0093173.51\n+\n4.83\n(\n90\n)\n=\n261.19\n\ny\n^\n=\u00e2\u0080\u0093173.51+4.83(90)=261.19\n\nThe final-exam score is predicted to be 261.19.  The largest the final-exam score can be is 200.\n\n\n\n\n\n\n"}]}